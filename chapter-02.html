<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>BabyGPT: Chapter 2 - From Counts to Functions</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Space+Grotesk:wght@400;700&family=Crimson+Pro:ital,wght@0,400;1,400&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg-primary: #0a0a0f;
      --bg-secondary: #12121a;
      --bg-tertiary: #1a1a25;
      --text-primary: #e8e6e3;
      --text-secondary: #9d9d9d;
      --text-muted: #5a5a6e;
      --accent-cyan: #00d9ff;
      --accent-magenta: #ff006e;
      --accent-yellow: #ffd60a;
      --accent-green: #39ff14;
      --border-color: #2a2a3a;
      --code-bg: #0d0d14;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Crimson Pro', Georgia, serif;
      background: var(--bg-primary);
      color: var(--text-primary);
      line-height: 1.7;
      font-size: 18px;
      min-height: 100vh;
    }

    body::before {
      content: '';
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-image: url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noise'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.9' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noise)'/%3E%3C/svg%3E");
      opacity: 0.03;
      pointer-events: none;
      z-index: 1000;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 60px 30px;
    }

    .chapter-header {
      margin-bottom: 80px;
      position: relative;
    }

    .chapter-number {
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      color: var(--accent-cyan);
      letter-spacing: 3px;
      text-transform: uppercase;
      margin-bottom: 20px;
      display: flex;
      align-items: center;
      gap: 12px;
    }

    .chapter-number::before {
      content: '';
      width: 40px;
      height: 1px;
      background: var(--accent-cyan);
    }

    .chapter-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(48px, 8vw, 72px);
      font-weight: 700;
      line-height: 1.1;
      margin-bottom: 20px;
      background: linear-gradient(135deg, var(--text-primary) 0%, var(--text-secondary) 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .chapter-subtitle {
      font-size: 22px;
      color: var(--text-secondary);
      font-style: italic;
      max-width: 600px;
    }

    .section {
      margin-bottom: 80px;
    }

    .section-header {
      display: flex;
      align-items: center;
      gap: 16px;
      margin-bottom: 30px;
    }

    .section-number {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      color: var(--accent-magenta);
      background: rgba(255, 0, 110, 0.1);
      padding: 6px 12px;
      border-radius: 4px;
      border: 1px solid rgba(255, 0, 110, 0.3);
    }

    .section-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 28px;
      font-weight: 700;
      color: var(--text-primary);
    }

    p {
      margin-bottom: 20px;
      color: var(--text-primary);
    }

    .highlight {
      color: var(--accent-yellow);
      font-weight: 700;
    }

    .term {
      color: var(--accent-cyan);
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.9em;
    }

    /* Callouts */
    .callout {
      background: var(--bg-secondary);
      border-radius: 8px;
      padding: 25px;
      margin: 25px 0;
      border-left: 4px solid var(--accent-cyan);
    }

    .callout.warning {
      border-left-color: var(--accent-yellow);
    }

    .callout.insight {
      border-left-color: var(--accent-magenta);
    }

    .callout.info {
      border-left-color: var(--accent-cyan);
    }

    .callout-title {
      font-family: 'Space Grotesk', sans-serif;
      font-weight: 700;
      font-size: 16px;
      margin-bottom: 15px;
      color: var(--text-primary);
    }

    .callout p:last-child {
      margin-bottom: 0;
    }

    /* Transfer/Structure sharing visualization */
    .transfer-viz {
      margin: 15px 0;
    }

    .transfer-corpus {
      background: var(--bg-tertiary);
      border-radius: 8px;
      padding: 15px;
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .transfer-sentence {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 8px 12px;
      border-radius: 4px;
    }

    .transfer-sentence.seen {
      background: rgba(0, 212, 170, 0.1);
      border-left: 3px solid var(--accent-green);
    }

    .transfer-sentence.unseen {
      background: rgba(255, 107, 107, 0.1);
      border-left: 3px solid #ff6b6b;
      opacity: 0.7;
    }

    .transfer-sentence .count {
      color: var(--text-muted);
      font-size: 11px;
    }

    .transfer-comparison {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin: 15px 0;
    }

    .transfer-method {
      background: var(--bg-tertiary);
      border-radius: 8px;
      padding: 15px;
    }

    .transfer-title {
      font-weight: 700;
      margin-bottom: 12px;
      font-size: 13px;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .transfer-entries {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .transfer-entry {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      display: flex;
      justify-content: space-between;
      padding: 6px 10px;
      background: var(--code-bg);
      border-radius: 4px;
    }

    .transfer-note {
      margin-top: 12px;
      font-size: 12px;
      color: var(--text-secondary);
      font-style: italic;
    }

    /* Scaling comparison visualization */
    .scaling-viz {
      margin: 15px 0;
    }

    .scaling-comparison {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
    }

    .scaling-method {
      background: var(--bg-tertiary);
      border-radius: 8px;
      padding: 15px;
    }

    .scaling-title {
      font-weight: 700;
      margin-bottom: 12px;
      font-size: 13px;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .scaling-bars {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .scaling-bar-row {
      display: grid;
      grid-template-columns: 70px 1fr 50px;
      align-items: center;
      gap: 8px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 10px;
      color: var(--text-secondary);
    }

    .scaling-bar {
      height: 16px;
      border-radius: 3px;
    }

    .scaling-bar.counts {
      background: linear-gradient(90deg, #ff6b6b, #ee5a24);
    }

    .scaling-bar.neural {
      background: linear-gradient(90deg, var(--accent-cyan), var(--accent-green));
    }

    /* Responsive */
    @media (max-width: 600px) {
      .container {
        padding: 40px 20px;
      }

      .transfer-comparison,
      .scaling-comparison {
        grid-template-columns: 1fr;
      }

      .scaling-bar-row {
        grid-template-columns: 60px 1fr 40px;
        font-size: 9px;
      }

      .transfer-entry {
        font-size: 10px;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- Header -->
    <header class="chapter-header">
      <div class="chapter-number">Chapter 02</div>
      <h1 class="chapter-title">From Counts to Functions</h1>
      <p class="chapter-subtitle">Count-based models hit a wall. To break through, we need to learn representations that generalize.</p>
    </header>

    <!-- Section 2.1 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">2.1</span>
        <h2 class="section-title">The Limits of Counting</h2>
      </div>

      <p>In Chapter 1, we built a language model by counting. Given a context, we counted how often each character followed it, then normalized to get probabilities. Simple, interpretable, and... fundamentally limited.</p>

      <p>Two problems kill count-based models at scale:</p>

      <!-- Problem 1: No Generalization -->
      <div class="callout warning">
        <div class="callout-title">Problem 1: No Generalization</div>
        <p>Here's a concrete example. Your training corpus contains these sentences:</p>
        <div class="transfer-viz">
          <div class="transfer-corpus">
            <div class="transfer-sentence seen">"the <strong>cat</strong> sat on the mat" <span class="count">×847</span></div>
            <div class="transfer-sentence seen">"the <strong>dog</strong> ran in the yard" <span class="count">×612</span></div>
            <div class="transfer-sentence unseen">"the <strong>dog</strong> sat on the mat" <span class="count">×0</span></div>
          </div>
        </div>
        <p>Now ask: given the context "the dog", what's the probability of "sat" coming next? Count-based answer: <strong>zero</strong>. Never seen that exact sequence. But intuitively, "the dog sat" is perfectly valid English—dogs sit all the time!</p>
        <p>The problem is how count-based models see words:</p>
        <div class="transfer-comparison">
          <div class="transfer-method">
            <div class="transfer-title">Count-Based View</div>
            <div class="transfer-entries">
              <div class="transfer-entry">"cat" = symbol #7</div>
              <div class="transfer-entry">"dog" = symbol #12</div>
              <div class="transfer-entry">"rat" = symbol #89</div>
            </div>
            <div class="transfer-note">Arbitrary IDs. No relation between them.</div>
          </div>
          <div class="transfer-method">
            <div class="transfer-title">Usage Patterns</div>
            <div class="transfer-entries">
              <div class="transfer-entry">"cat" → appears after "the", before verbs</div>
              <div class="transfer-entry">"dog" → appears after "the", before verbs</div>
              <div class="transfer-entry">"rat" → appears after "the", before verbs</div>
            </div>
            <div class="transfer-note">Identical contexts. Functionally similar!</div>
          </div>
        </div>
        <p style="margin-bottom: 0;"><strong>The insight:</strong> Words that appear in similar positions are <em>functionally interchangeable</em>. "Cat", "dog", and "rat" all fit the slot "the ___ [verb]". If we could represent words by their <em>usage patterns</em> instead of arbitrary IDs, then similar words would naturally share predictions.</p>
      </div>

      <!-- Problem 2: Storage Explosion -->
      <div class="callout warning">
        <div class="callout-title">Problem 2: Storage Explosion</div>
        <p>Count-based models store every unique n-gram they've seen. As the corpus grows, so does the model:</p>
        <div class="scaling-viz">
          <div class="scaling-comparison">
            <div class="scaling-method">
              <div class="scaling-title">Count-Based</div>
              <div class="scaling-bars">
                <div class="scaling-bar-row"><span>1M tokens</span><div class="scaling-bar counts" style="width: 20%;"></div><span>50MB</span></div>
                <div class="scaling-bar-row"><span>10M tokens</span><div class="scaling-bar counts" style="width: 45%;"></div><span>400MB</span></div>
                <div class="scaling-bar-row"><span>100M tokens</span><div class="scaling-bar counts" style="width: 80%;"></div><span>3GB</span></div>
                <div class="scaling-bar-row"><span>1B tokens</span><div class="scaling-bar counts" style="width: 100%;"></div><span>25GB+</span></div>
              </div>
            </div>
            <div class="scaling-method">
              <div class="scaling-title">Neural (fixed size)</div>
              <div class="scaling-bars">
                <div class="scaling-bar-row"><span>1M tokens</span><div class="scaling-bar neural" style="width: 30%;"></div><span>10MB</span></div>
                <div class="scaling-bar-row"><span>10M tokens</span><div class="scaling-bar neural" style="width: 30%;"></div><span>10MB</span></div>
                <div class="scaling-bar-row"><span>100M tokens</span><div class="scaling-bar neural" style="width: 30%;"></div><span>10MB</span></div>
                <div class="scaling-bar-row"><span>1B tokens</span><div class="scaling-bar neural" style="width: 30%;"></div><span>10MB</span></div>
              </div>
            </div>
          </div>
        </div>
        <p style="margin-bottom: 0;">A neural model is a <em>fixed-size compression</em> of the training data. You choose how many parameters you want, then train. More data makes it smarter without making it bigger. This is why GPT-4 can "know" trillions of tokens without storing them verbatim.</p>
      </div>

      <p>Both problems point to the same solution: we need to move from <span class="highlight">lookup tables</span> to <span class="highlight">learned functions</span>.</p>
    </section>

    <!-- Section 2.2 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">2.2</span>
        <h2 class="section-title">The Key Insight: Embeddings</h2>
      </div>

      <p>What if, instead of treating each word as an arbitrary ID, we represented it as a <em>point in space</em>?</p>

      <p>This is the core idea behind <span class="term">embeddings</span>. Each word gets mapped to a vector—a list of numbers that encodes its "meaning" in a way the model can manipulate.</p>

      <div class="callout insight">
        <div class="callout-title">Coming Soon</div>
        <p style="margin-bottom: 0;">This chapter is under construction. We'll build embeddings from scratch, showing how similar words end up nearby in vector space—and how this enables the generalization that count-based models can't achieve.</p>
      </div>
    </section>

  </div>
</body>
</html>
