<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>BabyGPT: Chapter 1 - The Meat Grinder</title>
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&family=Space+Grotesk:wght@400;700&family=Crimson+Pro:ital,wght@0,400;1,400&display=swap" rel="stylesheet">
  <style>
    :root {
      --bg-primary: #0a0a0f;
      --bg-secondary: #12121a;
      --bg-tertiary: #1a1a25;
      --text-primary: #e8e6e3;
      --text-secondary: #9d9d9d;
      --text-muted: #5a5a6e;
      --accent-cyan: #00d9ff;
      --accent-magenta: #ff006e;
      --accent-yellow: #ffd60a;
      --accent-green: #39ff14;
      --border-color: #2a2a3a;
      --code-bg: #0d0d14;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Crimson Pro', Georgia, serif;
      background: var(--bg-primary);
      color: var(--text-primary);
      line-height: 1.7;
      font-size: 18px;
      min-height: 100vh;
    }

    /* Grain overlay */
    body::before {
      content: '';
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background-image: url("data:image/svg+xml,%3Csvg viewBox='0 0 256 256' xmlns='http://www.w3.org/2000/svg'%3E%3Cfilter id='noise'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='0.9' numOctaves='4' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='100%25' height='100%25' filter='url(%23noise)'/%3E%3C/svg%3E");
      opacity: 0.03;
      pointer-events: none;
      z-index: 1000;
    }

    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 60px 30px;
    }

    /* Header */
    .chapter-header {
      margin-bottom: 80px;
      position: relative;
    }

    .chapter-number {
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      color: var(--accent-cyan);
      letter-spacing: 3px;
      text-transform: uppercase;
      margin-bottom: 20px;
      display: flex;
      align-items: center;
      gap: 12px;
    }

    .chapter-number::before {
      content: '';
      width: 40px;
      height: 1px;
      background: var(--accent-cyan);
    }

    .chapter-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: clamp(48px, 8vw, 72px);
      font-weight: 700;
      line-height: 1.1;
      margin-bottom: 20px;
      background: linear-gradient(135deg, var(--text-primary) 0%, var(--text-secondary) 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .chapter-subtitle {
      font-size: 22px;
      color: var(--text-secondary);
      font-style: italic;
      max-width: 600px;
    }

    /* Sections */
    .section {
      margin-bottom: 80px;
    }

    .section-header {
      display: flex;
      align-items: center;
      gap: 16px;
      margin-bottom: 30px;
    }

    .section-number {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      color: var(--accent-magenta);
      background: rgba(255, 0, 110, 0.1);
      padding: 6px 12px;
      border-radius: 4px;
      border: 1px solid rgba(255, 0, 110, 0.3);
    }

    .section-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 28px;
      font-weight: 700;
      color: var(--text-primary);
    }

    p {
      margin-bottom: 20px;
      color: var(--text-primary);
    }

    .highlight {
      color: var(--accent-yellow);
      font-weight: 700;
    }

    .term {
      color: var(--accent-cyan);
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.9em;
    }

    /* Math blocks */
    .math-block {
      background: var(--bg-secondary);
      border-left: 3px solid var(--accent-cyan);
      padding: 25px 30px;
      margin: 30px 0;
      border-radius: 0 8px 8px 0;
      font-family: 'JetBrains Mono', monospace;
      font-size: 16px;
      overflow-x: auto;
    }

    .math-block .equation {
      color: var(--accent-yellow);
      font-size: 18px;
      margin-bottom: 15px;
    }

    .math-block .explanation {
      color: var(--text-secondary);
      font-size: 14px;
      font-family: 'Crimson Pro', serif;
      font-style: italic;
    }

    /* Callout boxes */
    .callout {
      background: var(--bg-tertiary);
      border: 1px solid var(--border-color);
      border-radius: 8px;
      padding: 25px;
      margin: 30px 0;
      position: relative;
    }

    .callout.warning {
      border-color: var(--accent-magenta);
      background: rgba(255, 0, 110, 0.05);
    }

    .callout.warning::before {
      content: '‚ö†';
      position: absolute;
      top: -12px;
      left: 20px;
      background: var(--bg-primary);
      padding: 0 10px;
      font-size: 20px;
    }

    .callout.insight {
      border-color: var(--accent-green);
      background: rgba(57, 255, 20, 0.05);
    }

    .callout.insight::before {
      content: 'üí°';
      position: absolute;
      top: -12px;
      left: 20px;
      background: var(--bg-primary);
      padding: 0 10px;
      font-size: 18px;
    }

    .callout-title {
      font-family: 'Space Grotesk', sans-serif;
      font-weight: 700;
      font-size: 16px;
      margin-bottom: 10px;
      color: var(--text-primary);
    }

    /* Interactive demos */
    .demo-container {
      background: var(--bg-secondary);
      border: 1px solid var(--border-color);
      border-radius: 12px;
      overflow: hidden;
      margin: 40px 0;
    }

    .demo-header {
      background: var(--bg-tertiary);
      padding: 15px 20px;
      border-bottom: 1px solid var(--border-color);
      display: flex;
      align-items: center;
      gap: 12px;
    }

    .demo-dot {
      width: 12px;
      height: 12px;
      border-radius: 50%;
    }

    .demo-dot.red { background: #ff5f56; }
    .demo-dot.yellow { background: #ffbd2e; }
    .demo-dot.green { background: #27ca40; }

    .demo-title {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      color: var(--text-secondary);
      margin-left: auto;
    }

    .demo-body {
      padding: 25px;
    }

    .demo-input {
      width: 100%;
      background: var(--code-bg);
      border: 1px solid var(--border-color);
      border-radius: 6px;
      padding: 15px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 16px;
      color: var(--text-primary);
      margin-bottom: 20px;
      resize: none;
    }

    .demo-input:focus {
      outline: none;
      border-color: var(--accent-cyan);
      box-shadow: 0 0 0 3px rgba(0, 217, 255, 0.1);
    }

    .demo-label {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--text-muted);
      text-transform: uppercase;
      letter-spacing: 1px;
      margin-bottom: 10px;
      display: block;
    }

    .demo-output {
      background: var(--code-bg);
      border-radius: 6px;
      padding: 20px;
      min-height: 60px;
    }

    /* Token display */
    .token-grid {
      display: flex;
      flex-wrap: wrap;
      gap: 8px;
      margin-bottom: 20px;
    }

    .token {
      display: flex;
      flex-direction: column;
      align-items: center;
      background: var(--bg-tertiary);
      border: 1px solid var(--border-color);
      border-radius: 6px;
      padding: 10px 14px;
      min-width: 50px;
      transition: all 0.2s ease;
    }

    .token:hover {
      border-color: var(--accent-cyan);
      transform: translateY(-2px);
    }

    .token-char {
      font-family: 'JetBrains Mono', monospace;
      font-size: 18px;
      color: var(--accent-yellow);
      margin-bottom: 4px;
    }

    .token-char.space {
      color: var(--text-muted);
    }

    .token-id {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--accent-cyan);
    }

    /* Vocab table */
    .vocab-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(70px, 1fr));
      gap: 6px;
      margin-top: 15px;
    }

    .vocab-item {
      display: flex;
      justify-content: space-between;
      align-items: center;
      background: var(--bg-tertiary);
      padding: 8px 12px;
      border-radius: 4px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
    }

    .vocab-char {
      color: var(--accent-yellow);
    }

    .vocab-id {
      color: var(--text-muted);
    }

    /* Sliding window demo */
    .window-demo {
      margin-top: 20px;
    }

    .window-controls {
      display: flex;
      gap: 10px;
      margin-bottom: 20px;
      flex-wrap: wrap;
      align-items: center;
    }

    .window-btn {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      padding: 10px 20px;
      border: 1px solid var(--border-color);
      background: var(--bg-tertiary);
      color: var(--text-primary);
      border-radius: 6px;
      cursor: pointer;
      transition: all 0.2s ease;
    }

    .window-btn:hover {
      border-color: var(--accent-cyan);
      background: rgba(0, 217, 255, 0.1);
    }

    .window-btn:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }

    .window-slider {
      flex: 1;
      min-width: 150px;
      -webkit-appearance: none;
      height: 6px;
      background: var(--bg-tertiary);
      border-radius: 3px;
      outline: none;
    }

    .window-slider::-webkit-slider-thumb {
      -webkit-appearance: none;
      width: 18px;
      height: 18px;
      background: var(--accent-cyan);
      border-radius: 50%;
      cursor: pointer;
    }

    .window-position {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      color: var(--text-secondary);
      min-width: 60px;
    }

    .tape-container {
      overflow-x: auto;
      padding: 10px 0;
      margin-bottom: 20px;
    }

    .tape {
      display: flex;
      gap: 2px;
      min-width: max-content;
    }

    .tape-cell {
      width: 36px;
      height: 50px;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      background: var(--bg-tertiary);
      border: 2px solid transparent;
      border-radius: 4px;
      font-family: 'JetBrains Mono', monospace;
      transition: all 0.2s ease;
    }

    .tape-cell.in-context {
      border-color: var(--accent-cyan);
      background: rgba(0, 217, 255, 0.15);
    }

    .tape-cell.is-target {
      border-color: var(--accent-magenta);
      background: rgba(255, 0, 110, 0.15);
    }

    .tape-cell-char {
      font-size: 14px;
      color: var(--text-primary);
    }

    .tape-cell-id {
      font-size: 10px;
      color: var(--text-muted);
    }

    /* Context/target display */
    .context-display {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin-top: 20px;
    }

    .context-box {
      background: var(--code-bg);
      border-radius: 8px;
      padding: 20px;
    }

    .context-box.input {
      border-left: 3px solid var(--accent-cyan);
    }

    .context-box.target {
      border-left: 3px solid var(--accent-magenta);
    }

    .context-label {
      font-family: 'JetBrains Mono', monospace;
      font-size: 11px;
      text-transform: uppercase;
      letter-spacing: 1px;
      margin-bottom: 10px;
    }

    .context-label.input { color: var(--accent-cyan); }
    .context-label.target { color: var(--accent-magenta); }

    .context-value {
      font-family: 'JetBrains Mono', monospace;
      font-size: 16px;
      color: var(--text-primary);
      word-break: break-all;
    }

    .context-ids {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--text-muted);
      margin-top: 8px;
    }

    /* Time steps table */
    .timesteps {
      margin-top: 25px;
      background: var(--code-bg);
      border-radius: 8px;
      overflow: hidden;
    }

    .timestep-row {
      display: grid;
      grid-template-columns: 50px 1fr 100px;
      padding: 12px 20px;
      border-bottom: 1px solid var(--border-color);
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      transition: background 0.2s ease;
    }

    .timestep-row:last-child {
      border-bottom: none;
    }

    .timestep-row:hover {
      background: var(--bg-tertiary);
    }

    .timestep-row.header {
      background: var(--bg-tertiary);
      font-size: 11px;
      text-transform: uppercase;
      letter-spacing: 1px;
      color: var(--text-muted);
    }

    .timestep-t {
      color: var(--text-muted);
    }

    .timestep-context {
      color: var(--accent-cyan);
    }

    .timestep-target {
      color: var(--accent-magenta);
    }

    /* Code blocks */
    .code-block {
      background: var(--code-bg);
      border-radius: 8px;
      margin: 30px 0;
      overflow: hidden;
    }

    .code-header {
      background: var(--bg-tertiary);
      padding: 10px 20px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--text-muted);
      border-bottom: 1px solid var(--border-color);
    }

    .code-content {
      padding: 20px;
      overflow-x: auto;
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      line-height: 1.6;
      white-space: pre;
    }

    .code-content .comment { color: var(--text-muted); }
    .code-content .keyword { color: var(--accent-magenta); }
    .code-content .string { color: var(--accent-green); }
    .code-content .function { color: var(--accent-cyan); }
    .code-content .number { color: var(--accent-yellow); }

    /* Lists */
    ul, ol {
      margin: 20px 0;
      padding-left: 25px;
    }

    li {
      margin-bottom: 10px;
      color: var(--text-primary);
    }

    li::marker {
      color: var(--accent-cyan);
    }

    /* Footer nav */
    .chapter-nav {
      display: flex;
      justify-content: space-between;
      align-items: center;
      margin-top: 80px;
      padding-top: 40px;
      border-top: 1px solid var(--border-color);
    }

    .nav-btn {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 14px;
      padding: 15px 25px;
      border: 1px solid var(--border-color);
      background: transparent;
      color: var(--text-primary);
      border-radius: 8px;
      cursor: pointer;
      transition: all 0.2s ease;
      text-decoration: none;
      display: flex;
      align-items: center;
      gap: 10px;
    }

    .nav-btn:hover {
      border-color: var(--accent-cyan);
      background: rgba(0, 217, 255, 0.1);
    }

    .nav-btn.disabled {
      opacity: 0.3;
      cursor: not-allowed;
    }

    /* Invariants checklist */
    .invariants {
      background: var(--bg-secondary);
      border: 2px solid var(--accent-green);
      border-radius: 12px;
      padding: 30px;
      margin: 40px 0;
    }

    .invariants-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 18px;
      font-weight: 700;
      color: var(--accent-green);
      margin-bottom: 20px;
      display: flex;
      align-items: center;
      gap: 10px;
    }

    .invariant-item {
      display: flex;
      align-items: flex-start;
      gap: 12px;
      margin-bottom: 15px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
    }

    .invariant-check {
      color: var(--accent-green);
      font-size: 16px;
    }

    /* Probability example with inline bars */
    .probability-example {
      background: var(--bg-secondary);
      border-radius: 8px;
      padding: 25px;
      margin: 25px 0;
    }

    .prob-example-row {
      display: flex;
      align-items: center;
      gap: 15px;
      margin-bottom: 12px;
    }

    .prob-char {
      font-family: 'JetBrains Mono', monospace;
      font-size: 16px;
      color: var(--accent-yellow);
      width: 25px;
      text-align: center;
    }

    .prob-bar-inline {
      height: 24px;
      background: var(--accent-cyan);
      border-radius: 4px;
      transition: width 0.3s ease;
    }

    .prob-bar-inline.highlight {
      background: var(--accent-magenta);
    }

    .prob-value {
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      color: var(--text-secondary);
      min-width: 50px;
    }

    .prob-example-sum {
      margin-top: 20px;
      padding-top: 15px;
      border-top: 1px solid var(--border-color);
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      color: var(--text-secondary);
    }

    /* Worked examples */
    .worked-example {
      background: var(--bg-secondary);
      border: 1px solid var(--border-color);
      border-radius: 12px;
      padding: 25px;
      margin: 30px 0;
    }

    .worked-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 16px;
      font-weight: 700;
      color: var(--accent-cyan);
      margin-bottom: 25px;
      padding-bottom: 15px;
      border-bottom: 1px solid var(--border-color);
    }

    .worked-step {
      display: flex;
      gap: 20px;
      margin-bottom: 25px;
    }

    .worked-step.final {
      padding-top: 20px;
      border-top: 1px dashed var(--border-color);
    }

    .worked-step:last-child {
      margin-bottom: 0;
    }

    .worked-step-num {
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      color: var(--accent-magenta);
      background: rgba(255, 0, 110, 0.1);
      width: 28px;
      height: 28px;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      flex-shrink: 0;
    }

    .worked-step-content {
      flex: 1;
    }

    .worked-step-content p {
      margin-bottom: 10px;
    }

    .worked-step-content p:last-child {
      margin-bottom: 0;
    }

    .mini-calc {
      display: flex;
      flex-wrap: wrap;
      gap: 15px;
      background: var(--code-bg);
      padding: 12px 15px;
      border-radius: 6px;
      margin: 10px 0;
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      color: var(--text-primary);
    }

    .worked-note {
      font-size: 14px;
      color: var(--text-muted);
      font-style: italic;
      margin-top: 8px;
    }

    .worked-equation {
      font-family: 'JetBrains Mono', monospace;
      font-size: 16px;
      color: var(--text-primary);
      background: var(--code-bg);
      padding: 15px 20px;
      border-radius: 6px;
      border-left: 3px solid var(--accent-yellow);
    }

    /* Corpus display */
    .corpus-display {
      background: var(--bg-secondary);
      border-radius: 8px;
      padding: 20px 25px;
      margin: 25px 0;
    }

    .corpus-sentence {
      font-family: 'JetBrains Mono', monospace;
      font-size: 16px;
      color: var(--accent-yellow);
      padding: 8px 0;
      border-bottom: 1px solid var(--border-color);
    }

    .corpus-sentence:last-child {
      border-bottom: none;
    }

    /* Frequency tables */
    .frequency-table {
      background: var(--code-bg);
      border-radius: 6px;
      overflow: hidden;
      margin: 15px 0;
    }

    .freq-row {
      display: grid;
      grid-template-columns: 1fr 80px 120px;
      padding: 10px 15px;
      border-bottom: 1px solid var(--border-color);
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
    }

    .freq-row:last-child {
      border-bottom: none;
    }

    .freq-row.header {
      background: var(--bg-tertiary);
      font-size: 11px;
      text-transform: uppercase;
      letter-spacing: 1px;
      color: var(--text-muted);
    }

    .freq-row.sum {
      background: var(--bg-tertiary);
      border-top: 2px solid var(--border-color);
      color: var(--accent-green);
    }

    .freq-char {
      color: var(--accent-yellow);
    }

    .freq-count {
      color: var(--text-primary);
      text-align: center;
    }

    .freq-prob {
      color: var(--accent-cyan);
      text-align: right;
    }

    /* Context traces */
    .context-trace {
      background: var(--code-bg);
      border-radius: 6px;
      padding: 15px;
      margin: 10px 0;
    }

    .trace-item {
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      padding: 6px 0;
      display: flex;
      align-items: center;
      gap: 8px;
    }

    .trace-context {
      color: var(--accent-cyan);
      background: rgba(0, 217, 255, 0.1);
      padding: 2px 8px;
      border-radius: 4px;
    }

    .trace-next {
      color: var(--accent-magenta);
      background: rgba(255, 0, 110, 0.1);
      padding: 2px 8px;
      border-radius: 4px;
    }

    .trace-source {
      color: var(--text-muted);
      font-size: 12px;
      margin-left: auto;
    }

    /* Training examples table */
    .training-examples {
      background: var(--code-bg);
      border-radius: 8px;
      overflow: hidden;
      margin: 25px 0;
    }

    .training-row {
      display: grid;
      grid-template-columns: 60px 1fr 80px;
      padding: 14px 20px;
      border-bottom: 1px solid var(--border-color);
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      align-items: center;
    }

    .training-row:last-child {
      border-bottom: none;
    }

    .training-row.header {
      background: var(--bg-tertiary);
      font-size: 11px;
      text-transform: uppercase;
      letter-spacing: 1px;
      color: var(--text-muted);
    }

    .training-row:not(.header):hover {
      background: var(--bg-tertiary);
    }

    .step-num {
      color: var(--text-muted);
    }

    .context-col {
      color: var(--accent-cyan);
    }

    .target-col {
      color: var(--accent-magenta);
    }

    /* Exercises */
    .exercise {
      background: var(--bg-secondary);
      border: 1px solid var(--border-color);
      border-radius: 12px;
      margin: 30px 0;
      overflow: hidden;
    }

    .exercise-header {
      background: var(--bg-tertiary);
      padding: 15px 25px;
      border-bottom: 1px solid var(--border-color);
      display: flex;
      align-items: center;
      gap: 15px;
    }

    .exercise-number {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--accent-green);
      background: rgba(57, 255, 20, 0.1);
      padding: 5px 12px;
      border-radius: 4px;
      border: 1px solid rgba(57, 255, 20, 0.3);
    }

    .exercise-title {
      font-family: 'Space Grotesk', sans-serif;
      font-size: 16px;
      font-weight: 700;
      color: var(--text-primary);
    }

    .exercise-body {
      padding: 25px;
    }

    .exercise-hint, .exercise-solution {
      margin-top: 20px;
    }

    .hint-toggle, .solution-toggle {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      padding: 10px 20px;
      border: 1px solid var(--border-color);
      background: var(--bg-tertiary);
      color: var(--text-primary);
      border-radius: 6px;
      cursor: pointer;
      transition: all 0.2s ease;
    }

    .hint-toggle:hover {
      border-color: var(--accent-yellow);
      background: rgba(255, 214, 10, 0.1);
    }

    .solution-toggle:hover {
      border-color: var(--accent-green);
      background: rgba(57, 255, 20, 0.1);
    }

    .hint-content, .solution-content {
      display: none;
      margin-top: 15px;
      padding: 20px;
      background: var(--code-bg);
      border-radius: 8px;
      border-left: 3px solid var(--accent-yellow);
    }

    .solution-content {
      border-left-color: var(--accent-green);
    }

    .hint-content.show, .solution-content.show {
      display: block;
    }

    /* Corridor visualization */
    .corridor-viz {
      display: flex;
      flex-direction: column;
      gap: 8px;
      margin-bottom: 20px;
    }

    .corridor-row {
      display: flex;
      align-items: center;
      gap: 15px;
    }

    .corridor-bar-container {
      flex: 1;
      height: 36px;
      background: var(--bg-tertiary);
      border-radius: 4px;
      overflow: hidden;
      position: relative;
    }

    .corridor-bar {
      height: 100%;
      background: linear-gradient(90deg, var(--accent-cyan), var(--accent-magenta));
      border-radius: 4px;
      transition: width 0.4s ease-out;
      position: relative;
    }

    .corridor-bar::after {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: linear-gradient(180deg, rgba(255,255,255,0.1) 0%, transparent 50%);
    }

    .corridor-label {
      min-width: 280px;
      display: flex;
      justify-content: space-between;
      align-items: center;
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
    }

    .corridor-text {
      color: var(--text-secondary);
    }

    .corridor-prob {
      color: var(--accent-yellow);
      font-weight: 700;
    }

    .corridor-formula {
      background: var(--code-bg);
      border-radius: 6px;
      padding: 15px 20px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 14px;
      border-left: 3px solid var(--accent-yellow);
    }

    .formula-result {
      color: var(--text-secondary);
    }

    .formula-result .prob-term {
      color: var(--accent-cyan);
    }

    .formula-result .prob-equals {
      color: var(--text-muted);
      margin: 0 8px;
    }

    .formula-result .prob-final {
      color: var(--accent-yellow);
      font-weight: 700;
    }

    /* N-gram examples */
    .ngram-examples {
      background: var(--bg-tertiary);
      border-radius: 8px;
      padding: 15px 20px;
      margin: 15px 0;
    }

    .ngram-row {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      padding: 6px 0;
    }

    .ngram-row:not(:last-child) {
      border-bottom: 1px solid var(--border-color);
    }

    .ngram-name {
      color: var(--accent-cyan);
      margin-right: 10px;
    }

    .ngram-row code {
      background: var(--code-bg);
      padding: 2px 6px;
      border-radius: 3px;
      margin: 0 3px;
    }

    /* Sparsity visualization */
    .sparsity-viz {
      display: flex;
      flex-direction: column;
      gap: 8px;
      margin: 15px 0;
    }

    .sparsity-row {
      display: grid;
      grid-template-columns: 140px 1fr 80px;
      align-items: center;
      gap: 12px;
    }

    .sparsity-context {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      color: var(--accent-cyan);
      text-align: right;
    }

    .sparsity-bar-container {
      height: 20px;
      background: var(--bg-tertiary);
      border-radius: 4px;
      overflow: hidden;
    }

    .sparsity-bar {
      height: 100%;
      background: linear-gradient(90deg, var(--accent-magenta), var(--accent-cyan));
      border-radius: 4px;
      transition: width 0.3s ease;
    }

    .sparsity-count {
      font-family: 'JetBrains Mono', monospace;
      font-size: 11px;
      color: var(--text-secondary);
    }

    /* Transfer/Structure sharing visualization */
    .transfer-viz {
      margin: 15px 0;
    }

    .transfer-corpus {
      background: var(--bg-tertiary);
      border-radius: 8px;
      padding: 15px;
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .transfer-sentence {
      font-family: 'JetBrains Mono', monospace;
      font-size: 13px;
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 8px 12px;
      border-radius: 4px;
    }

    .transfer-sentence.seen {
      background: rgba(0, 212, 170, 0.1);
      border-left: 3px solid var(--accent-green);
    }

    .transfer-sentence.unseen {
      background: rgba(255, 107, 107, 0.1);
      border-left: 3px solid #ff6b6b;
      opacity: 0.7;
    }

    .transfer-sentence .count {
      color: var(--text-muted);
      font-size: 11px;
    }

    .transfer-comparison {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin: 15px 0;
    }

    .transfer-method {
      background: var(--bg-tertiary);
      border-radius: 8px;
      padding: 15px;
    }

    .transfer-title {
      font-weight: 700;
      margin-bottom: 12px;
      font-size: 13px;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .transfer-entries {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .transfer-entry {
      font-family: 'JetBrains Mono', monospace;
      font-size: 12px;
      display: flex;
      justify-content: space-between;
      padding: 6px 10px;
      background: var(--code-bg);
      border-radius: 4px;
    }

    .transfer-entry.connected {
      border-left: 2px solid var(--accent-cyan);
    }

    .transfer-entry .prob {
      font-weight: 600;
    }

    .transfer-entry .prob.high {
      color: var(--accent-green);
    }

    .transfer-entry .prob.medium {
      color: var(--accent-cyan);
    }

    .transfer-entry .prob.zero {
      color: #ff6b6b;
    }

    .transfer-note {
      margin-top: 12px;
      font-size: 12px;
      color: var(--text-secondary);
      font-style: italic;
    }

    /* Scaling comparison visualization */
    .scaling-viz {
      margin: 15px 0;
    }

    .scaling-comparison {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
    }

    .scaling-method {
      background: var(--bg-tertiary);
      border-radius: 8px;
      padding: 15px;
    }

    .scaling-title {
      font-weight: 700;
      margin-bottom: 12px;
      font-size: 13px;
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .scaling-bars {
      display: flex;
      flex-direction: column;
      gap: 8px;
    }

    .scaling-bar-row {
      display: grid;
      grid-template-columns: 70px 1fr 50px;
      align-items: center;
      gap: 8px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 10px;
      color: var(--text-secondary);
    }

    .scaling-bar {
      height: 16px;
      border-radius: 3px;
    }

    .scaling-bar.counts {
      background: linear-gradient(90deg, #ff6b6b, #ee5a24);
    }

    .scaling-bar.neural {
      background: linear-gradient(90deg, var(--accent-cyan), var(--accent-green));
    }

    /* Responsive */
    @media (max-width: 600px) {
      .container {
        padding: 40px 20px;
      }

      .context-display {
        grid-template-columns: 1fr;
      }

      .training-row {
        grid-template-columns: 40px 1fr 60px;
        font-size: 12px;
      }

      .sparsity-row {
        grid-template-columns: 100px 1fr 60px;
        gap: 8px;
      }

      .sparsity-context {
        font-size: 10px;
      }

      .scaling-comparison,
      .transfer-comparison {
        grid-template-columns: 1fr;
      }

      .scaling-bar-row {
        grid-template-columns: 60px 1fr 40px;
        font-size: 9px;
      }

      .transfer-entry {
        font-size: 10px;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- Header -->
    <header class="chapter-header">
      <div class="chapter-number">Chapter 01</div>
      <h1 class="chapter-title">The Meat Grinder</h1>
      <p class="chapter-subtitle">Raw text walks in wearing poetry and vibes. Our model only eats integers. This is where we build the digestive tract.</p>
    </header>

    <!-- Section 1.1 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.1</span>
        <h2 class="section-title">The Physics of the Problem</h2>
      </div>

      <p>We're building a machine that <span class="highlight">predicts the future</span>. Not stock prices, not the weather‚Äîthe next character in a sequence. That's it. That's the whole game.</p>

      <p>But before we write a single line of code, we need to understand <em>what we're even trying to do</em> mathematically. Because if you don't understand the goal, you're just typing with extra steps.</p>

      <p>Buckle up. We're going to derive the entire training objective from scratch. No hand-waving. No "trust me bro." Just vibes and first principles.</p>
    </section>

    <!-- Section 1.1.1 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.1.1</span>
        <h2 class="section-title">What Even Is Probability</h2>
      </div>

      <p>Probability is just <span class="highlight">how surprised you'd be</span> if something happened.</p>

      <p>If I tell you "the next word is 'the'"‚Äîyou're not surprised. <span class="term">'the'</span> shows up constantly. High probability, low surprise.</p>

      <p>If I tell you "the next word is 'defenestration'"‚Äîyou're very surprised. That word barely exists outside of "thrown out a window" trivia. Low probability, high surprise.</p>

      <div class="math-block">
        <div class="equation">P(event) ‚àà [0, 1]</div>
        <div class="explanation">Probability lives between 0 (impossible, you'd have a heart attack) and 1 (certain, you'd be bored).</div>
      </div>

      <p>Here's the thing that makes probability actually useful: if you list out <em>every possible thing that could happen</em>, those probabilities <span class="highlight">must add up to exactly 1</span>.</p>

      <p>Why? Because <em>something</em> has to happen. The universe doesn't allow "50% chance of heads, 50% chance of tails, and also 30% chance the coin vanishes into the shadow realm." That's 130%. Reality doesn't work like that.</p>

      <p>Let's make this concrete. Say you're predicting the next character after "<span class="term">hel</span>" and your vocabulary is just 5 characters: <span class="term">[a, l, o, p, z]</span>. Your model needs to output 5 numbers:</p>

      <div class="probability-example">
        <div class="prob-example-row">
          <span class="prob-char">a</span>
          <div class="prob-bar-inline" style="width: 5%;"></div>
          <span class="prob-value">0.05</span>
        </div>
        <div class="prob-example-row">
          <span class="prob-char">l</span>
          <div class="prob-bar-inline highlight" style="width: 40%;"></div>
          <span class="prob-value">0.40</span>
        </div>
        <div class="prob-example-row">
          <span class="prob-char">o</span>
          <div class="prob-bar-inline" style="width: 15%;"></div>
          <span class="prob-value">0.15</span>
        </div>
        <div class="prob-example-row">
          <span class="prob-char">p</span>
          <div class="prob-bar-inline highlight" style="width: 35%;"></div>
          <span class="prob-value">0.35</span>
        </div>
        <div class="prob-example-row">
          <span class="prob-char">z</span>
          <div class="prob-bar-inline" style="width: 5%;"></div>
          <span class="prob-value">0.05</span>
        </div>
        <div class="prob-example-sum">
          <span>Œ£ = 0.05 + 0.40 + 0.15 + 0.35 + 0.05 = <span class="highlight">1.00</span></span>
        </div>
      </div>

      <p>The model is betting 40% on <span class="term">l</span> (hello), 35% on <span class="term">p</span> (help), and leaving scraps for the rest. These five numbers form a <span class="term">probability distribution</span>‚Äîa complete accounting of all possibilities that sums to 1.</p>

      <div class="callout insight">
        <div class="callout-title">The Core Insight</div>
        <p>A language model is just a function that takes some context and outputs a probability distribution over the vocabulary. All the complexity you'll see later‚Äîall the matrix multiplications and nonlinearities and clever tricks‚Äîexists solely to make this function produce good distributions.</p>
        <p style="margin-bottom: 0;">Context in ‚Üí probability distribution out. Everything else is implementation details.</p>
      </div>
    </section>

    <!-- Section 1.1.2 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.1.2</span>
        <h2 class="section-title">The Problem With Sequences</h2>
      </div>

      <p>Cool, so we understand single-token probability. But text isn't one token. It's a <em>sequence</em> of tokens. The word "cat" is three events happening in order: <span class="term">c</span>, then <span class="term">a</span>, then <span class="term">t</span>.</p>

      <p>So here's the question: what's the probability of this <em>entire sequence</em> happening?</p>

      <p>Mathematicians call this the <span class="highlight">joint probability</span>:</p>

      <div class="math-block">
        <div class="equation">P(x‚ÇÅ, x‚ÇÇ, x‚ÇÉ, ..., x‚Çú)</div>
        <div class="explanation">"What's the probability of seeing x‚ÇÅ AND x‚ÇÇ AND x‚ÇÉ... all in that exact order?"</div>
      </div>

      <p>For "cat":</p>

      <div class="math-block">
        <div class="equation">P("cat") = P(c, a, t) = ???</div>
        <div class="explanation">Some number between 0 and 1 representing "how likely is this sequence in English?"</div>
      </div>

      <p>Now here's the problem. How do we actually <em>compute</em> this number?</p>

      <p>The naive approach: just memorize every possible sequence and its probability.</p>

      <div class="callout warning">
        <div class="callout-title">Why Counting Sequences Doesn't Scale</div>
        <p>You <em>could</em> count every sequence in your corpus and compute P(sequence) = count(sequence) / total. This actually works for short sequences. But three problems kill it:</p>
        <p><strong>1. Sparsity.</strong> Your corpus contains "the cat sat" but not "the cat stood". Count-based probability says P("the cat stood") = 0. That's wrong‚Äîit's valid English, just unseen. As sequences get longer, almost everything is unseen. A 10-character sequence has 27¬π‚Å∞ ‚âà 282 trillion possibilities. Your corpus has millions. The ratio of "seen" to "possible" approaches zero.</p>
        <p><strong>2. No generalization.</strong> Seeing "the cat sat" teaches you nothing about "the dog sat". Each sequence is memorized independently. You can't transfer knowledge from similar contexts.</p>
        <p style="margin-bottom: 0;"><strong>3. Storage.</strong> Even storing just the sequences you <em>did</em> see gets huge. And you need smoothing hacks (Laplace, Kneser-Ney) to handle zeros, which are ugly band-aids on a broken approach.</p>
      </div>

      <p>We need a <span class="highlight">decomposition strategy</span>‚Äîbreak the joint probability into smaller pieces.</p>

      <p>These smaller pieces have a name: <span class="term">n-grams</span>. An n-gram is just a chunk of n consecutive characters:</p>

      <div class="ngram-examples">
        <div class="ngram-row"><span class="ngram-name">1-gram (unigram):</span> <code>"c"</code>, <code>"a"</code>, <code>"t"</code></div>
        <div class="ngram-row"><span class="ngram-name">2-gram (bigram):</span> <code>"ca"</code>, <code>"at"</code>, <code>"th"</code></div>
        <div class="ngram-row"><span class="ngram-name">3-gram (trigram):</span> <code>"cat"</code>, <code>"the"</code>, <code>"sat"</code></div>
      </div>

      <p>Instead of asking "what's the probability of this entire sequence?", we ask "given this short n-gram context, what character comes next?"</p>

      <p>Why does this help? <span class="highlight">Shorter contexts appear far more often.</span></p>

      <!-- Why Shorter Contexts Help -->
      <div class="callout info" style="margin-top: 20px;">
        <div class="callout-title">The Sparsity Cure</div>
        <p>Shorter contexts appear <em>exponentially</em> more often than longer ones. Here's real data from a typical corpus:</p>
        <div class="sparsity-viz" id="sparsity-viz">
          <div class="sparsity-row">
            <span class="sparsity-context">"the cat sat on"</span>
            <div class="sparsity-bar-container"><div class="sparsity-bar" style="width: 0.5%;"></div></div>
            <span class="sparsity-count">3 times</span>
          </div>
          <div class="sparsity-row">
            <span class="sparsity-context">"the cat sat"</span>
            <div class="sparsity-bar-container"><div class="sparsity-bar" style="width: 2%;"></div></div>
            <span class="sparsity-count">12 times</span>
          </div>
          <div class="sparsity-row">
            <span class="sparsity-context">"cat sat"</span>
            <div class="sparsity-bar-container"><div class="sparsity-bar" style="width: 5%;"></div></div>
            <span class="sparsity-count">47 times</span>
          </div>
          <div class="sparsity-row">
            <span class="sparsity-context">"sat"</span>
            <div class="sparsity-bar-container"><div class="sparsity-bar" style="width: 25%;"></div></div>
            <span class="sparsity-count">2,847 times</span>
          </div>
          <div class="sparsity-row">
            <span class="sparsity-context">"at"</span>
            <div class="sparsity-bar-container"><div class="sparsity-bar" style="width: 60%;"></div></div>
            <span class="sparsity-count">18,392 times</span>
          </div>
          <div class="sparsity-row">
            <span class="sparsity-context">"t"</span>
            <div class="sparsity-bar-container"><div class="sparsity-bar" style="width: 100%;"></div></div>
            <span class="sparsity-count">91,247 times</span>
          </div>
        </div>
        <p style="margin-bottom: 0;"><strong>The insight:</strong> Instead of asking "how often did this exact 10-character sequence appear?" (answer: probably never), we ask "given the last few characters, what usually comes next?" The short context "sat" appeared 2,847 times‚Äîplenty of data to estimate what follows it. The long context "the cat sat on" appeared only 3 times‚Äîtoo sparse to learn from reliably.</p>
      </div>

      <p>So decomposition cures sparsity. But <em>how</em> do we actually factor the joint probability into a product of these shorter pieces? Here's the geometric intuition. Imagine all possible texts as a vast space‚Äîevery sequence that could ever be written. When you observe the first character is <span class="term">c</span>, you've just <span class="highlight">narrowed the corridor</span>. You're no longer in "all possible texts"‚Äîyou're in "texts that start with c." A smaller region.</p>

      <p>Now observe the second character is <span class="term">a</span>. The corridor narrows again. You're in "texts that start with ca"‚Äîan even smaller region. Each character you observe closes doors behind you, constraining where you can be.</p>

      <p>The probability of arriving at a specific sequence is just: <em>what fraction of the original space survives all these narrowings?</em> At step 1, some fraction starts with 'c'. Of those, some fraction continues with 'a'. Of those, some fraction continues with 't'. Multiply these fractions together and you get the probability of "cat".</p>

      <!-- Narrowing Corridor Demo -->
      <div class="demo-container">
        <div class="demo-header">
          <div class="demo-dot red"></div>
          <div class="demo-dot yellow"></div>
          <div class="demo-dot green"></div>
          <span class="demo-title">narrowing_corridor.js ‚Äî Live Demo</span>
        </div>
        <div class="demo-body">
          <label class="demo-label">Type a sequence (try "cat")</label>
          <input type="text" class="demo-input" id="corridor-input" value="" maxlength="6" placeholder="Start typing..." style="margin-bottom: 20px; text-transform: lowercase;">

          <div class="corridor-viz" id="corridor-viz">
            <div class="corridor-row" id="corridor-base">
              <div class="corridor-bar-container">
                <div class="corridor-bar" style="width: 100%;"></div>
              </div>
              <div class="corridor-label">
                <span class="corridor-text">All possible texts</span>
                <span class="corridor-prob">P = 1.00</span>
              </div>
            </div>
          </div>

          <div class="corridor-formula" id="corridor-formula">
            <span class="formula-result">Type to see the corridor narrow...</span>
          </div>
        </div>
      </div>

      <p>This isn't a trick or a formula to memorize. It's <em>forced</em> by the geometry of how sequences unfold. If you thought carefully about paths through possibility space, you'd inevitably invent it yourself.</p>

      <p>That decomposition has a name: the <span class="highlight">chain rule of probability</span>. But before we write it down formally, we need to make one concept precise: what exactly do we mean by "the fraction that continues with 'a', given we're already at 'c'"?</p>
    </section>

    <!-- Section 1.1.3 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.1.3</span>
        <h2 class="section-title">Conditional Probability</h2>
      </div>

      <p>That phrase‚Äî"given we're already at X"‚Äîis the key. It has a precise mathematical meaning called <span class="highlight">conditional probability</span>.</p>

      <p>Regular probability asks: "what's the chance of X happening?"</p>

      <p>Conditional probability asks: "what's the chance of X happening, <em>given that Y already happened</em>?"</p>

      <p>We write this as <span class="term">P(X | Y)</span>, pronounced "probability of X given Y". That vertical bar means "assuming Y is true."</p>

      <div class="worked-example">
        <div class="worked-title">Example: Weather and Umbrellas</div>
        <div class="worked-step">
          <div class="worked-step-num">?</div>
          <div class="worked-step-content">
            <p><strong>P(umbrella)</strong> = probability someone is carrying an umbrella</p>
            <p class="worked-note">Maybe 20% of people on any given day. This is an <em>unconditional</em> probability.</p>
          </div>
        </div>
        <div class="worked-step">
          <div class="worked-step-num">?</div>
          <div class="worked-step-content">
            <p><strong>P(umbrella | raining)</strong> = probability someone has an umbrella, <em>given that it's raining</em></p>
            <p class="worked-note">Now it's probably 80%. The condition restricts which scenarios we're counting‚Äîwe ignore sunny days entirely and only look at rainy days. Within that subset, umbrellas are common.</p>
          </div>
        </div>
      </div>

      <p>Conditioning <span class="highlight">shifts the distribution</span>. You're no longer asking "what's likely in general?" but "what's likely in this specific situation?" The probabilities still sum to 1, but the mass moves around based on what you know.</p>

      <p>For language, conditional probability is incredibly intuitive:</p>

      <ul>
        <li><span class="term">P(e)</span> = probability of the letter 'e' appearing (about 12% in English)</li>
        <li><span class="term">P(e | th)</span> = probability of 'e' appearing after "th" (much higher‚Äîthink "the", "them", "there")</li>
        <li><span class="term">P(q | th)</span> = probability of 'q' appearing after "th" (basically zero)</li>
      </ul>

      <p>The context changes the distribution. This is the whole insight that makes language models work.</p>
    </section>

    <!-- Section 1.1.4 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.1.4</span>
        <h2 class="section-title">The Chain Rule</h2>
      </div>

      <p>Now we can state the chain rule. It's a mathematical identity that connects joint probability (what we want) to conditional probability (what we can compute):</p>

      <div class="math-block">
        <div class="equation">P(A, B) = P(A) √ó P(B | A)</div>
        <div class="explanation">"The probability of A and B" = "probability of A" √ó "probability of B given A already happened"</div>
      </div>

      <p>Read that again. It's saying: to get the joint probability of two events, you multiply the probability of the first event by the conditional probability of the second event given the first.</p>

      <p>This telescopes for longer sequences:</p>

      <div class="math-block">
        <div class="equation">P(A, B, C) = P(A) √ó P(B | A) √ó P(C | A, B)</div>
        <div class="explanation">Each term conditions on everything that came before it.</div>
      </div>

      <p>For a sequence of <em>any</em> length:</p>

      <div class="math-block">
        <div class="equation">P(x‚ÇÅ, x‚ÇÇ, ..., x‚Çú) = P(x‚ÇÅ) √ó P(x‚ÇÇ|x‚ÇÅ) √ó P(x‚ÇÉ|x‚ÇÅ,x‚ÇÇ) √ó ... √ó P(x‚Çú|x‚ÇÅ,...,x‚Çú‚Çã‚ÇÅ)</div>
        <div class="explanation">The joint probability is the product of all conditional probabilities.</div>
      </div>

      <p>Or more compactly:</p>

      <div class="math-block">
        <div class="equation">P(x‚ÇÅ:‚Çú) = ‚àè·µ¢‚Çå‚ÇÅ·µó P(x·µ¢ | x‚ÇÅ:·µ¢‚Çã‚ÇÅ)</div>
        <div class="explanation">That ‚àè symbol means "product of all terms." This is the chain rule in its full glory.</div>
      </div>

      <div class="callout insight">
        <div class="callout-title">Why This Is Huge</div>
        <p>We just turned an impossible problem (store 27¬π‚Å∞‚Å∞ probabilities) into a tractable one (learn a function that computes P(next | context)).</p>
        <p style="margin-bottom: 0;">A language model doesn't need to memorize every possible sequence. It just needs to be really good at answering one question: "given what you've seen so far, what comes next?"</p>
      </div>
    </section>

    <!-- Section 1.1.5 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.1.5</span>
        <h2 class="section-title">Building Probabilities From a Corpus</h2>
      </div>

      <p>But where do these probabilities actually <em>come from</em>? Let's build them from scratch.</p>

      <p>Imagine our entire training dataset is just these four sentences:</p>

      <div class="corpus-display">
        <div class="corpus-sentence">"cat sat"</div>
        <div class="corpus-sentence">"cat can"</div>
        <div class="corpus-sentence">"a cat"</div>
        <div class="corpus-sentence">"sat at a"</div>
      </div>

      <p>That's our universe. 27 characters total (including spaces). Let's count things.</p>

      <div class="worked-example">
        <div class="worked-title">Step 1: What characters exist?</div>
        <div class="worked-step">
          <div class="worked-step-num">‚Üí</div>
          <div class="worked-step-content">
            <p>Unique characters: <span class="term">[space, a, c, n, s, t]</span></p>
            <p class="worked-note">That's our vocabulary. 6 tokens. (We're ignoring the quotes.)</p>
          </div>
        </div>
      </div>

      <div class="worked-example">
        <div class="worked-title">Step 2: Count first-character frequencies</div>
        <div class="worked-step">
          <div class="worked-step-num">‚Üí</div>
          <div class="worked-step-content">
            <p>Looking at what character starts each sentence:</p>
            <div class="frequency-table">
              <div class="freq-row header">
                <span>Char</span>
                <span>Count</span>
                <span>P(char)</span>
              </div>
              <div class="freq-row">
                <span class="freq-char">c</span>
                <span class="freq-count">2</span>
                <span class="freq-prob">2/4 = 0.50</span>
              </div>
              <div class="freq-row">
                <span class="freq-char">a</span>
                <span class="freq-count">1</span>
                <span class="freq-prob">1/4 = 0.25</span>
              </div>
              <div class="freq-row">
                <span class="freq-char">s</span>
                <span class="freq-count">1</span>
                <span class="freq-prob">1/4 = 0.25</span>
              </div>
              <div class="freq-row sum">
                <span>Œ£</span>
                <span>4</span>
                <span>1.00 ‚úì</span>
              </div>
            </div>
            <p class="worked-note">Two sentences start with 'c', one with 'a', one with 's'. So P(c) = 0.50.</p>
          </div>
        </div>
      </div>

      <div class="worked-example">
        <div class="worked-title">Step 3: Count what follows 'c'</div>
        <div class="worked-step">
          <div class="worked-step-num">‚Üí</div>
          <div class="worked-step-content">
            <p>Every time 'c' appears in our corpus, what comes next?</p>
            <div class="context-trace">
              <div class="trace-item"><span class="trace-context">c</span>‚Üí<span class="trace-next">a</span> <span class="trace-source">("cat sat")</span></div>
              <div class="trace-item"><span class="trace-context">c</span>‚Üí<span class="trace-next">a</span> <span class="trace-source">("cat can")</span></div>
              <div class="trace-item"><span class="trace-context">c</span>‚Üí<span class="trace-next">a</span> <span class="trace-source">("a cat")</span></div>
              <div class="trace-item"><span class="trace-context">c</span>‚Üí<span class="trace-next">a</span> <span class="trace-source">("cat can" - the second 'c')</span></div>
            </div>
            <div class="frequency-table" style="margin-top: 15px;">
              <div class="freq-row header">
                <span>Given 'c', next is...</span>
                <span>Count</span>
                <span>P(next | c)</span>
              </div>
              <div class="freq-row">
                <span class="freq-char">a</span>
                <span class="freq-count">4</span>
                <span class="freq-prob">4/4 = 1.00</span>
              </div>
            </div>
            <p class="worked-note">In this tiny corpus, 'c' is ALWAYS followed by 'a'. So P(a | c) = 1.00. (In real English it wouldn't be 100%, but our corpus is tiny.)</p>
          </div>
        </div>
      </div>

      <div class="worked-example">
        <div class="worked-title">Step 4: Count what follows 'ca'</div>
        <div class="worked-step">
          <div class="worked-step-num">‚Üí</div>
          <div class="worked-step-content">
            <p>Every time 'ca' appears, what comes next?</p>
            <div class="context-trace">
              <div class="trace-item"><span class="trace-context">ca</span>‚Üí<span class="trace-next">t</span> <span class="trace-source">("cat sat")</span></div>
              <div class="trace-item"><span class="trace-context">ca</span>‚Üí<span class="trace-next">t</span> <span class="trace-source">("cat can")</span></div>
              <div class="trace-item"><span class="trace-context">ca</span>‚Üí<span class="trace-next">t</span> <span class="trace-source">("a cat")</span></div>
              <div class="trace-item"><span class="trace-context">ca</span>‚Üí<span class="trace-next">n</span> <span class="trace-source">("cat can")</span></div>
            </div>
            <div class="frequency-table" style="margin-top: 15px;">
              <div class="freq-row header">
                <span>Given 'ca', next is...</span>
                <span>Count</span>
                <span>P(next | ca)</span>
              </div>
              <div class="freq-row">
                <span class="freq-char">t</span>
                <span class="freq-count">3</span>
                <span class="freq-prob">3/4 = 0.75</span>
              </div>
              <div class="freq-row">
                <span class="freq-char">n</span>
                <span class="freq-count">1</span>
                <span class="freq-prob">1/4 = 0.25</span>
              </div>
              <div class="freq-row sum">
                <span>Œ£</span>
                <span>4</span>
                <span>1.00 ‚úì</span>
              </div>
            </div>
            <p class="worked-note">After 'ca', we see 't' three times and 'n' once. So P(t | ca) = 0.75.</p>
          </div>
        </div>
      </div>

      <div class="callout insight">
        <div class="callout-title">We Just Built a Markov Chain</div>
        <p>What we did‚Äîcounting frequencies from a corpus‚Äîis called an <span class="term">n-gram model</span> or <span class="term">Markov chain</span>. It's a real technique that powered speech recognition and machine translation for decades.</p>
        <p style="margin-bottom: 0;">Neural language models do the same thing, but instead of counting, they learn a function that <em>approximates</em> these conditional probabilities. That's what we'll build in Chapter 2.</p>
      </div>
    </section>

    <!-- Section 1.1.6 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.1.6</span>
        <h2 class="section-title">Applying the Chain Rule</h2>
      </div>

      <p>Now let's use our counts to actually compute a joint probability. What's P("cat") according to our tiny corpus?</p>

      <div class="worked-example">
        <div class="worked-title">Computing P("cat") Step by Step</div>

        <div class="worked-step">
          <div class="worked-step-num">1</div>
          <div class="worked-step-content">
            <p><strong>Apply the chain rule:</strong></p>
            <div class="worked-equation">P("cat") = P(c) √ó P(a | c) √ó P(t | c,a)</div>
          </div>
        </div>

        <div class="worked-step">
          <div class="worked-step-num">2</div>
          <div class="worked-step-content">
            <p><strong>P(c) = ?</strong></p>
            <p>From our first-character counts: 2 out of 4 sentences start with 'c'.</p>
            <div class="mini-calc">P(c) = 2/4 = <span class="highlight">0.50</span></div>
          </div>
        </div>

        <div class="worked-step">
          <div class="worked-step-num">3</div>
          <div class="worked-step-content">
            <p><strong>P(a | c) = ?</strong></p>
            <p>Every 'c' in our corpus is followed by 'a'. 4 out of 4 times.</p>
            <div class="mini-calc">P(a | c) = 4/4 = <span class="highlight">1.00</span></div>
          </div>
        </div>

        <div class="worked-step">
          <div class="worked-step-num">4</div>
          <div class="worked-step-content">
            <p><strong>P(t | ca) = ?</strong></p>
            <p>After 'ca', we see 't' three times and 'n' once.</p>
            <div class="mini-calc">P(t | ca) = 3/4 = <span class="highlight">0.75</span></div>
          </div>
        </div>

        <div class="worked-step final">
          <div class="worked-step-num">‚úì</div>
          <div class="worked-step-content">
            <p><strong>Multiply it all together:</strong></p>
            <div class="worked-equation">P("cat") = 0.50 √ó 1.00 √ó 0.75 = <span class="highlight">0.375</span></div>
            <p class="worked-note">In our tiny universe, the sequence "cat" has a 37.5% probability. Not bad for three letters!</p>
          </div>
        </div>
      </div>

      <p>Let's sanity check this. What's P("can")?</p>

      <div class="math-block">
        <div class="equation">P("can") = P(c) √ó P(a|c) √ó P(n|ca) = 0.50 √ó 1.00 √ó 0.25 = 0.125</div>
        <div class="explanation">"can" is less likely than "cat" because P(n|ca) < P(t|ca). Makes sense‚Äî"cat" appears more often.</div>
      </div>

      <div class="callout insight">
        <div class="callout-title">The Training Objective Revealed</div>
        <p>Now you understand what a language model is actually trying to do: <span class="highlight">maximize the probability it assigns to real text</span>.</p>
        <p>If the training data contains "cat" a lot, a good model should give P("cat") a high value. If the training data never contains "xqz", P("xqz") should be tiny.</p>
        <p style="margin-bottom: 0;">The model's job is to learn conditional probabilities P(next | context) that, when multiplied together via the chain rule, give high probability to real sequences and low probability to garbage.</p>
      </div>
    </section>

    <!-- Section 1.2 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.2</span>
        <h2 class="section-title">The Actual Meat Grinder: Tokenization</h2>
      </div>

      <p>Okay, enough theory. Let's build something.</p>

      <p>Here's the problem: computers don't understand letters. They understand numbers. Your GPU can multiply matrices all day long, but it has no concept of what "a" or "cat" means.</p>

      <p>We need a translation layer. <span class="highlight">Tokenization</span> is the process of converting text into a sequence of integers that a model can actually process.</p>

      <p>For BabyGPT, we're doing the simplest possible version: <span class="term">character-level tokenization</span>. Each unique character gets a unique integer ID.</p>
    </section>

    <!-- Section 1.2.1 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.2.1</span>
        <h2 class="section-title">Building the Vocabulary</h2>
      </div>

      <p>Step 1: Look at your training data and find every unique character.</p>

      <p>Let's switch to a fresh example (the "cat" corpus served its purpose for probability). Say our training corpus is:</p>

      <div class="corpus-display">
        <div class="corpus-sentence">"hello world"</div>
      </div>

      <p>The unique characters are: <span class="term">[space, d, e, h, l, o, r, w]</span></p>

      <p>That's 8 characters. We sort them (for reproducibility) and assign each an integer:</p>

      <div class="frequency-table">
        <div class="freq-row header">
          <span>Character</span>
          <span>Token ID</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">[space]</span>
          <span class="freq-count">0</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">d</span>
          <span class="freq-count">1</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">e</span>
          <span class="freq-count">2</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">h</span>
          <span class="freq-count">3</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">l</span>
          <span class="freq-count">4</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">o</span>
          <span class="freq-count">5</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">r</span>
          <span class="freq-count">6</span>
          <span></span>
        </div>
        <div class="freq-row">
          <span class="freq-char">w</span>
          <span class="freq-count">7</span>
          <span></span>
        </div>
      </div>

      <p>This mapping is our <span class="term">vocabulary</span>. The size of this vocabulary (8 in this case) is called <span class="term">vocab_size</span>. It determines how many possible tokens the model can output.</p>

      <div class="code-block">
        <div class="code-header">vocab.py</div>
        <div class="code-content"><span class="comment"># Build vocabulary from training data</span>
<span class="keyword">def</span> <span class="function">build_vocab</span>(text):
    chars = <span class="function">sorted</span>(<span class="function">set</span>(text))  <span class="comment"># unique chars, sorted</span>
    stoi = {ch: i <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="function">enumerate</span>(chars)}  <span class="comment"># string to int</span>
    itos = {i: ch <span class="keyword">for</span> ch, i <span class="keyword">in</span> stoi.items()}       <span class="comment"># int to string</span>
    <span class="keyword">return</span> stoi, itos

<span class="comment"># Example</span>
text = <span class="string">"hello world"</span>
stoi, itos = <span class="function">build_vocab</span>(text)

<span class="function">print</span>(stoi)  <span class="comment"># {' ': 0, 'd': 1, 'e': 2, 'h': 3, 'l': 4, 'o': 5, 'r': 6, 'w': 7}</span>
<span class="function">print</span>(itos)  <span class="comment"># {0: ' ', 1: 'd', 2: 'e', 3: 'h', 4: 'l', 5: 'o', 6: 'r', 7: 'w'}</span></div>
      </div>

      <p><span class="term">stoi</span> (string-to-int) encodes text. <span class="term">itos</span> (int-to-string) decodes it. That's the whole tokenizer.</p>
    </section>

    <!-- Section 1.2.2 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.2.2</span>
        <h2 class="section-title">Encoding and Decoding</h2>
      </div>

      <p>Now we can convert between text and integers:</p>

      <div class="code-block">
        <div class="code-header">encode_decode.py</div>
        <div class="code-content"><span class="keyword">def</span> <span class="function">encode</span>(text, stoi):
    <span class="keyword">return</span> [stoi[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> text]

<span class="keyword">def</span> <span class="function">decode</span>(tokens, itos):
    <span class="keyword">return</span> <span class="string">''</span>.join([itos[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokens])

<span class="comment"># Encode "hello"</span>
tokens = <span class="function">encode</span>(<span class="string">"hello"</span>, stoi)
<span class="function">print</span>(tokens)  <span class="comment"># [3, 2, 4, 4, 5]</span>

<span class="comment"># Decode back</span>
text = <span class="function">decode</span>([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>], itos)
<span class="function">print</span>(text)    <span class="comment"># "hello"</span></div>
      </div>

      <p>"hello" becomes <span class="term">[3, 2, 4, 4, 5]</span>. That's it. Text is now numbers. The meat grinder has done its job.</p>
    </section>

    <!-- Interactive Demo -->
    <section class="section">
      <div class="demo-container">
        <div class="demo-header">
          <div class="demo-dot red"></div>
          <div class="demo-dot yellow"></div>
          <div class="demo-dot green"></div>
          <span class="demo-title">tokenizer.js ‚Äî Live Demo</span>
        </div>
        <div class="demo-body">
          <label class="demo-label">Input Text</label>
          <textarea class="demo-input" id="tokenizer-input" rows="2" placeholder="Type something...">hello world</textarea>

          <label class="demo-label">Tokenized Output</label>
          <div class="demo-output">
            <div class="token-grid" id="token-output"></div>
          </div>

          <label class="demo-label" style="margin-top: 20px;">Vocabulary</label>
          <div class="vocab-grid" id="vocab-display"></div>
        </div>
      </div>
    </section>

    <!-- Section 1.3 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.3</span>
        <h2 class="section-title">The Sliding Window</h2>
      </div>

      <p>We've got text ‚Üí integers. But we still need to turn this into <span class="highlight">training examples</span> the model can learn from.</p>

      <p>Remember the chain rule? The model needs to learn P(next | context). So each training example should be:</p>

      <ul>
        <li><strong>Input (context):</strong> some sequence of tokens</li>
        <li><strong>Target:</strong> the token that comes next</li>
      </ul>

      <p>We create these examples by sliding a window across our tokenized text.</p>
    </section>

    <!-- Section 1.3.1 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.3.1</span>
        <h2 class="section-title">Context Length</h2>
      </div>

      <p>The <span class="term">context length</span> (also called <span class="term">block size</span>) determines how many tokens the model sees before making a prediction.</p>

      <p>If context_length = 3, the model sees 3 tokens and predicts the 4th.</p>

      <p>Let's trace through "hello" with context_length = 3:</p>

      <div class="training-examples">
        <div class="training-row header">
          <span>Step</span>
          <span>Context (input)</span>
          <span>Target</span>
        </div>
        <div class="training-row">
          <span class="step-num">t=0</span>
          <span class="context-col">"hel" ‚Üí [3, 2, 4]</span>
          <span class="target-col">"l" ‚Üí 4</span>
        </div>
        <div class="training-row">
          <span class="step-num">t=1</span>
          <span class="context-col">"ell" ‚Üí [2, 4, 4]</span>
          <span class="target-col">"o" ‚Üí 5</span>
        </div>
      </div>

      <p>Each row is one training example. The model sees the context, tries to predict the target, and learns from its mistakes.</p>

      <div class="callout warning">
        <div class="callout-title">But Wait‚ÄîWhat About the Beginning?</div>
        <p>When we're at position 0, we don't <em>have</em> 3 characters of context yet. What do we do?</p>
        <p style="margin-bottom: 0;">We could pad with zeros, or we could train on variable-length contexts. For BabyGPT, we'll actually train on <em>all</em> context lengths from 1 to context_length. More training signal!</p>
      </div>
    </section>

    <!-- Section 1.3.2 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.3.2</span>
        <h2 class="section-title">Training With All Context Lengths</h2>
      </div>

      <p>Here's the full picture. For "hello" with max context_length = 4:</p>

      <div class="training-examples">
        <div class="training-row header">
          <span>Step</span>
          <span>Context (input)</span>
          <span>Target</span>
        </div>
        <div class="training-row">
          <span class="step-num">t=0</span>
          <span class="context-col">"h" ‚Üí [3]</span>
          <span class="target-col">"e" ‚Üí 2</span>
        </div>
        <div class="training-row">
          <span class="step-num">t=1</span>
          <span class="context-col">"he" ‚Üí [3, 2]</span>
          <span class="target-col">"l" ‚Üí 4</span>
        </div>
        <div class="training-row">
          <span class="step-num">t=2</span>
          <span class="context-col">"hel" ‚Üí [3, 2, 4]</span>
          <span class="target-col">"l" ‚Üí 4</span>
        </div>
        <div class="training-row">
          <span class="step-num">t=3</span>
          <span class="context-col">"hell" ‚Üí [3, 2, 4, 4]</span>
          <span class="target-col">"o" ‚Üí 5</span>
        </div>
      </div>

      <p>Every position in the text generates a training example. The model learns to predict with 1 token of context, 2 tokens, 3 tokens, all the way up to context_length tokens.</p>

      <p>This is efficient and gives the model practice with short contexts too‚Äîuseful for generating from scratch.</p>
    </section>

    <!-- Sliding Window Demo -->
    <section class="section">
      <div class="demo-container">
        <div class="demo-header">
          <div class="demo-dot red"></div>
          <div class="demo-dot yellow"></div>
          <div class="demo-dot green"></div>
          <span class="demo-title">sliding_window.js ‚Äî Live Demo</span>
        </div>
        <div class="demo-body">
          <div class="window-demo">
            <label class="demo-label">Input Text</label>
            <input type="text" class="demo-input" id="window-input" value="hello world" style="margin-bottom: 15px;">

            <label class="demo-label">Context Length</label>
            <div class="window-controls">
              <input type="range" class="window-slider" id="context-slider" min="1" max="8" value="4">
              <span class="window-position" id="context-display">4</span>
            </div>

            <label class="demo-label">Position</label>
            <div class="window-controls">
              <button class="window-btn" id="prev-btn">‚Üê Prev</button>
              <input type="range" class="window-slider" id="position-slider" min="0" max="10" value="0">
              <span class="window-position" id="position-display">0</span>
              <button class="window-btn" id="next-btn">Next ‚Üí</button>
            </div>

            <label class="demo-label" style="margin-top: 20px;">Token Tape</label>
            <div class="tape-container">
              <div class="tape" id="tape-display"></div>
            </div>

            <div class="context-display">
              <div class="context-box input">
                <div class="context-label input">Context (Input)</div>
                <div class="context-value" id="context-text">‚Äî</div>
                <div class="context-ids" id="context-ids">‚Äî</div>
              </div>
              <div class="context-box target">
                <div class="context-label target">Target</div>
                <div class="context-value" id="target-text">‚Äî</div>
                <div class="context-ids" id="target-id">‚Äî</div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Section 1.4 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.4</span>
        <h2 class="section-title">Putting It Together</h2>
      </div>

      <p>Let's write the complete data preparation pipeline:</p>

      <div class="code-block">
        <div class="code-header">data.py</div>
        <div class="code-content"><span class="keyword">import</span> torch

<span class="keyword">def</span> <span class="function">prepare_data</span>(text, context_length):
    <span class="comment"># Step 1: Build vocabulary</span>
    chars = <span class="function">sorted</span>(<span class="function">set</span>(text))
    vocab_size = <span class="function">len</span>(chars)
    stoi = {ch: i <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="function">enumerate</span>(chars)}
    itos = {i: ch <span class="keyword">for</span> ch, i <span class="keyword">in</span> stoi.items()}

    <span class="comment"># Step 2: Encode entire text</span>
    data = torch.<span class="function">tensor</span>([stoi[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> text], dtype=torch.long)

    <span class="comment"># Step 3: Create input/target pairs</span>
    <span class="comment"># For a sequence [a, b, c, d, e] with context_length=4:</span>
    <span class="comment"># Input:  [a, b, c, d]</span>
    <span class="comment"># Target: [b, c, d, e]  (shifted by 1)</span>

    X, Y = [], []
    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="function">range</span>(<span class="function">len</span>(data) - context_length):
        X.append(data[i : i + context_length])
        Y.append(data[i + <span class="number">1</span> : i + context_length + <span class="number">1</span>])

    X = torch.<span class="function">stack</span>(X)  <span class="comment"># (num_examples, context_length)</span>
    Y = torch.<span class="function">stack</span>(Y)  <span class="comment"># (num_examples, context_length)</span>

    <span class="keyword">return</span> X, Y, stoi, itos, vocab_size

<span class="comment"># Example usage</span>
text = <span class="string">"hello world"</span>
X, Y, stoi, itos, vocab_size = <span class="function">prepare_data</span>(text, context_length=<span class="number">4</span>)

<span class="function">print</span>(<span class="string">f"Vocab size: {vocab_size}"</span>)      <span class="comment"># 8</span>
<span class="function">print</span>(<span class="string">f"Training examples: {len(X)}"</span>)  <span class="comment"># 7</span>
<span class="function">print</span>(<span class="string">f"X shape: {X.shape}"</span>)            <span class="comment"># torch.Size([7, 4])</span>
<span class="function">print</span>(<span class="string">f"Y shape: {Y.shape}"</span>)            <span class="comment"># torch.Size([7, 4])</span></div>
      </div>

      <div class="callout insight">
        <div class="callout-title">Why Y Has the Same Shape as X</div>
        <p>Notice that both X and Y have shape (num_examples, context_length). That's because for each position in X, there's a corresponding target in Y.</p>
        <p>X[i] = [a, b, c, d] means: at position 0, predict b from [a]. At position 1, predict c from [a,b]. And so on.</p>
        <p style="margin-bottom: 0;">Y[i] = [b, c, d, e] gives us all those targets in one tensor. This parallel structure is crucial for efficient training‚Äîwe compute loss for all positions at once.</p>
      </div>
    </section>

    <!-- Section 1.5 -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.5</span>
        <h2 class="section-title">What We Built</h2>
      </div>

      <div class="invariants">
        <div class="invariants-title">‚úì Chapter 1 Invariants</div>
        <div class="invariant-item">
          <span class="invariant-check">‚úì</span>
          <span>A language model computes P(next | context) via the chain rule</span>
        </div>
        <div class="invariant-item">
          <span class="invariant-check">‚úì</span>
          <span>Probabilities must sum to 1 over the vocabulary</span>
        </div>
        <div class="invariant-item">
          <span class="invariant-check">‚úì</span>
          <span>Tokenization: text ‚Üî integers via stoi/itos dictionaries</span>
        </div>
        <div class="invariant-item">
          <span class="invariant-check">‚úì</span>
          <span>Training examples: (context, target) pairs from sliding window</span>
        </div>
        <div class="invariant-item">
          <span class="invariant-check">‚úì</span>
          <span>X[i] shifted by 1 = Y[i] (next-token prediction)</span>
        </div>
      </div>

      <p>Raw text went in. A tensor of integers came out. We understand exactly what the model is trying to learn (conditional probabilities) and how we'll supervise it (next-token prediction).</p>

      <p>Next chapter: we build the actual neural network. The model that takes those integers and outputs a probability distribution. The brain that learns to predict.</p>
    </section>

    <!-- Exercises -->
    <section class="section">
      <div class="section-header">
        <span class="section-number">1.6</span>
        <h2 class="section-title">Exercises</h2>
      </div>

      <div class="exercise">
        <div class="exercise-header">
          <span class="exercise-number">Exercise 1.1</span>
          <span class="exercise-title">Chain Rule by Hand</span>
        </div>
        <div class="exercise-body">
          <p>Using the corpus from section 1.1.5 ("cat sat", "cat can", "a cat", "sat at a"), compute:</p>
          <ol>
            <li>P("sat")</li>
            <li>P("at")</li>
            <li>P("can")</li>
          </ol>
          <p>Show your work using the chain rule decomposition.</p>

          <div class="exercise-solution">
            <button class="solution-toggle" onclick="toggleSolution(this)">Show Solution</button>
            <div class="solution-content">
              <p><strong>P("sat"):</strong></p>
              <pre>P(s) = 1/4 = 0.25  (one sentence starts with 's')
P(a|s) = count "sa" / count "s" = 2/2 = 1.0
P(t|sa) = count "sat" / count "sa" = 2/2 = 1.0
P("sat") = 0.25 √ó 1.0 √ó 1.0 = 0.25</pre>

              <p><strong>P("at"):</strong></p>
              <pre>P(a) = 1/4 = 0.25  (one sentence starts with 'a')
P(t|a) = (times 'a' followed by 't') / (times 'a' followed by anything)
       = 6/8 = 0.75
P("at") = 0.25 √ó 0.75 = 0.1875</pre>

              <p><strong>P("can"):</strong></p>
              <pre>P(c) = 2/4 = 0.5
P(a|c) = 4/4 = 1.0
P(n|ca) = 1/4 = 0.25
P("can") = 0.5 √ó 1.0 √ó 0.25 = 0.125</pre>
            </div>
          </div>
        </div>
      </div>

      <div class="exercise">
        <div class="exercise-header">
          <span class="exercise-number">Exercise 1.2</span>
          <span class="exercise-title">Vocabulary Edge Cases</span>
        </div>
        <div class="exercise-body">
          <p>What happens if you try to encode "goodbye" using a vocabulary built from "hello world"?</p>
          <p>Write code that handles this gracefully (either by raising a clear error or by using an unknown token).</p>

          <div class="exercise-hint">
            <button class="hint-toggle" onclick="toggleHint(this)">Show Hint</button>
            <div class="hint-content">
              <p>The characters 'g', 'b', 'y' don't exist in the vocabulary built from "hello world".</p>
              <p>Common solutions: (1) raise KeyError with helpful message, (2) add an &lt;UNK&gt; token to the vocabulary.</p>
            </div>
          </div>

          <div class="exercise-solution">
            <button class="solution-toggle" onclick="toggleSolution(this)">Show Solution</button>
            <div class="solution-content">
              <pre>def encode_safe(text, stoi, unk_token=None):
    result = []
    for ch in text:
        if ch in stoi:
            result.append(stoi[ch])
        elif unk_token is not None:
            result.append(stoi[unk_token])
        else:
            raise KeyError(f"Character '{ch}' not in vocabulary. "
                          f"Available: {list(stoi.keys())}")
    return result

# Option 1: Raise clear error
encode_safe("goodbye", stoi)  # KeyError: Character 'g' not in vocabulary

# Option 2: Add UNK token to vocab first
stoi['<UNK>'] = len(stoi)
itos[len(itos)] = '<UNK>'
encode_safe("goodbye", stoi, unk_token='<UNK>')  # Works!</pre>
            </div>
          </div>
        </div>
      </div>

      <div class="exercise">
        <div class="exercise-header">
          <span class="exercise-number">Exercise 1.3</span>
          <span class="exercise-title">Counting Training Examples</span>
        </div>
        <div class="exercise-body">
          <p>If your text has N characters and your context_length is C, how many training examples do you get?</p>
          <p>Derive the formula and verify it with "hello world" (N=11) and context_length=4.</p>

          <div class="exercise-solution">
            <button class="solution-toggle" onclick="toggleSolution(this)">Show Solution</button>
            <div class="solution-content">
              <p><strong>Formula:</strong> num_examples = N - C</p>
              <p>Why? Each training example needs C characters for context plus 1 character for target. So positions 0 through N-C-1 can be starting points, giving N-C examples.</p>
              <pre>N = 11 (len("hello world"))
C = 4
num_examples = 11 - 4 = 7

Verify:
"hell" ‚Üí "ello"  (position 0)
"ello" ‚Üí "llo "  (position 1)
"llo " ‚Üí "lo w"  (position 2)
"lo w" ‚Üí "o wo"  (position 3)
"o wo" ‚Üí " wor"  (position 4)
" wor" ‚Üí "worl"  (position 5)
"worl" ‚Üí "orld"  (position 6)
= 7 examples ‚úì</pre>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Navigation -->
    <nav class="chapter-nav">
      <a class="nav-btn disabled">‚Üê Previous</a>
      <a href="chapter-02.html" class="nav-btn">Next: The Transformer ‚Üí</a>
    </nav>
  </div>

  <!-- JavaScript for interactive demos -->
  <script>
    // Tokenizer Demo
    function initTokenizerDemo() {
      const input = document.getElementById('tokenizer-input');
      const tokenOutput = document.getElementById('token-output');
      const vocabDisplay = document.getElementById('vocab-display');

      function updateTokenizer() {
        const text = input.value;
        const chars = [...new Set(text)].sort();
        const stoi = {};
        chars.forEach((ch, i) => stoi[ch] = i);

        // Display vocabulary
        vocabDisplay.innerHTML = chars.map(ch => `
          <div class="vocab-item">
            <span class="vocab-char">${ch === ' ' ? '‚ê£' : ch}</span>
            <span class="vocab-id">${stoi[ch]}</span>
          </div>
        `).join('');

        // Display tokens
        tokenOutput.innerHTML = [...text].map(ch => `
          <div class="token">
            <span class="token-char ${ch === ' ' ? 'space' : ''}">${ch === ' ' ? '‚ê£' : ch}</span>
            <span class="token-id">${stoi[ch]}</span>
          </div>
        `).join('');
      }

      input.addEventListener('input', updateTokenizer);
      updateTokenizer();
    }

    // Sliding Window Demo
    function initWindowDemo() {
      const textInput = document.getElementById('window-input');
      const contextSlider = document.getElementById('context-slider');
      const contextDisplay = document.getElementById('context-display');
      const positionSlider = document.getElementById('position-slider');
      const positionDisplay = document.getElementById('position-display');
      const prevBtn = document.getElementById('prev-btn');
      const nextBtn = document.getElementById('next-btn');
      const tapeDisplay = document.getElementById('tape-display');
      const contextText = document.getElementById('context-text');
      const contextIds = document.getElementById('context-ids');
      const targetText = document.getElementById('target-text');
      const targetId = document.getElementById('target-id');

      function updateWindow() {
        const text = textInput.value;
        const contextLength = parseInt(contextSlider.value);
        const position = parseInt(positionSlider.value);

        // Build vocab
        const chars = [...new Set(text)].sort();
        const stoi = {};
        chars.forEach((ch, i) => stoi[ch] = i);

        // Update sliders
        contextDisplay.textContent = contextLength;
        const maxPos = Math.max(0, text.length - 1);
        positionSlider.max = maxPos;
        positionDisplay.textContent = position;

        // Update buttons
        prevBtn.disabled = position <= 0;
        nextBtn.disabled = position >= maxPos;

        // Update tape
        tapeDisplay.innerHTML = [...text].map((ch, i) => {
          const isContext = i >= position && i < position + contextLength && i < text.length - 1;
          const isTarget = i === Math.min(position + contextLength, text.length - 1) && position + contextLength <= text.length - 1;
          const actualIsTarget = i === position + contextLength && i < text.length;

          let classes = 'tape-cell';
          if (i >= position && i < position + contextLength && i < text.length - 1) classes += ' in-context';
          if (i === position + contextLength && i < text.length) classes += ' is-target';

          return `
            <div class="${classes}">
              <span class="tape-cell-char">${ch === ' ' ? '‚ê£' : ch}</span>
              <span class="tape-cell-id">${stoi[ch]}</span>
            </div>
          `;
        }).join('');

        // Update context/target display
        const ctxEnd = Math.min(position + contextLength, text.length - 1);
        const ctx = text.slice(position, ctxEnd);
        const tgt = text[ctxEnd] || '';

        if (ctxEnd < text.length && ctx.length > 0) {
          contextText.textContent = ctx.replace(/ /g, '‚ê£');
          contextIds.textContent = `[${[...ctx].map(ch => stoi[ch]).join(', ')}]`;
          targetText.textContent = tgt === ' ' ? '‚ê£' : tgt;
          targetId.textContent = stoi[tgt] !== undefined ? stoi[tgt] : '‚Äî';
        } else {
          contextText.textContent = '‚Äî';
          contextIds.textContent = '‚Äî';
          targetText.textContent = '‚Äî';
          targetId.textContent = '‚Äî';
        }
      }

      textInput.addEventListener('input', updateWindow);
      contextSlider.addEventListener('input', updateWindow);
      positionSlider.addEventListener('input', updateWindow);
      prevBtn.addEventListener('click', () => {
        positionSlider.value = Math.max(0, parseInt(positionSlider.value) - 1);
        updateWindow();
      });
      nextBtn.addEventListener('click', () => {
        positionSlider.value = Math.min(parseInt(positionSlider.max), parseInt(positionSlider.value) + 1);
        updateWindow();
      });

      updateWindow();
    }

    // Hint/Solution toggles
    function toggleHint(btn) {
      const content = btn.nextElementSibling;
      content.classList.toggle('show');
      btn.textContent = content.classList.contains('show') ? 'Hide Hint' : 'Show Hint';
    }

    function toggleSolution(btn) {
      const content = btn.nextElementSibling;
      content.classList.toggle('show');
      btn.textContent = content.classList.contains('show') ? 'Hide Solution' : 'Show Solution';
    }

    // Corridor Demo
    function initCorridorDemo() {
      const input = document.getElementById('corridor-input');
      const viz = document.getElementById('corridor-viz');
      const formula = document.getElementById('corridor-formula');

      // Simulated conditional probabilities (illustrative)
      // In practice these would come from corpus statistics
      function getConditionalProb(context, nextChar) {
        // Some illustrative probabilities to show narrowing
        const probs = {
          '': { 'c': 0.08, 'a': 0.08, 't': 0.09, 's': 0.06, 'h': 0.06, 'e': 0.13, default: 0.04 },
          'c': { 'a': 0.35, 'o': 0.25, 'h': 0.15, default: 0.05 },
          'ca': { 't': 0.45, 'n': 0.20, 'r': 0.15, default: 0.05 },
          'cat': { ' ': 0.60, 's': 0.15, default: 0.05 },
          'h': { 'e': 0.40, 'a': 0.20, 'i': 0.15, default: 0.05 },
          'he': { 'l': 0.25, 'r': 0.20, 'a': 0.15, default: 0.08 },
          'hel': { 'l': 0.50, 'p': 0.25, default: 0.05 },
          'hell': { 'o': 0.55, ' ': 0.20, default: 0.05 },
        };
        const contextProbs = probs[context] || probs[''];
        return contextProbs[nextChar] || contextProbs.default || 0.04;
      }

      function updateCorridor() {
        const text = input.value.toLowerCase().slice(0, 6);

        // Build corridor rows
        let html = `
          <div class="corridor-row">
            <div class="corridor-bar-container">
              <div class="corridor-bar" style="width: 100%;"></div>
            </div>
            <div class="corridor-label">
              <span class="corridor-text">All possible texts</span>
              <span class="corridor-prob">100%</span>
            </div>
          </div>
        `;

        let cumulativeProb = 1.0;
        let formulaParts = [];

        for (let i = 0; i < text.length; i++) {
          const context = text.slice(0, i);
          const char = text[i];
          const condProb = getConditionalProb(context, char);
          cumulativeProb *= condProb;

          const widthPercent = Math.max(cumulativeProb * 100, 0.5);
          const stepPercent = (condProb * 100).toFixed(0);

          // Plain English: "8% start with c", "35% of those continue with a"
          formulaParts.push(`${stepPercent}%`);

          html += `
            <div class="corridor-row">
              <div class="corridor-bar-container">
                <div class="corridor-bar" style="width: ${widthPercent}%;"></div>
              </div>
              <div class="corridor-label">
                <span class="corridor-text">Texts starting with "${text.slice(0, i + 1)}"</span>
                <span class="corridor-prob">${(cumulativeProb * 100).toFixed(2)}%</span>
              </div>
            </div>
          `;
        }

        viz.innerHTML = html;

        // Update formula - plain multiplication, no P(x|y) notation
        if (text.length === 0) {
          formula.innerHTML = '<span class="formula-result">Type to see the corridor narrow...</span>';
        } else {
          const formulaStr = formulaParts.map(p => `<span class="prob-term">${p}</span>`).join(' √ó ');
          const finalPercent = (cumulativeProb * 100).toFixed(2);
          formula.innerHTML = `
            <span class="formula-result">
              ${formulaStr} <span class="prob-equals">=</span> <span class="prob-final">${finalPercent}% of all texts</span>
            </span>
          `;
        }
      }

      input.addEventListener('input', updateCorridor);
      updateCorridor();
    }

    // Initialize demos
    document.addEventListener('DOMContentLoaded', () => {
      initTokenizerDemo();
      initWindowDemo();
      initCorridorDemo();
    });
  </script>
</body>
</html>
