{"id":"babygpt-04s","title":"1.1.5 Gap: Why telescoping cancellation works conceptually","description":"Line ~463-465: Cancellation is SHOWN but not EXPLAINED. Is it luck? Feature of decomposition? FIX: Add explanation that telescoping works because we're decomposing a joint probability into a product of conditionals. Each P(x_i | x_\u003ci) appears once in numerator (for position i) and once in denominator (as part of x_\u003ci+1 context). The structure guarantees cancellation — it's not coincidence.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:08.603807-05:00","updated_at":"2025-12-20T21:05:37.442196-05:00","closed_at":"2025-12-20T21:05:37.442196-05:00","close_reason":"FIXED: Points at actual numbers in example - 'Look at the 5: denominator of 5/6 and numerator of 4/5. Both are C(the).' Shows WHY they cancel, not just that they do."}
{"id":"babygpt-0gn","title":"2.3 Gap: Why THIS definition of similarity","description":"Line ~364-367: 'Close if they predict similar next characters' presented as obvious. WHY this definition? Why not semantic similarity? FIX: Add justification: we're building a LANGUAGE MODEL, not a thesaurus. The model's job is to predict next tokens. Two tokens are 'the same' for our purposes if they make the same predictions. Semantic similarity is irrelevant — predictive role is everything.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:27.539968-05:00","updated_at":"2025-12-20T21:29:45.991283-05:00","closed_at":"2025-12-20T21:29:45.991283-05:00","close_reason":"FIXED: Added justification in Section 2.3 explaining why similarity is defined by predictive role (language model purpose) rather than semantic similarity. Brief, first-principles explanation pointing at the model's job."}
{"id":"babygpt-0jb","title":"2.4 Gap: HOW reusability solves capacity","description":"Line ~546-548: Claims 27^8 contexts can't be stored but embeddings work because 'reusable'. HOW? Math not shown. FIX: Add explicit math. Lookup table: 27^8 × 27 = 282 trillion params. Embedding approach: 27 × D embeddings + D × 27 output weights. For D=64: 27×64 + 64×27 = 3,456 params. That's 80 billion times smaller. The compression comes from SHARING embeddings across contexts.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:00.813514-05:00","updated_at":"2025-12-20T21:39:52.578793-05:00","closed_at":"2025-12-20T21:39:52.578793-05:00","close_reason":"Fixed: Added explicit math - 282 trillion vs 3,456 params"}
{"id":"babygpt-0ky","title":"2.13 Validate Ch2 invariant: Softmax converts logits to probs","description":"AUDIT: Read section 2.7, verify softmax formula and role (converts logits to probs that sum to 1). IF missing: add formula. IF role unclear: add explicit 'why softmax?' callout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:20.801913-05:00","updated_at":"2025-12-20T19:47:18.814628-05:00","closed_at":"2025-12-20T19:47:18.814628-05:00","close_reason":"Closed"}
{"id":"babygpt-0la","title":"1.4 Validate Ch1 map waypoint 1.3.2 Free Lunch","description":"AUDIT: Navigate to 1.3.2, verify 'One block of text becomes thousands of training targets' is accurate. IF mismatch: update description. Check if the 'free lunch' concept is actually explained.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:23.742655-05:00","updated_at":"2025-12-20T19:31:19.283738-05:00","closed_at":"2025-12-20T19:31:19.283738-05:00","close_reason":"Closed"}
{"id":"babygpt-0ok","title":"PHASE 2: Invariant Validation","description":"Validate all chapter invariants are properly established in content (14 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:58.324938-05:00","updated_at":"2025-12-20T20:30:20.161825-05:00","closed_at":"2025-12-20T20:30:20.161825-05:00","close_reason":"All invariant validation issues resolved. Invariants properly established and consistently referenced across both chapters."}
{"id":"babygpt-0q9","title":"8.5 Gap audit: perplexity","description":"AUDIT: Perplexity appears in 1.1.7.2 and 1.3.1. Verify defined before first use with formula. IF gap: create ticket to add perplexity definition section.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:47.075429-05:00","updated_at":"2025-12-20T19:56:22.071385-05:00","closed_at":"2025-12-20T19:56:22.071385-05:00","close_reason":"Closed","comments":[{"id":11,"issue_id":"babygpt-0q9","author":"andrewlouis","text":"NEEDS_FIX: Perplexity first used at line 757 (section 1.1.7.1) without definition. Formula appears later at line 903 in Exercise 1.1. Need to add perplexity definition (perplexity = 2^H where H is cross-entropy) before line 757, or add a forward reference callout.","created_at":"2025-12-21T00:48:19Z"}]}
{"id":"babygpt-0scq","title":"ExplosionDemo (Generalization Wall): Redesign layout to use less space + improve readability (Screenshot 2025-12-21 15.50.01)","description":"Context\n- ExplosionDemo renders the \"Generalization Wall\" VizCard (Fig style) showing how |V|^T explodes vs a fixed dataset size.\n- Screenshot 2025-12-21 15.50.01: the current layout feels large for the information density, and looks less elegant than other vizzes.\n- We already have a separate issue for line-breaks in numeric lines (babygpt-p09k). This issue is about overall layout and density, not the specific line wrap bug.\n\nWhere\n- Component: src/components/ExplosionDemo.tsx\n- Styles: src/components/ExplosionDemo.module.css\n\nTask\n1) Redesign the visual layout so it is more compact and premium.\n   - Reduce vertical height.\n   - Keep the same core information: slider, T value, possibilities, data size, coverage state, and the crossover bar.\n   - Prefer a tighter grid and stronger alignment.\n2) Make the \"Possibilities\" math line readable without relying on enormous horizontal space.\n3) Keep the Show expansion details block, but make it feel optional and unobtrusive.\n4) Coordinate with babygpt-p09k: do not reintroduce line-wrapping on key numeric expressions.\n\nAcceptance\n- The Generalization Wall feels visually on-par with other vizzes (clean, compact, readable).\n- Still understandable at a glance (what changes with T, and why we hit the wall).\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T19:33:36.261899-05:00","updated_at":"2025-12-22T17:51:03.957863-05:00","closed_at":"2025-12-22T17:51:03.957863-05:00","close_reason":"ExplosionDemo (Generalization Wall) is already redesigned into a compact two-column layout with aligned controls/metrics; expansion is kept optional and does not line-wrap key numbers."}
{"id":"babygpt-0w0","title":"CSS: Consolidate border-radius to design tokens","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:16.703776-05:00","updated_at":"2025-12-20T23:32:31.882141-05:00","closed_at":"2025-12-20T23:32:31.882141-05:00","close_reason":"Closed"}
{"id":"babygpt-167","title":"NeuralTrainingDemo: Polish sweep","description":"Audit NeuralTrainingDemo for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (training animation, weight updates)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:07.042608-05:00","updated_at":"2025-12-20T22:54:58.05393-05:00","closed_at":"2025-12-20T22:54:58.05393-05:00","close_reason":"Wrong - this IS a gold standard (Dec 19), not needing sweep"}
{"id":"babygpt-1ko4","title":"WorkedExample: soften inline code chip styling (Screenshot 2025-12-21 20.46.54)","description":"User feedback: inline \u003ccode\u003e pills inside WorkedExample feel visually 'ill fitting' against the WorkedExample glass background (Screenshot 2025-12-21 20.46.54, hash-order worked example).\\n\\nGoal: within WorkedExample blocks, inline code should look calmer / more integrated (less heavy background + shadow), while still clearly 'code'.\\n\\nImplementation: adjust src/components/WorkedExample.module.css to override global inline code styling for code descendants of .workedExample (reduce/remove box-shadow, use subtler background + border, keep cyan text). Ensure code blocks (\u003cpre\u003e\u003ccode\u003e) remain unstyled (inherit existing pre rule behavior).\\n\\nAcceptance: hash-order worked example looks consistent with surrounding card; build passes (npm run build).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:25:40.20134-05:00","updated_at":"2025-12-21T21:27:53.640133-05:00","closed_at":"2025-12-21T21:27:53.640133-05:00","close_reason":"Overrode global inline \u003ccode\u003e styling within WorkedExample to reduce heavy background/shadow; kept code blocks unaffected; verified build."}
{"id":"babygpt-1l3","title":"Gradient Tugs: Add Karpathy-style code walkthrough","description":"GradientTraceDemo: Two-panel (code left, output right). Tiny example: vocab=[q,u,a], dim=4, context=q, target=u. 8 steps: lookup → scores → softmax → loss → centroid → gradient → update → verify. Show actual numbers. Controls: Next/Prev/Reset. Key: watch E[q] move toward E[u] with real coordinates.","status":"closed","issue_type":"feature","created_at":"2025-12-20T22:13:13.851354-05:00","updated_at":"2025-12-20T22:28:05.909701-05:00","closed_at":"2025-12-20T22:28:05.909701-05:00","close_reason":"Implemented GradientTraceDemo: 8-step Karpathy-style code walkthrough showing actual numbers (q-u-a vocab, 4D embeddings). Each step shows code and computed output. Demonstrates lookup, dot product scores, softmax, loss, centroid, gradient, update, and verification that P(u) increased."}
{"id":"babygpt-1oup","title":"Ch2 §2.1: machine preview diagram (3 arrows) immediately after 'locations not labels'","description":"Context\n- This is a direct follow-on to the Grassmann cold-open risk: after we say tokens should be treated as coordinates, we need an immediate, concrete preview of the actual embedding LM pipeline.\n- This issue exists even if the longer preview is added elsewhere: it must happen *right after* the coordinates punchline, or we lose impatient readers.\n\nWhere\n- `src/chapters/Chapter2.tsx` → Section 2.1, immediately after the first paragraph that lands the coordinates/locations punchline.\n\nTask\n- Add a compact, diagram-like block (no screenshots needed; build it in HTML/CSS) showing:\n  1) `context id` → `embedding lookup` (row selection)\n  2) `embedding` → `logits` (linear layer)\n  3) `logits` → `softmax` → `P(next)`\n- Keep it as a \"30-second\" preview: 6–12 lines total.\n- Make it visually consistent with the rest of Chapter 2 (use existing surfaces/typography; don't invent a new component family).\n\nAcceptance criteria\n- Readers see the machine before the deeper justification.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T20:42:11.633195-05:00","updated_at":"2025-12-23T20:46:04.716874-05:00","closed_at":"2025-12-23T20:46:04.716874-05:00","close_reason":"Duplicate/superseded by babygpt-27d4 (same machine preview requirement, more detailed)"}
{"id":"babygpt-1qr","title":"2.3 Gap: Why dot product, not Euclidean or KL","description":"Line ~380-382: Dot product presented as THE similarity metric. Why not Euclidean distance or KL divergence? FIX: Add Callout explaining the choice. Dot product: (1) differentiable, (2) distributes over addition (crucial for gradients), (3) computation is just multiply-accumulate (fast on GPUs). Euclidean works but gradients are messier. KL requires logs. Dot product is the engineering sweet spot.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:34.701876-05:00","updated_at":"2025-12-20T21:30:53.858216-05:00","closed_at":"2025-12-20T21:30:53.858216-05:00","close_reason":"FIXED: Added engineering justification for dot product in Section 2.6. Explains three constraints: (1) differentiability (clean gradients), (2) distributes over addition (crucial for gradient flow/attention), (3) GPU-efficient (multiply-accumulate primitive). Addresses why not Euclidean (messier gradients, doesn't compose) or KL divergence (requires logs)."}
{"id":"babygpt-1vk7","title":"Math: Add inline KaTeX for exponents/fractions (stop using caret like |V|^T in \u003cTerm\u003e)","description":"Context\n- We currently write inline math using plain text inside \u003cTerm\u003e (e.g., `|V|^T`, `50,000^{10}`, `1/27`).\n- This renders ugly caret/fraction text instead of proper typesetting.\n\nWhere (examples to fix)\n- src/chapters/Chapter1.tsx\n  - Find: “The number of length‑T sequences is |V|^T”\n  - Find: “50,000^{10}”\n- src/chapters/Chapter2.tsx\n  - Find: “For V = 27, the baseline is 1/27 ≈ 0.037.”\n\nWhat to do\n1) Create a small reusable inline math component, e.g.:\n- src/components/MathInline.tsx\n- Render KaTeX with displayMode: false.\n- Accept a string prop like `equation`.\n- Export from src/components/index.ts.\n\n2) Replace the worst offender inline expressions in Chapter 1 and Chapter 2.\n- Replace \u003cTerm\u003e|V|^T\u003c/Term\u003e with \u003cMathInline equation={String.raw`|V|^T`} /\u003e (or similar).\n- Replace inline fractions like 1/27 with \\frac{1}{27}.\n- Do not over-convert code examples; only convert mathematical prose.\n\n3) Styling\n- Inline math should match surrounding font size/line height.\n- Use the same cyan accent as MathBlock where appropriate.\n\nDefinition of done\n- Inline math renders with proper superscripts/fractions.\n- No caret characters in the rendered UI for these expressions.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:37.925555-05:00","updated_at":"2025-12-21T19:02:07.396375-05:00","closed_at":"2025-12-21T19:02:07.396375-05:00","close_reason":"Added MathInline (KaTeX inline) and replaced caret/scratch math in Ch1/Ch2; build passes"}
{"id":"babygpt-20f","title":"5.4 Validate SectionLinks in Ch2 exercises","description":"AUDIT: Lines 1804, 1825, 1836, 1876 in Ch2 exercises. Verify each SectionLink to 2.2, 2.3, 2.6 points to correct section. IF any broken: fix. IF any inaccurate: update link text.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:40.221003-05:00","updated_at":"2025-12-20T20:26:48.405294-05:00","closed_at":"2025-12-20T20:26:48.405294-05:00","close_reason":"FIXED: Corrected 2 SectionLinks in Ch2 exercises (lines 1845, 1856) from 2.2 to 2.3 - the CharacterClusterViz (similarity panel, corpus editor) is in Section 2.3","comments":[{"id":13,"issue_id":"babygpt-20f","author":"andrewlouis","text":"NEEDS_FIX: Line 1825 has incorrect SectionLink - says 'cosine similarity (in the Section 2.2 panel)' but CharacterClusterViz (which shows cosine similarity) is in Section 2.3, not 2.2. Should be: 'cosine similarity (in the \u003cSectionLink to=\"2.3\"\u003eSection 2.3\u003c/SectionLink\u003e panel)'. Other SectionLinks at lines 1804, 1825 (second one to 2.6), and 1876 are correct.","created_at":"2025-12-21T01:03:52Z"}]}
{"id":"babygpt-27d4","title":"Ch2 §2.1: add a concrete 10-line 'machine preview' right after the map/coordinates punchline","description":"Context\n- Chapter 2 opens with a beautiful Grassmann/coordinates story.\n- Risk: it is conceptually heavy before the reader has seen a concrete embedding model. Some readers will feel like the chapter is \"dodging the job\".\n- Minimal fix: right after the first strong punchline (\"tokens aren't labels, they're locations\" / \"we need coordinates not hash addresses\"), immediately cash it out into a brutally concrete preview of the machine we're about to build.\n\nWhere\n- `src/chapters/Chapter2.tsx` → Section 2.1 (Grassmann's Insight). Find the first moment where we state the punchline about coordinates/locations.\n\nWhat to build\n1) Insert a compact preview box (use existing components; do not invent a new design language):\n   - Prefer `Callout variant=\"info\"` OR a simple `div` using `.panel-dark` / `.inset-box` inside the section.\n2) Show the minimal forward pass as a 3–4 line pipeline with arrows. Something like:\n   - `context token id` → `embedding lookup` → `logits` → `softmax` → `P(next token)`\n3) Make it mechanically explicit, but keep it short:\n   - Example:\n     - `i` → `e_i = E[i]`\n     - `ℓ = e_i W_out + b`\n     - `P(next) = softmax(ℓ)`\n4) Make the text honest:\n   - This is the simplest version (bigram-style) and we'll later talk about how to handle multi-token contexts.\n\nAcceptance criteria\n- Within ~10 lines, a reader can say: \"Oh, *this* is the machine Chapter 2 is building.\" \n- The Grassmann story now feels like justification of a concrete move, not runway without takeoff.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T20:36:56.336669-05:00","updated_at":"2025-12-24T21:25:34.234208-05:00","closed_at":"2025-12-24T21:25:34.234208-05:00","close_reason":"Added a concrete 5-step embedding-LM preview box immediately after the 'tokens are locations' punchline; build passes"}
{"id":"babygpt-2a3","title":"PHASE 11: Explanatory Gap Remediation","status":"closed","priority":1,"issue_type":"epic","created_at":"2025-12-20T20:47:16.994855-05:00","updated_at":"2025-12-21T17:54:51.984024-05:00","closed_at":"2025-12-21T17:54:51.984024-05:00","close_reason":"All Phase 11 remediation child beads completed and shipped"}
{"id":"babygpt-2a3.1","title":"Ch2: Add visible recap + stronger section-to-section bridges (no hidden arc list)","description":"User feedback: Chapter 2 needs a clearer recap of what we've learned so far *and* how each section built on the previous one. Right now the recap list is mostly hidden behind the collapsible \u003cdetails\u003e in Section 2.9 (\"Optional: the Chapter 2 arc (one list)\").\n\nTarget file: src/chapters/Chapter2.tsx\nPrimary location: \u003cSection number=\"2.9\" title=\"Synthesis: From Counts to Coordinates\"\u003e\n\nWhat to do:\n- Replace the collapsed/optional recap with a short, always-visible recap (e.g. an \u003col\u003e or \u003cul\u003e) covering 2.1 → 2.8 in 6–8 bullets.\n- Each bullet must explicitly state:\n  1) what that section contributed, and\n  2) why it needed the prior step (dependency language like “We needed X before Y because…”).\n- Keep the voice humble + plain. Avoid mic-drop phrasing and avoid the rhetorical pattern “not X, but Y.”\n- Keep it tight: 1–2 sentences per bullet.\n- Keep the existing Invariants block, but ensure the reader gets orientation even if they don’t click anything.\n\nNon-goals:\n- Don’t rewrite the whole chapter. This is about orientation/bridging and removing the feeling of “teleporting” between concepts.\n","acceptance_criteria":"Section 2.9 contains a visible recap (not behind \u003cdetails\u003e) that links 2.1→2.8 with clear dependencies; reader can skim in ~30s and stay oriented.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:16:36.306078-05:00","updated_at":"2025-12-21T15:36:26.283754-05:00","closed_at":"2025-12-21T15:36:26.283754-05:00","close_reason":"Made Ch2 Section 2.9 recap visible (removed collapsible arc list) and rewrote bullets to explicitly state dependencies between 2.1–2.8; build passes.","labels":["chapter2","copy","structure"],"dependencies":[{"issue_id":"babygpt-2a3.1","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:16:36.307689-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.10","title":"Ch1 KenLM section: integrate 'words vs characters' note into core prose (remove callout styling)","description":"User feedback (Screenshot 2025-12-21 16:47:38): The 'Does KenLM use words or characters?' callout feels like core content and may be better as regular prose rather than a big boxed Callout.\n\nTarget file\n- src/chapters/Chapter1.tsx\n\nCurrent location\n- Section 1.1.7.1 \"The Hidden Graph \u0026 The Speed Limit\"\n- Search for: \u003cCallout variant=\"info\" title=\"Does KenLM use words or characters?\"\u003e\n\nWhat to do\n- Replace the Callout with 1–2 normal \u003cParagraph\u003e blocks integrated into the surrounding narrative.\n- Keep the key points:\n  - KenLM is token-agnostic; commonly word-level in classic usage.\n  - This chapter sometimes uses words for readability and characters for small vocab; same math.\n- Make the transition into the following content smooth (no abrupt jump).\n\nConstraints\n- Keep it short; this is a clarification, not a sidebar.\n- Avoid adding new jargon.\n\nDefinition of done\n- The section reads better and feels less 'boxy'.\n- npm run build passes.\n","acceptance_criteria":"- The 'Does KenLM use words or characters?' content appears as normal Paragraph content (not a Callout) near the KenLM intro.\\n- Flow reads naturally without breaking with a big box.\\n- No loss of information.\\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:02:34.329496-05:00","updated_at":"2025-12-21T17:04:32.429484-05:00","closed_at":"2025-12-21T17:04:32.429484-05:00","close_reason":"Integrated KenLM 'words vs characters' clarification into core prose (removed info callout box) in Section 1.1.7.1; build passes.","labels":["chapter1","copy","structure"],"dependencies":[{"issue_id":"babygpt-2a3.10","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T17:02:34.332563-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.11","title":"Ch1 KenLM hashing: add first-principles explanation + toy hash walkthrough (order sensitivity)","description":"User feedback (Screenshot 2025-12-21 16:48:17): The hashing section should go deeper. The line “mix the bits of every character based on its position” needs a first-principles explanation and a concrete example.\n\nTarget file\n- src/chapters/Chapter1.tsx\n\nCurrent location\n- Section 1.1.7.1, right after \u003cKenLMDemo /\u003e under the heading “Wait, doesn't hashing lose the order?”\n- Currently: one paragraph + a MathBlock “Hash('dog sat') ≠ Hash('sat dog')” and then moves on.\n\nWhat to do\n- Expand this into a short, concrete walkthrough that shows *how* order sensitivity happens.\n- Include a toy hash function (very small) and step through 2 short strings with the same characters in different order.\n\nSuggested structure\n1) Keep the existing intuition question.\n2) Add a WorkedExample or CodeWalkthrough demonstrating a toy rolling hash:\n   - Start with h = 0\n   - For each character: h = (h * B + code(ch)) mod M\n   - Show “ab” vs “ba” (or “dog” vs “god”) produce different h because earlier characters get multiplied (different 'place value').\n3) Briefly connect this back to real hashes (MurmurHash/CityHash) without trying to teach their internals.\n\nTone constraints\n- Humble, clear; no mic-drop phrasing.\n- Avoid deep hash jargon; keep it accessible.\n\nDefinition of done\n- Reader can explain *why* swapping order changes the hash, not just memorize the statement.\n- npm run build passes.\n","acceptance_criteria":"- Section 1.1.7.1 expands the 'hash is order-sensitive' claim with a concrete toy example showing different intermediate states for swapped order.\\n- Uses simple, readable math/code (no heavy jargon), tied back to KenLM context.\\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T17:07:44.710068-05:00","updated_at":"2025-12-21T17:09:19.397754-05:00","closed_at":"2025-12-21T17:09:19.397754-05:00","close_reason":"Expanded KenLM hashing section with a first-principles toy rolling-hash WorkedExample showing why order changes the result; kept ties to real hashes; build passes.","labels":["chapter1","copy","kenlm"],"dependencies":[{"issue_id":"babygpt-2a3.11","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T17:07:44.714672-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.12","title":"Ch1: fix DecoderDemo reference (demo below) + reorder viz under paragraph","description":"User request: Update the Beam Search paragraph to say “The demo below” (not “above”) and move the DecoderDemo component so it appears after that paragraph.\n\nTarget file\n- src/chapters/Chapter1.tsx\n\nCurrent location\n- Section 1.1.7.1 around the Decoder paragraphs:\n  - Currently \u003cDecoderDemo /\u003e renders before the paragraph that says “The demo above shows a Beam Search…”.\n\nWhat to change\n- Move \u003cDecoderDemo /\u003e to render after the paragraph.\n- Update the paragraph to say “The demo below…”\n\nDefinition of done\n- Layout matches the wording.\n- npm run build passes.\n","acceptance_criteria":"- Paragraph says 'The demo below' (not above).\\n- \u003cDecoderDemo /\u003e appears directly after that paragraph.\\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:13:09.715187-05:00","updated_at":"2025-12-21T17:14:43.8487-05:00","closed_at":"2025-12-21T17:14:43.8487-05:00","close_reason":"Changed Beam Search paragraph to 'demo below' and moved \u003cDecoderDemo /\u003e to immediately follow it; build passes.","labels":["chapter1","copy","layout"],"dependencies":[{"issue_id":"babygpt-2a3.12","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T17:13:09.721426-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.13","title":"Ch1 surprise examples: render 1/2, 1/4, 1/8 as true fractions (KaTeX)","description":"User feedback (Screenshot 2025-12-21 16:50:22): Fractions like 1/2, 1/4, 1/8 are currently rendered as plain text (slash) and look ugly/inconsistent with other LaTeX-rendered math.\n\nTarget file\n- src/chapters/Chapter1.tsx\n\nCurrent location\n- Section 1.1.7.1 around the lead-in to the surprise formula.\n- Search for: \"p = 1/2 feels like\" (currently uses \u003cTerm\u003ep = 1/2\u003c/Term\u003e etc).\n\nWhat to do\n- Replace the inline slash fractions with a KaTeX-rendered expression using \\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}.\n- Prefer a compact MathBlock that shows the three calibrations, then keep the existing general formula MathBlock below.\n\nDefinition of done\n- Fractions render as true fractions and match surrounding math styling.\n- npm run build passes.\n","acceptance_criteria":"- The 'p=1/2, 1/4, 1/8' examples render as stacked fractions (not slash text).\\n- Uses consistent KaTeX/MathBlock styling with surrounding math.\\n- No layout regressions.\\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T17:19:15.420452-05:00","updated_at":"2025-12-21T17:24:18.66286-05:00","closed_at":"2025-12-21T17:24:18.66286-05:00","close_reason":"Replace slash fractions with KaTeX \\frac{} examples; build passes","labels":["chapter1","math","typesetting"],"dependencies":[{"issue_id":"babygpt-2a3.13","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T17:19:15.426049-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.2","title":"Ch2: Add strong exercise motivation + 'what code you will write' preface","description":"User feedback: Chapter 2 exercises currently start abruptly. Add a proper motivational ramp that explains (1) why these exercises exist, (2) what skill they’re testing, and (3) what code the reader should be able to write after finishing them.\n\nTarget file: src/chapters/Chapter2.tsx\nLocation: \u003cSection number=\"2.11\" title=\"Exercises\"\u003e — insert content BEFORE Exercise 2.1.\n\nWhat to add:\n- 2–4 short Paragraphs that connect Chapter 2 content to the exercises.\n- A concrete “What you’ll be able to implement after this” list (bullets are fine). Suggested items:\n  - embedding lookup (row selection / E[ix])\n  - dot product as similarity score\n  - softmax to turn scores into probabilities\n  - cross-entropy / negative log-prob as loss\n  - one training step nudging embeddings based on error (high-level; no calculus)\n- A sentence explaining why this matters for the next chapter (context window widening / combining multiple token vectors).\n\nTone constraints:\n- Humble, precise, non-preachy.\n- Avoid minimizing language like “this is easy / just memorize / no magic.”\n\nNon-goals:\n- Don’t change the exercises themselves unless needed for consistency with the new preface.\n","acceptance_criteria":"Section 2.11 starts with a short motivation + explicit 'by the end you can implement X' list; exercises feel purposeful and tied to upcoming code.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:17:00.186047-05:00","updated_at":"2025-12-21T15:36:58.069514-05:00","closed_at":"2025-12-21T15:36:58.069514-05:00","close_reason":"Added Exercises preface in Ch2 (purpose + pipeline + concrete 'what you can implement' list + forward pointer); build passes.","labels":["chapter2","copy","exercises"],"dependencies":[{"issue_id":"babygpt-2a3.2","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:17:00.191387-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.3","title":"Ch1: Rewrite the post-CorridorDemo 'multiplication keeps track' sentence for precision","description":"User feedback: This sentence is currently vague / feels like hand-waving:\n  \"It can look like a formula at first. It's just the bookkeeping for 'of those, of those, of those.' Once you see the corridor narrowing, multiplication is what keeps track.\"\n\nTarget file: src/chapters/Chapter1.tsx\nLocation: around the CorridorDemo in Section 1.1 (search for \"Once you see\\n          the corridor narrowing\"). Current line numbers ~460–461.\n\nWhat to do:\n- Replace that paragraph with a clearer, first-principles statement that explicitly ties the corridor metaphor to the chain rule product.\n- Use concrete language: each factor is “given we survived the first k tokens, what fraction survives one more step?”; multiplying factors yields the final fraction of worlds/texts.\n- Avoid vague meta-language (“bookkeeping”) as the main message.\n- Keep it short (1–2 Paragraphs). The longer “filter worlds” explanation that follows can stay, but ensure there is no redundancy.\n\nOptional improvement:\n- If redundancy is high, merge/trim adjacent paragraphs so the reader gets one clean explanation, not three near-duplicates.\n\nTone constraints:\n- Humble and precise; no mic-drop phrasing.\n","acceptance_criteria":"The paragraph after \u003cCorridorDemo /\u003e states precisely what multiplication is tracking (conditional survival fractions / chain rule) without vague 'bookkeeping' language.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:17:23.705458-05:00","updated_at":"2025-12-21T15:36:10.434906-05:00","closed_at":"2025-12-21T15:36:10.434906-05:00","close_reason":"Replaced vague 'multiplication keeps track' line after CorridorDemo with explicit survival-fraction/chain-rule wording; smoothed into worlds-filtering example; build passes.","labels":["chapter1","copy","probability"],"dependencies":[{"issue_id":"babygpt-2a3.3","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:17:23.710391-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.4","title":"Ch1: Strengthen first-principles explanation of encode/decode (beyond 'agreed mapping')","description":"User feedback: The current encode/decode intro is too weak first-principles:\n  \"The word code just means “an agreed mapping.” In this chapter, our codebook is two tiny dictionaries: stoi and itos.\"\n\nTarget file: src/chapters/Chapter1.tsx\nLocation: \u003cSection number=\"1.2.2\" title=\"Encoding and Decoding\"\u003e (search \"The word code\"). Current line ~1357.\n\nWhat to do:\n- Rewrite the intro to explain *why* we encode at all:\n  - GPUs/arrays can’t index into matrices with letters/strings; they need integer row IDs.\n  - Encoding turns symbols into stable addresses (0..V-1) so we can store vectors/logits in arrays.\n  - Decoding is the inverse mapping back to readable text.\n- Keep “codebook” language only if it’s made concrete (a bijection between tokens and integer IDs). Avoid airy phrasing.\n- Include one tiny concrete example (2–3 lines of prose): e.g. vocab { 'h':3, 'e':2, 'l':4, 'o':5 } so \"hello\" → [3,2,4,4,5] and back.\n- Keep the existing note that IDs are labels (permutation doesn’t matter), but tighten to avoid repetition.\n\nTone constraints:\n- Humble, plain, non-minimizing.\n","acceptance_criteria":"Section 1.2.2 explains encode/decode as 'addressing rows in arrays' with a tiny concrete example; removes/rewrites the weak 'agreed mapping' line.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:17:46.683694-05:00","updated_at":"2025-12-21T15:35:52.027174-05:00","closed_at":"2025-12-21T15:35:52.027174-05:00","close_reason":"Rewrote 1.2.2 encode/decode as first-principles 'IDs are array addresses' explanation; kept reversible stoi/itos example; build passes.","labels":["chapter1","copy","tokenization"],"dependencies":[{"issue_id":"babygpt-2a3.4","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:17:46.687282-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.5","title":"ExplosionDemo ('Generalization Wall'): redesign to be more compact and information-dense","description":"User feedback (Screenshot 2025-12-21 at 15.50.01): The 'Generalization Wall' viz feels wasteful in vertical space for the amount of information conveyed. We want a tighter, more expressive layout that communicates the combinatorics explosion quickly.\n\nWhere this appears\n- Chapter 1, Section 1.1.3 “Why This Happens”\n- Used via: \u003cExplosionDemo /\u003e in src/chapters/Chapter1.tsx\n\nTarget files\n- src/components/ExplosionDemo.tsx\n- src/components/ExplosionDemo.module.css\n\nCurrent structure (summary)\n- Header (title/subtitle)\n- Slider for context length T (1–10)\n- Equation block showing 27^T expanded as repeated multiplication + 'Your data' fixed to 1,000,000\n- Log-scale bar (oversaturated → sparse) + crossover line\n- Coverage result block + optional 'insight' paragraph (only for T\u003e=6)\n\nWhat to change\n1) Reduce vertical footprint while keeping clarity:\n   - Convert the viz into a 2-column grid on desktop:\n     - Left: slider + key numbers (T, possibilities, data)\n     - Right: the log-scale 'crossover' bar + coverage/result state\n   - On mobile: stack cleanly (no horizontal overflow).\n\n2) Make the math less bulky:\n   - Remove or hide the full exponent chain “27 × 27 × …” by default.\n   - Keep a compact representation like “27^T ≈ 205.9×10^12” and optionally reveal the expanded chain behind a small “Show expansion” toggle or \u003cdetails\u003e.\n\n3) Tighten spacing + hierarchy:\n   - Reduce padding/margins between blocks.\n   - Ensure the most important output (coverage + state label) stays above the fold.\n\n4) Visual polish:\n   - Bring it in line with other premium vizzes (dark glass, cyan/magenta accents, crisp mono labels).\n   - Consider wrapping in VizCard for consistent header/padding (optional but preferred).\n\nNotes\n- Keep existing logic/behavior unless it blocks the redesign.\n- Preserve accessibility: slider label, readable text contrast.\n\nDefinition of done\n- Screenshot comparison shows the viz occupies significantly less height while remaining clearer.\n- npm run build passes.\n","acceptance_criteria":"- ExplosionDemo conveys the same message with noticeably less vertical space (target: ~30–40% shorter at desktop widths).\\n- Layout remains readable and premium (matches BabyGPT viz style).\\n- Mobile layout remains usable.\\n- npm run build passes.","notes":"Additional user screenshot (2025-12-21 16:28): current layout wraps awkwardly: the 'T = 10' readout splits across lines and the long possibilities expansion wraps. Redesign must keep the primary numeric line unbroken (no wrap) and move any long expansion behind a compact toggle or horizontal scroll.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T16:27:05.18175-05:00","updated_at":"2025-12-21T16:38:43.200476-05:00","closed_at":"2025-12-21T16:38:43.200476-05:00","close_reason":"Redesigned ExplosionDemo into compact 2-column VizCard layout; moved long 27×... expansion into a small details toggle w/ horizontal scroll; fixed T readout wrapping; build passes.","labels":["chapter1","design","viz"],"dependencies":[{"issue_id":"babygpt-2a3.5","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T16:27:05.194754-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.6","title":"Ch1 intro: remove premature entropy jargon; reposition Shannon 1.3 bits claim after surprise definition","description":"User feedback: Current Chapter 1 opening paragraphs introduce domain-specific vocabulary too early (entropy, bits per character, 'surprise') and feel like forward-referencing before the concepts are explained.\n\nTarget file\n- src/chapters/Chapter1.tsx\n\nCurrent location\n- Section 1.1 \"The Physics of the Problem\" paragraphs around the Shannon intro:\n  - \"Shannon estimated that English has an entropy around 1.3 bits per character...\"\n  - \"Entropy is a number: the fundamental limit...\"\n\nWhat to do\n- Rewrite the Section 1.1 Shannon intro so it stays intuitive and avoids jargon that hasn’t been defined yet.\n- Move the numeric Shannon estimate (~1.3 bits/character) to Section 1.1.1 AFTER the MathBlock defining surprise = -log2(p).\n- Keep citations block as-is (or keep equivalent references) but ensure the narrative doesn’t require knowing what entropy/bits are before we define them.\n\nTone constraints\n- Humble, clear, quietly playful. Avoid mic-drop or “trust me” language.\n","acceptance_criteria":"- Section 1.1 no longer uses 'entropy'/'bits per character' before surprise/log definition.\\n- Shannon benchmark (1.3 bits/char) appears only after surprise is defined (Section 1.1.1).\\n- Paragraphs still flow cleanly into Section 1.1.1.\\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T16:44:22.010137-05:00","updated_at":"2025-12-21T16:46:16.284741-05:00","closed_at":"2025-12-21T16:46:16.284741-05:00","close_reason":"Removed entropy/bits jargon from Section 1.1 opener; moved Shannon ~1.3 bits/char benchmark to Section 1.1.1 after surprise is defined; build passes.","labels":["chapter1","copy","structure"],"dependencies":[{"issue_id":"babygpt-2a3.6","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T16:44:22.014841-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.7","title":"Ch1: remove/move 'Why can neural networks handle unseen contexts?' collapsible (currently out of place)","description":"User feedback (Screenshot 2025-12-21 16:40): The collapsible “Why can neural networks handle unseen contexts?” feels out of place here, because neural networks are treated as a first-class topic elsewhere. The insertion currently breaks the flow of the FormalRigor block.\n\nCurrent location\n- src/chapters/Chapter1.tsx inside \u003cFormalRigor title=\"Chain Rule: Formal Rigor\"\u003e, near the end of the block.\n- Search for: \u003csummary\u003eWhy can neural networks handle unseen contexts?\u003c/summary\u003e\n\nWhat to do\nOption A (preferred): remove from FormalRigor entirely and move the concept to a better location:\n- Either Section 1.7 “What’s Next” (bridge to Chapter 2), or early Chapter 2, where neural nets are the topic.\n- Keep it short (1–2 Paragraphs). Focus on the conceptual difference: counting divides by counts and hits undefined contexts; neural nets compute a smooth function that always outputs a distribution.\n- Add a forward pointer like “We’ll cash this in in Chapter 2.”\n\nOption B: delete entirely if it’s redundant with nearby content and the doc already explains the transition sufficiently.\n\nConstraints\n- Avoid heavy jargon. Don’t introduce new math.\n- Ensure the narrative flow in the FormalRigor block improves (no jarring pivot).\n\nDefinition of done\n- The FormalRigor block reads cleanly from end to citations.\n- If moved, new placement feels natural and helps the Chapter 1→2 transition.\n- npm run build passes.\n","acceptance_criteria":"- The collapsible 'Why can neural networks handle unseen contexts?' is no longer inside FormalRigor in Section 1.1.5.\\n- If we still want the idea, it appears in a more appropriate location (likely Section 1.7 or Chapter 2) with a short cross-link.\\n- Chapter 1 flow feels less jarring in the chain-rule formal block.\\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T16:49:30.981936-05:00","updated_at":"2025-12-21T16:51:59.865037-05:00","closed_at":"2025-12-21T16:51:59.865037-05:00","close_reason":"Removed the neural-network unseen-contexts collapsible from the Chain Rule FormalRigor block (it was jarring/redundant); flow now goes straight into citations; build passes.","labels":["chapter1","copy","structure"],"dependencies":[{"issue_id":"babygpt-2a3.7","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T16:49:30.98633-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.8","title":"ExplosionDemo: polish alignment + reduce nested-box feel inside VizCard","description":"User feedback (Screenshot 2025-12-21 16:41:48): The redesigned Generalization Wall (ExplosionDemo) isn’t landing as beautifully as it could. It may be an alignment issue, and the nested 'box inside a box' look might be hurting it.\n\nTarget files\n- src/components/ExplosionDemo.tsx\n- src/components/ExplosionDemo.module.css\n\nContext\n- ExplosionDemo was recently refactored into a compact 2-column layout wrapped in VizCard.\n- Inside the VizCard content we currently render two additional panel-like boxes (.left and .right) with their own backgrounds/borders.\n\nWhat to do\n- Revisit visual hierarchy:\n  - Reduce the “nested card” feeling (prefer: one card surface, then subtle separators/dividers).\n  - Check alignment between left and right columns (padding, baseline, label alignment).\n  - Keep the compactness win and the non-wrapping T readout.\n\nDefinition of done\n- The component looks cleaner and more premium (closer to other vizzes), with less visual clutter from multiple borders.\n- npm run build passes.\n","acceptance_criteria":"- Generalization Wall viz feels less 'boxed-in' (no awkward nested-card look).\\n- Left/right columns align cleanly; spacing feels intentional.\\n- No regressions: T readout stays on one line; expansion remains tucked behind toggle.\\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T16:55:48.187835-05:00","updated_at":"2025-12-21T16:57:11.721806-05:00","closed_at":"2025-12-21T16:57:11.721806-05:00","close_reason":"Polished ExplosionDemo: removed inner panel boxes and replaced with subtle column divider + consistent padding; reduces nested-card feel; build passes.","labels":["chapter1","design","viz"],"dependencies":[{"issue_id":"babygpt-2a3.8","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T16:55:48.197544-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.9","title":"KenLMDemo: upgrade 'Visualizing Linear Probing' to interactive query + richer probing path (match Packed Array style)","description":"User feedback: The current “Visualizing Linear Probing” demo is too shallow and not visually consistent with the existing 'Packed Array' memory representation (Screenshot 2025-12-21 16:45). We want a more comprehensive mechanics demo.\n\nWhere\n- Component: src/components/KenLMDemo.tsx (+ KenLMDemo.module.css)\n- Used in: src/chapters/Chapter1.tsx Section 1.1.7.1 (KenLM / linear probing discussion)\n\nCurrent issues\n- Query is hard-coded (\"dog sat\"), so the learner can’t explore.\n- Only shows a single collision then immediate hit; doesn’t show a longer probe chain or a miss (empty slot).\n- Visual representation differs from PointerVsFlat's 'Packed Array' block; we should standardize.\n\nWhat to implement\n1) Interactive query input\n- Add an input where the reader can type a query string (keep it small; e.g. max 20 chars; lowercase).\n- Provide a few preset buttons (e.g., 'dog sat', 'cat sat', 'dog ran', 'cat ran', 'a cat').\n\n2) Deterministic toy hash + probe path\n- Use a simple deterministic hash function for the demo (not crypto; stable in JS) that maps to MEMORY_SIZE.\n- Keep the memory slots fixed (toy table) but compute the starting index from the input.\n- Animate steps:\n  - Step 1: compute hash → index\n  - Step 2+: probe slots sequentially until:\n    - hit (key matches), OR\n    - miss (empty slot), OR\n    - stop after MEMORY_SIZE probes (table full / give up)\n- Show the full probe path (e.g., highlight visited indices 3 → 4 → 5).\n\n3) Stronger example table\n- Pick initial memory contents that can demonstrate:\n  - collision chain length \u003e 2\n  - an example miss (ends on empty)\n  - at least one successful hit\n\n4) Visual style: unify with Packed Array\n- Reuse the same field concepts/visual language as PointerVsFlat (Hash, Prob, Backoff, offsets).\n- The memory map can be a vertical list of slots, but each slot should show:\n  - index\n  - stored key (or hash) and values\n  - stored original hash/index for collisions (optional)\n- Colors: cyan/magenta accents, dark glass, mono labels.\n\nConstraints\n- No new heavy dependencies.\n- Keep component self-contained and readable.\n\nDefinition of done\n- The demo teaches linear probing clearly through interaction.\n- Styling feels consistent with PointerVsFlat's packed array panel.\n- npm run build passes.\n","acceptance_criteria":"- KenLMDemo supports user-typed query and animates/provides a clear probe path for that query.\\n- Example is more comprehensive (demonstrates: hash, collision, multiple probes, miss case).\\n- Visual style aligns with the 'Packed Array' representation used in PointerVsFlat (consistent fields/colors).\\n- Mobile layout remains usable.\\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T17:00:41.505517-05:00","updated_at":"2025-12-21T17:44:19.184834-05:00","closed_at":"2025-12-21T17:44:19.184834-05:00","close_reason":"KenLMDemo now supports interactive query input, step-by-step probe path (hit/collision/miss), and packed-array styling; build passes","labels":["design","interaction","kenlm","viz"],"dependencies":[{"issue_id":"babygpt-2a3.9","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T17:00:41.509822-05:00","created_by":"daemon"}]}
{"id":"babygpt-2ez","title":"1.6 Validate Ch1 map waypoint 1.7 What's Next","description":"AUDIT: Navigate to 1.7, verify 'map not phone book' description. IF mismatch: update. Check if section properly foreshadows Ch2 without premature detail.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:24.327936-05:00","updated_at":"2025-12-20T19:31:19.841105-05:00","closed_at":"2025-12-20T19:31:19.841105-05:00","close_reason":"Closed"}
{"id":"babygpt-2hzh","title":"Ch2 Softmax: add hand-computed worked examples (softmax + temperature)","description":"Context\n- Softmax is a core conceptual bridge in Chapter 2: logits (scores) → probability distribution.\n- We already have interactive widgets (SoftmaxWidget + SoftmaxSimplexViz), but we need a couple of tiny numeric worked examples that a reader can compute by hand.\n- These worked examples are the “ground wire” that makes the visuals feel trustworthy.\n\nWhere\n- File: src/chapters/Chapter2.tsx\n- Section: 2.7 “From Scores to Probabilities (Softmax)”\n- Place the worked examples near the first softmax formula and/or near the temperature definition, before moving on.\n\nTask\n1) Add a WorkedExample: \"Softmax by hand\"\n   - Use a tiny 3-class logit vector (reuse [2, -3, 4] from the narrative).\n   - Show: subtract max → exponentiate → normalize → final probabilities (approx).\n   - Explicitly state that subtracting max does not change probabilities.\n\n2) Add a WorkedExample: \"Temperature sharpens\"\n   - Reuse the same logits but compute the distribution at two temperatures (e.g., T=1 vs T=0.5).\n   - Show (approx) how probabilities concentrate as T decreases.\n   - Include one odds-ratio fact: p_i/p_j = exp((logit_i - logit_j)/T).\n\nConstraints\n- Keep the examples short (2–5 steps each) and readable.\n- Use MathInline for exponentials/fractions when needed.\n\nAcceptance\n- A reader can compute the example with a calculator and match the order of magnitude.\n- Examples directly reinforce the widget intuition.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T20:33:30.188634-05:00","updated_at":"2025-12-21T20:36:00.285847-05:00","closed_at":"2025-12-21T20:36:00.285847-05:00","close_reason":"Added two WorkedExamples in Ch2 §2.7: 'Softmax by hand' (subtract max → exp → normalize) and 'Temperature sharpens' (T=0.5 example + odds ratio p_i/p_j = exp((ℓ_i-ℓ_j)/T)); build passes."}
{"id":"babygpt-2ml","title":"PHASE 8: Gap Detection","description":"Find concepts used before defined (7 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:11:00.094835-05:00","updated_at":"2025-12-20T20:30:36.701205-05:00","closed_at":"2025-12-20T20:30:36.701205-05:00","close_reason":"Gap detection audit complete. Fixed: perplexity definition. Other concepts properly introduced before use."}
{"id":"babygpt-2ox","title":"SoftmaxSimplexViz: Polish sweep","description":"Sweep SoftmaxSimplexViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states, ambient glow?)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Trail animation smoothness\n- Compare against GradientDescentViz/CrossEntropyViz as gold standard","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:51:31.167963-05:00","updated_at":"2025-12-20T22:52:16.12619-05:00","closed_at":"2025-12-20T22:52:16.12619-05:00","close_reason":"Recreating with expanded gold standards"}
{"id":"babygpt-2tf","title":"3.10 KNOWN: Audit 1.1.8 Applying Chain Rule placement","description":"KNOWN ISSUE: Appears after KenLM/Sparsity tangents, may break narrative flow","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:13:54.365264-05:00","updated_at":"2025-12-20T19:41:38.060262-05:00","closed_at":"2025-12-20T19:41:38.060262-05:00","close_reason":"Closed"}
{"id":"babygpt-30e","title":"1.14 Audit Ch2 ChapterMap missing sections","description":"AUDIT: List all 13 Ch2 sections, compare to 6 ChapterMap waypoints. IDENTIFY: critical sections missing (softmax 2.7? synthesis 2.9?). CREATE TICKET to add missing waypoints if navigation UX suffers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:39.002014-05:00","updated_at":"2025-12-20T20:24:36.059618-05:00","closed_at":"2025-12-20T20:24:36.059618-05:00","close_reason":"FIXED: Added missing ChapterMap waypoints for sections 2.5, 2.7, 2.8, 2.9","comments":[{"id":10,"issue_id":"babygpt-30e","author":"andrewlouis","text":"NEEDS_FIX: ChapterMap has 6 waypoints but Chapter 2 has 14 sections (including subsections). Critical missing waypoints: 2.5 (Embedding Lookup - important mechanical detail), 2.7 (Softmax - critical for probabilities), 2.8 (Tensors - batching), 2.9 (Synthesis - ties counts to coordinates). Recommendation: Add waypoints for 2.5, 2.7, and 2.9 at minimum. 2.8 optional but helpful for navigation.","created_at":"2025-12-21T00:39:13Z"}]}
{"id":"babygpt-3538","title":"Add ConditioningShiftViz: visualize how conditioning shifts a probability distribution (3D-ish)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.33.28.png. They want a beautiful visualization showing “probability distribution shift” when you condition (P(X) vs P(X|context)). They suggested a 3D graph vibe like a PMF plot from school.\n\nWhere to place\n- Chapter 1, Section 1.1.4 “Conditional Probability” (near the bullet list with P(e), P(e|th), P(q|th)).\n- File: src/chapters/Chapter1.tsx\n\nImplementation\n1) Create a new component: src/components/ConditioningShiftViz.tsx + src/components/ConditioningShiftViz.module.css\n2) Export it from src/components/index.ts (barrel file).\n3) The viz should show at least two distributions over the same support:\n   - Unconditional distribution (baseline)\n   - Conditional distribution given a context (shifted)\n4) “3D” is optional but the output must feel premium: use perspective/isometric bars or a layered depth effect via SVG/CSS. Avoid adding heavy new npm dependencies unless absolutely necessary.\n5) Include clear labels and a simple toggle (e.g., buttons) to switch between distributions, or show both at once.\n\nAcceptance\n- Viz clearly communicates “conditioning changes the whole distribution”.\n- Visual style matches existing BabyGPT viz aesthetics (dark glass, cyan/magenta accents).\n- Responsive on mobile.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:08.400275-05:00","updated_at":"2025-12-21T16:03:27.845155-05:00","closed_at":"2025-12-21T16:03:27.845155-05:00","close_reason":"Added ConditioningShiftViz (3D-ish layered bar chart) and inserted into Ch1 Section 1.1.4; exported via barrel; npm run build passes.","dependencies":[{"issue_id":"babygpt-3538","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.141152-05:00","created_by":"daemon"}]}
{"id":"babygpt-3ch","title":"Dot Product: Add 3B1B-quality geometric animations","description":"GeometricDotProductViz: SVG arrows with projection/shadow animation. Vectors as arrows from origin (cyan A, magenta B). Shadow animation sequence: perpendicular drop, landing point, multiply by |B|. Interactive drag to change angle. Insert BEFORE existing DotProductViz. Pattern: follow GradientDescentViz for SVG+CSS transitions.","status":"closed","issue_type":"feature","created_at":"2025-12-20T22:13:12.208942-05:00","updated_at":"2025-12-20T22:30:35.703355-05:00","closed_at":"2025-12-20T22:30:35.703355-05:00","close_reason":"Implemented GeometricDotProductViz with 3B1B-style visualization: draggable vector arrows, projection/shadow animation, angle arc (green for positive, red for negative), real-time dot product calculation, magnitude breakdown. Added bridge text connecting geometric 'shadow' view to probability 'overlap' view."}
{"id":"babygpt-3jd","title":"task","description":"Perplexity is first used at line 757 (section 1.1.7.1 'One Billion Word Benchmark' callout) without definition. The formula (perplexity = 2^H) appears later at line 903 in Exercise 1.1. REMEDIATION: Add perplexity definition (perplexity = 2^H where H is cross-entropy) either: (1) before line 757, OR (2) add forward reference callout at line 757 like 'perplexity (a metric we'll define shortly)' and ensure clear definition by line 903.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:56:01.399602-05:00","updated_at":"2025-12-20T20:27:41.047137-05:00","closed_at":"2025-12-20T20:27:41.047137-05:00","close_reason":"FIXED: Added inline definition of perplexity at first use (line 757): explains it measures model confusion as effective number of choices, lower is better, with example"}
{"id":"babygpt-3rk","title":"Code blocks: prevent inline-code pill styling inside \u003cpre\u003e","description":"Global inline \u003ccode\u003e styling was bleeding into Shiki-rendered code blocks, making each line look like a pill. Add a global pre code reset so code blocks render cleanly.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:45:18.997273-05:00","updated_at":"2025-12-21T10:45:26.195125-05:00","closed_at":"2025-12-21T10:45:26.195125-05:00","close_reason":"Fixed by resetting pre code styles in global.css"}
{"id":"babygpt-3u3","title":"2.10 Gap: Why this gradient is the right direction","description":"Line ~1088-1090: Gradient formula given as 'predicted centroid minus actual embedding'. Shows WHAT, not WHY this is the right direction. FIX: Add intuition: gradient descent moves embeddings to reduce loss. The gradient points toward where the embedding SHOULD have been (the actual) and away from where it wrongly pointed (other candidates). It's literally 'move toward truth, away from mistakes'.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:32.099972-05:00","updated_at":"2025-12-20T21:52:27.317001-05:00","closed_at":"2025-12-20T21:52:27.317001-05:00","close_reason":"Fixed: Added WHY this direction - score = context·candidate, moving closer to E[actual] increases dot product with right answer → higher probability → lower loss"}
{"id":"babygpt-3w3","title":"Restore missing Chapter 1 callouts","description":"Add: This Becomes the Training Objective, Shannon's Information Theory, Meet the engineers, The Engineering Workhorse: KenLM","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T18:34:04.240474-05:00","updated_at":"2025-12-20T18:47:03.787481-05:00","closed_at":"2025-12-20T18:47:03.787481-05:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Added: This Becomes the Training Objective, Shannon's Information Theory (Citations), When Was This the Dominant Approach, From Surprise to Loss","created_at":"2025-12-20T23:38:34Z"},{"id":4,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Added: Sampling: Let It Talk (intro paragraphs before A concrete benchmark)","created_at":"2025-12-20T23:42:00Z"},{"id":5,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Remaining candidates from backup not in current: Build a toy model, Build the pipeline, Define the goal, Dot Product, Grassmann's Vision, Hit the limit, Meet the engineers, Overlap Isn't Meaning, Pick the biggest term, Retrain on corpus, Shannon's Question, Solution 1: Decomposition, Sort by score/coordinate, The Atoms, The Embedding Table, The Engineering Workhorse: KenLM, The Ground Truth, The Handoff, The Job Description, The Math: Chain Rule","created_at":"2025-12-20T23:43:44Z"},{"id":6,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Analysis complete: Most 'missing' titles are: (1) ChapterMap navigation entries (Build a toy model, Define the goal, Hit the limit, Meet the engineers, The Ground Truth, The Handoff - not actual content sections), (2) Interactive component button labels (Pick the biggest term, Sort characters by score, Retrain on corpus), (3) Content exists under different names (Overlap Isn't Meaning → Overlap vs. Understanding, Solution 1: Decomposition → content at line 337, Shannon's Question → content at line 105). All actual content sections have been restored.","created_at":"2025-12-20T23:45:49Z"}]}
{"id":"babygpt-3wls","title":"UI: Fix CodeBlock background/overlay artifacts inside VizCard (Screenshot 2025-12-20 23.48.40)","description":"Context\n- Code snippets are rendered with the CodeBlock component inside VizCard (glass card + ambient glow).\n- Screenshot 2025-12-20 23.48.40 shows the code panel background looking visually wrong (glass/gradients bleeding through, banding, or inconsistent opacity), which hurts readability and breaks the premium look.\n\nWhere\n- CodeBlock: src/components/CodeBlock.tsx and src/components/CodeBlock.module.css\n- VizCard: src/components/VizCard.tsx and src/components/VizCard.module.css\n- Example: find lookup_table.json in src/chapters/Chapter1.tsx (search for the string \"lookup_table.json\").\n\nTask\n1) Reproduce the issue in the browser and identify the exact failure mode (background bleed, stacking context, transparency, etc.).\n2) Fix CSS so the code panel has a stable, opaque background that does NOT inherit card-glass gradients behind it.\n   - Goal: code panel should feel like a solid terminal panel inside the glass card.\n3) Verify Shiki output is not picking up global inline code styling (no cyan pill backgrounds inside code blocks).\n4) Ensure the fix does not regress CodeBlock when used outside VizCard.\n\nAcceptance\n- CodeBlock background is clean and consistent inside VizCard (no bleed / no stripes).\n- No unwanted inline-code styling inside code blocks.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T19:32:43.977016-05:00","updated_at":"2025-12-22T15:24:24.395664-05:00","closed_at":"2025-12-22T15:24:24.395664-05:00","close_reason":"Adjusted CodeBlock surface + header styling to match terminal/code surface and avoid background artifacts."}
{"id":"babygpt-431","title":"1.1.3 Gap: Why expect transfer between 'the cat' and 'the dog'","description":"Line ~400: P=0 for unseen context stated as problem, but WHY should we expect 'the cat' knowledge to help with 'the dog'? What principle? FIX: Add explicit statement: humans generalize because 'cat' and 'dog' share syntactic role (noun after article). Counting models have no notion of 'role' — only exact string match. This is the fundamental limit we're building toward.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:52.673357-05:00","updated_at":"2025-12-20T20:56:09.459048-05:00","closed_at":"2025-12-20T20:56:09.459048-05:00","close_reason":"FIXED: Added explicit explanation of syntactic role as basis for generalization - humans expect 'cat' and 'dog' to be related because they share grammatical function (nouns after article), while counting models lack this notion of role"}
{"id":"babygpt-43p","title":"9.3 Redundancy audit: why embeddings motivation","description":"AUDIT: Compare embedding motivation in 1.6, 1.7, 2.1, 2.2. IF excessive: create ticket to reduce OR add 'building on the motivation from...' to make spiral pedagogy explicit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:08.898664-05:00","updated_at":"2025-12-20T20:29:54.992536-05:00","closed_at":"2025-12-20T20:29:54.992536-05:00","close_reason":"PASS: 1.6/1.7 set up the problem (sparsity), 2.1/2.2 provide the solution (embeddings). Progressive build-up, not redundancy."}
{"id":"babygpt-4bi","title":"3.8 KNOWN: Audit 1.1.7.1 KenLM placement","description":"KNOWN ISSUE: Engineering (hashing, linear probing) before motivation. Check if decoder/beam search concepts introduced without context","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:13:33.283801-05:00","updated_at":"2025-12-20T19:40:32.292831-05:00","closed_at":"2025-12-20T19:40:32.292831-05:00","close_reason":"Closed"}
{"id":"babygpt-4o8s","title":"UI: Fix table symmetry/padding when tables are inside Callout/VizCard containers","description":"Context\n- Screenshot 2025-12-21 17.54.17: Tables look horizontally asymmetric when embedded inside containers (Callout/VizCard/etc).\n- The left/right gutters should feel equal; currently one side looks tighter.\n\nWhere (likely)\n- Table-like components:\n  - src/components/FrequencyTable.tsx + .module.css\n  - src/components/TrainingExamples.tsx + .module.css\n  - src/components/ContextTrace.tsx + .module.css\n  - Any other “table” UI used in chapters\n- Container components:\n  - src/components/Callout.module.css\n  - src/components/VizCard.module.css\n\nWhat to do\n1) Reproduce the issue in the browser.\n   - Run `npm run dev`.\n   - Navigate to the section shown in the screenshot.\n   - Identify which table component is involved.\n\n2) Fix the CSS so the rendered table has visually symmetric left/right padding.\n   - Do NOT “eyeball” by adding random margins. Use a consistent rule.\n   - Recommended: define a shared padding token for table rows (e.g., 16px/20px) and apply consistently.\n\n3) Verify in multiple placements:\n   - Table in normal chapter flow\n   - Table inside a Callout\n   - Table inside a VizCard\n   - Mobile widths\n\nDefinition of done\n- The table looks centered/symmetric in the contexts above.\n- No horizontal scrollbars introduced.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:36.833964-05:00","updated_at":"2025-12-22T15:25:26.511268-05:00","closed_at":"2025-12-22T15:25:26.511268-05:00","close_reason":"Fixed container symmetry by compensating for callout border-left and standardizing callout padding/surface."}
{"id":"babygpt-4wn","title":"CSS: Extract card/panel background patterns to utilities","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:11.182352-05:00","updated_at":"2025-12-20T23:30:02.384798-05:00","closed_at":"2025-12-20T23:30:02.384798-05:00","close_reason":"Closed"}
{"id":"babygpt-5a96","title":"Ch1: Fix perplexity 'tiny example' math formatting (no caret exponents)","description":"Context\n- The perplexity tiny example currently uses plain text/Term for math like `2^{1.5}`, which renders as a caret instead of a superscript.\n- Screenshot feedback: “not Latex formatted (fractions are not proper fractions and so on, same for ^)”.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Find paragraph starting: “Tiny example: suppose over two steps…” (around line ~979)\n\nWhat to do\n- Replace inline caret math with KaTeX-rendered math.\n- Prefer either:\n  1) A short MathBlock showing the computation steps, OR\n  2) Use a MathInline component (if one exists) for inline exponents.\n\nTarget rendering (conceptually)\n- p1=0.5 → surprise=1 bit\n- p2=0.25 → surprise=2 bits\n- average surprise = (1+2)/2 = 1.5 bits/token\n- perplexity = 2^{1.5} ≈ 2.83\n\nConstraints\n- Keep the prose readable; do not dump a full derivation.\n\nDefinition of done\n- No caret/exponent rendering issues in this paragraph.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:37.386795-05:00","updated_at":"2025-12-21T19:02:08.104239-05:00","closed_at":"2025-12-21T19:02:08.104239-05:00","close_reason":"Perplexity tiny example now uses KaTeX inline fractions/exponents (MathInline) instead of caret text; build passes"}
{"id":"babygpt-5dh","title":"GradientDescentViz: Polish sweep","description":"Audit GradientDescentViz for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (gradients, transitions, hover states)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:04.481612-05:00","updated_at":"2025-12-20T22:54:57.310352-05:00","closed_at":"2025-12-20T22:54:57.310352-05:00","close_reason":"Wrong - this IS a gold standard (Dec 15), not needing sweep"}
{"id":"babygpt-5k0","title":"4.11 Audit 2.10 gradient formula dependencies","description":"AUDIT: Read section 2.10 gradient formula, verify all prereqs present: dot product (2.6), softmax (2.7), cross-entropy. IF any missing: add SectionLink. Check if formula has intuitive explanation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:23.421747-05:00","updated_at":"2025-12-20T19:56:29.380412-05:00","closed_at":"2025-12-20T19:56:29.380412-05:00","close_reason":"Closed"}
{"id":"babygpt-5ux","title":"1.3 Gap: Causal mask never explained","description":"Line ~1418-1422: 'Causal Mask' mentioned but never explained. What does 'blocking vision' mean mathematically? FIX: Add Callout explaining causal masking. In attention, each position can 'see' other positions. Causal mask sets attention weights to 0 for future positions (i \u003e j). Mathematically: mask[i,j] = 0 if j \u003e i, else 1. Multiply attention scores by mask before softmax.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:55.60735-05:00","updated_at":"2025-12-20T21:21:05.10468-05:00","closed_at":"2025-12-20T21:21:05.10468-05:00","close_reason":"FIXED: First principles - 'When predicting char 5, only look at 0-4. If you could see 5, you'd copy it.' No jargon (attention/autoregressive/time arrow)."}
{"id":"babygpt-665","title":"9.1 Redundancy audit: sparsity explanations","description":"AUDIT: Compare sparsity discussion in 1.1.2, 1.1.3, 1.1.7.2, 1.6. Document what each adds. IF excessive overlap: create ticket to consolidate OR add 'As we saw in section X...' callbacks to make repetition intentional.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:08.324132-05:00","updated_at":"2025-12-20T19:51:53.838302-05:00","closed_at":"2025-12-20T19:51:53.838302-05:00","close_reason":"Closed"}
{"id":"babygpt-66l","title":"Inline code styling polish sweep - fix background and visual consistency across all chapters","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:10:53.971804-05:00","updated_at":"2025-12-20T23:14:34.396536-05:00","closed_at":"2025-12-20T23:14:34.396536-05:00","close_reason":"Closed"}
{"id":"babygpt-6ar","title":"10.1 Transition audit: 1.1.6-\u003e1.1.7.1","description":"AUDIT: Check bridge from 'building probabilities from corpus' to 'KenLM engineering'. IF abrupt: create ticket to add transition paragraph like 'Now that we can build probabilities, how do we make this fast?'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.1061-05:00","updated_at":"2025-12-20T19:53:45.333726-05:00","closed_at":"2025-12-20T19:53:45.333726-05:00","close_reason":"Closed"}
{"id":"babygpt-6bq","title":"GradientTraceDemo: Polish sweep","description":"Sweep GradientTraceDemo for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (code highlighting, step transitions, output formatting)\n- Accessibility (aria labels, keyboard nav for step buttons)\n- Mobile responsiveness (two-panel layout on small screens)\n- Compare against CodeWalkthrough/NeuralTrainingDemo as gold standard","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:51:32.019662-05:00","updated_at":"2025-12-20T22:52:16.390006-05:00","closed_at":"2025-12-20T22:52:16.390006-05:00","close_reason":"Recreating with expanded gold standards"}
{"id":"babygpt-6ef5","title":"Math typesetting: fix inline fractions/exponents showing as raw text (e.g., 'frac14')","description":"Problem (from UI review): Some inline math is rendering as raw text instead of formatted KaTeX. Symptom example: text shows “frac14” literally, and caret exponents show as “^” instead of superscripts.\n\nRoot causes to look for:\n- LaTeX written as plain text instead of using MathInline/MathBlock.\n- Backslashes not escaped / not using String.raw in JSX strings (JS escape sequences like \\f, \\t, \\n can corrupt LaTeX).\n- Math written with \u003cTerm\u003e or \u003ccode\u003e when it should be KaTeX.\n\nWhere:\n- Any chapter content under src/chapters (likely Chapter 1 + Chapter 2), plus any components that render inline math.\n\nFix goals:\n1) Audit for inline math that is not KaTeX (fractions, exponents, products).\n2) Replace with \u003cMathInline equation={String.raw\"...\"}/\u003e (or \u003cMathBlock ...\u003e) so KaTeX renders true fractions and superscripts.\n3) For simple fractions like 1/2, 1/4, 1/8: if the intent is “math typography”, render them as KaTeX fractions (e.g., \\\\frac{1}{2}) instead of plain 1/2.\n\nSearch hints:\n- Grep for suspicious patterns:\n  - \"\\\\frac\" in non-raw strings\n  - \"\\t\" / \"\\f\" near math snippets (accidental escapes)\n  - caret exponents in prose (e.g., |V|^T) that should be KaTeX\n  - instances of Term used for math that should be MathInline\n\nAcceptance:\n- Reported problem spots render as real fractions/superscripts.\n- No new regressions: npm -s run build passes.\n","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T13:42:45.003824-05:00","updated_at":"2025-12-23T14:22:20.043413-05:00","closed_at":"2025-12-23T14:22:20.043413-05:00","close_reason":"Audited for raw 'fracNN' / non-KaTeX inline math; normalized remaining MathBlock to String.raw; no remaining instances found in Chapter 1/2."}
{"id":"babygpt-6fl","title":"3.15 Audit 1.7 What's Next dependencies","description":"AUDIT: Read section 1.7, verify embeddings are foreshadowed but NOT explained. IF too much detail: create ticket to trim. IF not enough foreshadowing: add 'we need a way to share knowledge' hook.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:55.799332-05:00","updated_at":"2025-12-20T19:42:29.661732-05:00","closed_at":"2025-12-20T19:42:29.661732-05:00","close_reason":"Closed"}
{"id":"babygpt-6mfs","title":"Ch2: Add missing citations (audit + cite all historical/technical claims)","description":"Problem\n- User feedback: Chapter 2 is missing many citations.\n- We make historical and technical claims (Grassmann, colorimetry, embeddings, Boltzmann/temperature, etc.) that should be sourced.\n\nScope\n- File: `src/chapters/Chapter2.tsx`\n- Add citations using the existing `\u003cCite n={...} /\u003e` and `\u003cCitations /\u003e` components.\n\nWhat to do\n1. Read Chapter 2 and list every claim that needs a source:\n   - historical claims (dates, people, quotes)\n   - scientific claims (color mixing rules, statistical mechanics claims)\n   - ML claims (word2vec paper already cited; add missing for softmax/cross-entropy facts if not obvious)\n2. For each claim, add a `\u003cCite n={k} /\u003e` at the end of the sentence/paragraph.\n3. Add corresponding entries in the Chapter 2 citations list (ensure numbering is consistent and sequential).\n4. Prefer primary sources or reputable books/papers; avoid random blogs.\n\nAcceptance criteria\n- Chapter 2 has citations wherever a reasonable skeptical reader would ask “source?”\n- No broken citation numbering (every Cite has an entry).\n- `npm run build` passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T21:53:21.981357-05:00","updated_at":"2025-12-23T01:46:03.239017-05:00","closed_at":"2025-12-23T01:46:03.239017-05:00","close_reason":"Added citations for Shannon surprise / cross-entropy / perplexity and added core info-theory + DL references to Chapter 2; build passes; will sync/push."}
{"id":"babygpt-6ol0","title":"CausalMaskViz: redesign for expressiveness + premium layout (match other vizzes)","description":"Context\n- User feedback + screenshot: Screenshot 2025-12-21 at 10.48.20.png. The current causal mask grid is clean but feels weak compared to other BabyGPT visualizations. Goal: a reader should almost grasp the causal-mask idea immediately from the viz layout/interaction.\n\nWhere\n- Component: src/components/CausalMaskViz.tsx (search for: \"Hover over the grid to check visibility\")\n- Styles: src/components/CausalMaskViz.module.css\n- Used in chapter content via barrel import (src/components/index.ts already exports CausalMaskViz).\n\nWhat\n- Current viz: a static-ish 0/1 lower-triangular grid with hover. It communicates the rule, but doesn't feel “alive” or conceptually rich.\n\nTask\n1) Audit the current UX (run npm run dev, navigate to the CausalMaskViz section) and note what's missing compared to other premium vizzes (VizCard layout, clear legend, immediate story).\n2) Redesign the component to feel like other vizzes:\n   - Wrap in \u003cVizCard\u003e if consistent with other viz components (title/subtitle/figNum optional).\n   - Add a legend that clearly states: “row token can attend to column tokens”.\n   - Make hover interaction more informative: on hover over a row token T_i, highlight the visible set (0..i) and dim the rest; optionally show a short sentence like “T4 can see T1–T4; cannot see T5”.\n   - Consider adding a small “decoder lens” metaphor (corridor / visibility cone) or animation showing information flow left-to-right.\n   - Keep it lightweight (SVG/CSS/DOM). No new heavy dependencies.\n3) Match visual language of other vizzes (glass card, ambient glow, cyan/magenta accents, clean typography).\n4) Ensure accessibility:\n   - Keyboard focusable cells or row selectors (at least rows).\n   - ARIA labels for what is visible when selected/hovered.\n5) Verify responsiveness (mobile layout) and that it's still clear when scaled down.\n\nAcceptance\n- The viz looks and feels consistent with the best vizzes elsewhere (premium layout, clear labeling).\n- A first-time reader can understand “no peeking right” quickly, without reading paragraphs.\n- Hover/selection produces an obvious, explanatory effect (not subtle).\n- No regressions to build: npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T11:01:43.690454-05:00","updated_at":"2025-12-21T14:38:36.912101-05:00","closed_at":"2025-12-21T14:38:36.912101-05:00","close_reason":"Redesigned CausalMaskViz with VizCard + legend + row selection + strong highlight/dim interaction and accessible live readout; matches premium viz styling; build passes.","dependencies":[{"issue_id":"babygpt-6ol0","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T11:02:03.622402-05:00","created_by":"daemon"}]}
{"id":"babygpt-6r7","title":"PHASE 6: Terminology Consistency","description":"Audit term usage consistency across chapters (7 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:59.510839-05:00","updated_at":"2025-12-20T20:30:34.498443-05:00","closed_at":"2025-12-20T20:30:34.498443-05:00","close_reason":"Terminology consistency audit complete. All 7 issues passed: context/history, token/character, embedding/vector, loss/surprise/cross-entropy, logits/scores, context_length/block_size/T, notation consistent."}
{"id":"babygpt-6sy","title":"2.4 Validate Ch1 invariant: Training pairs from sliding window","description":"AUDIT: Read sections 1.3.x, verify (context, target) pair generation is clearly shown. IF missing: create ticket to add SlidingWindowDemo or visual. IF present but unclear: create ticket to add step-by-step walkthrough.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:00.861869-05:00","updated_at":"2025-12-20T19:32:32.920995-05:00","closed_at":"2025-12-20T19:32:32.920995-05:00","close_reason":"Closed"}
{"id":"babygpt-6u9","title":"3.4 Audit 1.1.3 Why This Happens dependencies","description":"AUDIT: Read section 1.1.3, verify 27^N explosion formula comes AFTER joint probability is motivated. Check ExplosionDemo has adequate textual setup. IF gap: add setup paragraph.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:32.143694-05:00","updated_at":"2025-12-20T19:42:01.188407-05:00","closed_at":"2025-12-20T19:42:01.188407-05:00","close_reason":"Closed"}
{"id":"babygpt-71u","title":"2.5 Validate Ch1 invariant: X[i] shifted by 1 = Y[i]","description":"AUDIT: Read sections 1.3.2/1.4, verify 'targets are inputs shifted by 1' is explicitly shown. IF missing: create ticket to add visual or code showing X/Y alignment. IF unclear: add explicit explanation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:01.151008-05:00","updated_at":"2025-12-20T19:32:33.208679-05:00","closed_at":"2025-12-20T19:32:33.208679-05:00","close_reason":"Closed"}
{"id":"babygpt-7eh","title":"2.6 Gap: Why similarity must distribute over addition","description":"Line ~704-710: The constraint (αa + βb)·c = α(a·c) + β(b·c) is given but not motivated. WHY require this? What breaks without it? FIX: Add explanation: if we blend embeddings (weighted average), we want the similarity of the blend to be the weighted average of similarities. This is ESSENTIAL for attention, where we compute weighted sums of values. Linearity = predictable blending behavior.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:17.439573-05:00","updated_at":"2025-12-20T21:46:40.899782-05:00","closed_at":"2025-12-20T21:46:40.899782-05:00","close_reason":"Fixed: Concrete numbers - 'a' scores 0.7, 'e' scores 0.5, average scores 0.6. You already know the answer."}
{"id":"babygpt-7f6o","title":"Ch1 copy: add precise meaning + origin of 'encode'/'decode' in tokenization section","description":"Context\n- User request: give the reader a precise, grounded definition of “encode” and “decode” (and a bit of etymology) when we introduce stoi/itos.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for the existing lines:\n  - “Encoding means: walk through the text…”\n  - “Decoding means: take IDs…”\n\nTask\n1) Expand that area slightly with a 2–4 sentence explanation:\n   - “code” as an agreed mapping between symbols\n   - encoding = symbols → IDs, decoding = IDs → symbols\n   - in this chapter, the “code” is just stoi/itos dictionaries\n2) Keep it short and concrete; avoid grandiose phrasing.\n\nAcceptance\n- Reader understands why the word “encode” is used here, not just what the steps are.\n- No extra jargon added.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:09.464484-05:00","updated_at":"2025-12-21T14:54:43.583289-05:00","closed_at":"2025-12-21T14:54:43.583289-05:00","close_reason":"Added short, concrete 'code as mapping' explanation before encode/decode definitions (stoi/itos as the codebook); build passes.","dependencies":[{"issue_id":"babygpt-7f6o","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.178805-05:00","created_by":"daemon"}]}
{"id":"babygpt-7g8","title":"7.5 Difficulty audit: Ch2 internal flow","description":"AUDIT: Map difficulty curve through Ch2 (philosophical-\u003econceptual-\u003emechanical-\u003emath). IF abrupt jumps: identify specific section boundaries needing transition text.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:29.491128-05:00","updated_at":"2025-12-20T20:07:49.488919-05:00","closed_at":"2025-12-20T20:07:49.488919-05:00","close_reason":"Closed"}
{"id":"babygpt-7zm","title":"SoftmaxSimplexViz: Polish sweep","description":"Sweep SoftmaxSimplexViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states, ambient glow?)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Trail animation smoothness\n\nGold standards to reference:\n- GradientDescentViz (ambient glow, formula display, step interaction)\n- CrossEntropyViz (curve rendering, slider interaction, dynamic labels)\n- CodeWalkthrough (panel layouts)\n- NeuralTrainingDemo (interactive training viz)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:52:30.870818-05:00","updated_at":"2025-12-20T23:11:20.692197-05:00","closed_at":"2025-12-20T23:11:20.692197-05:00","close_reason":"Closed"}
{"id":"babygpt-80n","title":"4.4 Audit 2.4 Vectors Are Storage dependencies","description":"AUDIT: Read section 2.4, verify transition from 'what to store' (2.3) to 'how to store it'. Check one-hot encoding connects to later matrix mult. IF gap: add bridging sentence.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.389554-05:00","updated_at":"2025-12-20T19:52:11.048675-05:00","closed_at":"2025-12-20T19:52:11.048675-05:00","close_reason":"Closed"}
{"id":"babygpt-830q","title":"Ch2 SoftmaxBarsViz: scores → probabilities (single-control challenge)","description":"Context\n- Chapter 2 needs a gentler, first-principles stair into softmax: bars first (2D), then simplex later.\n- Interactivity spec: prompt → one primary control loop → visible state → metric.\n\nGoal\n- Add (or refactor) a “Softmax Bars” interactive viz that shows 3 logits/scores turning into a probability distribution, with one simple challenge that forces commitment.\n\nWhere\n- Likely near existing softmax content: `src/components/SoftmaxSimplexViz.tsx` / `src/components/SoftmaxLandscapeViz.tsx` / `src/chapters/Chapter2.tsx` softmax sections.\n- This bars viz can live as a new component if needed, but prefer extending existing softmax components to avoid drift.\n\nDesign spec\n1) Prompt (commit)\n   - Present a challenge: “Make e about twice as likely as a. Don’t touch temperature yet.”\n   - Provide a “Lock attempt” button that freezes inputs and reveals the scorecard.\n\n2) Action (manipulate)\n   - ONE primary control set: 3 score sliders (e, a, i).\n   - Temperature control exists but is hidden/disabled until after the first challenge is completed.\n\n3) State (see it)\n   - Bars update live as scores change.\n   - Visually separate “scores” vs “probabilities” (two rows/panels).\n\n4) Metric (score it)\n   - Show live ratio: P(e)/P(a).\n   - Show surprise for a chosen “true token”: -log2 P(true).\n   - After “Lock attempt”, show success if ratio is within tolerance (e.g., 1.8–2.2), with gentle feedback.\n\nVisual constraints\n- Use semantic colors: cyan/magenta/yellow map to the three tokens consistently.\n- In-family surfaces: VizCard + panels.\n- Keep layout compact; no scroll-traps.\n\nAcceptance criteria\n- Reader can answer: “softmax turns scores into probabilities, and ratios come from score gaps” by doing the challenge once.\n- Works on mobile widths without clipping.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T22:45:35.916601-05:00","updated_at":"2025-12-24T23:57:31.072621-05:00","closed_at":"2025-12-24T23:57:31.072621-05:00","close_reason":"Added SoftmaxBarsViz with commit-and-lock ratio challenge and inserted it before the simplex in Chapter2; build passes"}
{"id":"babygpt-882","title":"6.1 Terminology audit: context vs history","description":"AUDIT: grep both terms in Ch1/Ch2, document usage counts and locations. IF inconsistent: create ticket to standardize on one term with search-replace. IF interchangeable: add callout explaining both terms.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:10.802391-05:00","updated_at":"2025-12-20T20:29:36.974728-05:00","closed_at":"2025-12-20T20:29:36.974728-05:00","close_reason":"PASS: 'context' is used consistently (60 uses in Ch1, 39 in Ch2). 'history' is rarely used. Terminology is clear."}
{"id":"babygpt-8c1t","title":"Polish Row Selection viz to match visual language (more elegant, less off-theme)","description":"Problem (from UI review): The Chapter 2 'Row Selection (You Can See It)' visualization feels off-theme compared to other gold vizzes (VizCard figure style + calm glass surfaces + consistent typography). It reads like a generic boxed widget with uneven density/hierarchy.\n\nWhere:\n- Chapter 2 section 2.5.1 'Row Selection (You Can See It)'\n- Find the component by searching for the heading text. Likely file: src/components/*OneHot* or *Embedding* lookup viz.\n\nRepro:\n1) Run dev server.\n2) Open /chapter-02.\n3) Scroll to section 2.5.1 'Row Selection (You Can See It)'.\n\nWhat feels off (observed):\n- Surface hierarchy: nested boxes within a big box, but the materials don’t match VizCard/panel-dark/inset-box palette.\n- Typography: labels ('X (one-hot)', 'W (embedding table)') and equation area feel inconsistent with mono label style elsewhere.\n- Layout: too much empty space in the middle, while important parts (row highlight, result vector) are visually under-emphasized.\n- Overall: not as 'premium' as gold references.\n\nDesign goal:\n- Make this viz feel like a 'gold' figure: one primary surface (VizCard), then one panel-dark stage, then inset-box sub-panels.\n- Use the established semantic palette: cyan for selection, magenta/yellow only if semantically justified; avoid arbitrary grays.\n- Tighten vertical rhythm: less dead space; clearer top→middle→result flow.\n\nConcrete implementation checklist:\n- Ensure the viz is wrapped in VizCard (title/subtitle/fig num if applicable), not a bespoke card background.\n- Inside VizCard content: use a single panel-dark 'stage' wrapper.\n- Convert the equation/expansion toggler into an inset-box with mono heading + small button aligned right.\n- Make the result line more legible: 'W[ix] → [..]' should be the visual payoff; give it a calm inset box with monospace and cyan accent for W[ix].\n- Ensure highlights/hover/focus match global tokens (.focus-glow, --radius-*, --transition-*).\n- Validate responsive: no clipping at ~390px, ~768px, ~1280px.\n\nGold references:\n- docs/assets/visual-audit/gold/audit-ch2-axioms-vizcard.png (figure baseline)\n- docs/assets/visual-audit/gold/audit-ch2-histograms-coordinates.png (clean step surface)\n\nAcceptance:\n- Viz matches visual-language spec and feels in-family.\n- Build passes: npm -s run build.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T14:28:16.83244-05:00","updated_at":"2025-12-23T14:38:32.803477-05:00","closed_at":"2025-12-23T14:38:32.803477-05:00","close_reason":"Refactored MatrixRowSelectViz to use VizCard + panel-dark/inset-box surfaces; moved controls into toolbar; replaced expansion toggle with collapsible details; added explicit vocab order explanation."}
{"id":"babygpt-8il","title":"CSS: Standardize transition timing to design tokens","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:12.085686-05:00","updated_at":"2025-12-20T23:30:48.107098-05:00","closed_at":"2025-12-20T23:30:48.107098-05:00","close_reason":"Closed"}
{"id":"babygpt-8jy","title":"PHASE 5: Cross-Reference Validation","description":"Validate all SectionLinks point correctly (5 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:59.192094-05:00","updated_at":"2025-12-20T20:30:33.445132-05:00","closed_at":"2025-12-20T20:30:33.445132-05:00","close_reason":"Cross-reference validation complete. Fixed: 2 incorrect SectionLinks in Ch2 exercises (2.2→2.3)."}
{"id":"babygpt-8lk","title":"4.7 Audit 2.7 Softmax dependencies","description":"AUDIT: Read section 2.7, verify it connects to Ch1 'sum to 1' invariant. Check logits are introduced before softmax applied. IF no Ch1 connection: add 'Recall from Ch1 that...' callout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:22.260705-05:00","updated_at":"2025-12-20T19:54:25.361206-05:00","closed_at":"2025-12-20T19:54:25.361206-05:00","close_reason":"Closed"}
{"id":"babygpt-8ne","title":"10.3 Transition audit: 1.5-\u003e1.6","description":"AUDIT: Check bridge from 'what we built' summary to 'the limit'. IF abrupt: create ticket to add transition like 'Everything works... until it doesn't.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.686574-05:00","updated_at":"2025-12-20T19:53:45.925594-05:00","closed_at":"2025-12-20T19:53:45.925594-05:00","close_reason":"Closed"}
{"id":"babygpt-8sj","title":"8.3 Gap audit: gradient","description":"AUDIT: Find first 'gradient' usage, verify 'slope' intuition is adequate prep. IF gap: create ticket to add intuitive gradient intro in 2.10 before formula.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:46.520425-05:00","updated_at":"2025-12-20T20:10:58.420491-05:00","closed_at":"2025-12-20T20:10:58.420491-05:00","close_reason":"Closed"}
{"id":"babygpt-8us","title":"1.12 Validate Ch2 map waypoint 2.6 Dot Product","description":"AUDIT: Navigate to 2.6, verify 'similarity score you can compute' description. IF mismatch: update. Check if dot product as similarity is clearly established.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:38.416958-05:00","updated_at":"2025-12-20T19:36:27.88625-05:00","closed_at":"2025-12-20T19:36:27.88625-05:00","close_reason":"Closed"}
{"id":"babygpt-8vw6","title":"Design: Standardize code colors (inline code, Term, CodeBlock tokens) across chapters","description":"Context\n- The site uses multiple \"code-like\" treatments: inline \u003ccode\u003e, Term, and CodeBlock (Shiki tokens).\n- User feedback: we should standardize code colors across the document, including inline. They like the current blue/cyan for code.\n- Currently global inline \u003ccode\u003e is cyan, Term is yellow (semantic term), and CodeBlock uses Shiki theme colors.\n\nWhere\n- Global CSS: src/styles/global.css (inline code styling)\n- Term component: src/components/Term.tsx and src/components/Term.module.css\n- CodeBlock: src/components/CodeBlock.tsx and src/components/CodeBlock.module.css\n\nTask\n1) Define a clear, intentional color system:\n   - When to use cyan vs yellow vs other accents.\n   - Make sure semantics stay consistent (Term for tokens/characters; Highlight for key concepts).\n2) Adjust CSS so inline code and Term do not look like two competing code styles.\n   - If changing Term color, audit usages in Chapter 1 and Chapter 2 to ensure readability and meaning.\n3) Ensure CodeBlock syntax colors remain readable and harmonious with the inline system.\n\nAcceptance\n- Inline code, Term, and CodeBlock look cohesive (no random-feeling palette shifts).\n- No loss of semantic meaning (tokens/characters still stand out as such).\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T19:34:02.323291-05:00","updated_at":"2025-12-22T15:24:02.478898-05:00","closed_at":"2025-12-22T15:24:02.478898-05:00","close_reason":"Standardized inline code/Term/CodeBlock styling via shared --code-inline-* tokens and CSS cleanup."}
{"id":"babygpt-8xcp","title":"Ch2 §2.7: Derive Boltzmann weighting from first principles (why exp, why normalize)","description":"Problem\n- User feedback: the current Boltzmann formula presentation still feels like “not first principles.”\n- We need a derivation that is both rigorous and accessible, not just a formula drop.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.7 From Scores to Probabilities (Softmax)`\n- The Boltzmann/temperature subsection.\n\nWhat to do (structure)\n1. Start with the problem statement in plain English:\n   - We have many states, each with an energy (a score/cost).\n   - We want probabilities that prefer lower energy.\n2. Use ONE small derivation that doesn’t require physics background:\n   - Key principle: when you combine two independent systems, energies add.\n   - We want the *relative preference* of energies to multiply when energies add.\n   - That implies a functional equation: f(E1+E2) = f(E1)·f(E2).\n   - The only positive solutions are exponentials: f(E) = exp(−βE).\n   - β is a scale parameter (connect to 1/T after the reader understands the role).\n3. Then normalize:\n   - P(state) = w(state) / Σ w(state)\n   - Explain what the normalization constant is doing (no magic).\n4. Only then show the standard Boltzmann form (with kT), and explain that k is unit conversion.\n\nCitations (required)\n- Add citations supporting:\n  - The additive-energy / exponential-weighting argument (stat mech or info theory text)\n  - The Boltzmann distribution form\n\nAcceptance criteria\n- A reader understands “exp shows up because add→multiply” before seeing any Greek.\n- The final equation feels inevitable, not dropped.\n- `npm run build` passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T21:53:52.404818-05:00","updated_at":"2025-12-22T19:38:18.157973-05:00","closed_at":"2025-12-22T19:38:18.157973-05:00","close_reason":"Rewrote Ch2 §2.7: microstate/atom bridge + first-principles exp/normalize derivation (incl 2-state example), introduced simplex before viz, added citations (7–11); build passes."}
{"id":"babygpt-8yn","title":"3.12 Audit 1.3 Sliding Window dependencies","description":"AUDIT: Read section 1.3, verify it requires tokenization (1.2) and refs decomposition from 1.1.5. IF missing refs: add SectionLinks. Check if sliding window concept has visual support.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:54.936707-05:00","updated_at":"2025-12-20T19:42:28.81226-05:00","closed_at":"2025-12-20T19:42:28.81226-05:00","close_reason":"Closed"}
{"id":"babygpt-95lb","title":"Design system: audit viz styling + define consistent visual language (colors, typography, surfaces)","description":"Problem\n- User feedback: visual language is drifting. Some vizzes look like a different product (colors, typography, density, surfaces).\n- We need a consistent vocabulary: what cyan/magenta/yellow mean, what backgrounds look like, how panels/labels look, how interactive elements look.\n\nGoal\n- Keep room for stylistic variety, but make the “rules” consistent:\n  - semantic color meanings\n  - consistent typography hierarchy\n  - consistent panel/surface styles\n  - consistent interaction affordances\n\nScope\n1. Audit all visualization components under `src/components/*{Viz,Demo}*.tsx` and interactive demos.\n2. Identify mismatches:\n   - fonts (serif vs mono vs sans), sizes, capitalization\n   - background panels (solid vs glass vs code-bg), borders, radius\n   - accent colors used inconsistently\n   - callout color meaning drift\n3. Produce a small design spec in-repo:\n   - update `docs/visual-language.md`\n   - document: semantic colors (cyan = distribution/“probability”; magenta = alternative/contrast; yellow = highlight/warning, etc), spacing rules, surface types\n4. Apply a first pass of fixes to the worst offenders (pick 3–5 components) so the spec is immediately “real”.\n\nProgress (2025-12-22)\n- Added Playwright audit screenshots to `docs/assets/visual-audit/` (gold vs drift examples).\n- Expanded `docs/visual-language.md` with a concrete target spec: semantic colors, surface palette, typography hierarchy, interaction patterns, spacing, and refactor order.\n\nHow to re-run the audit\n- Run `npm run dev` and open:\n  - Chapter 1: `http://127.0.0.1:5173/`\n  - Chapter 2: `http://127.0.0.1:5173/chapter-02`\n- Compare against the screenshots in `docs/assets/visual-audit/`.\n\nAcceptance criteria\n- A new engineer can read the doc and implement a new viz that matches the project without guessing.\n- 3–5 vizzes updated to comply with the spec.\n- `npm run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T21:52:54.336167-05:00","updated_at":"2025-12-22T11:35:02.084063-05:00","closed_at":"2025-12-22T11:35:02.084063-05:00","close_reason":"Added visual-language spec + screenshots, implemented legacy token aliases, and brought OneHotViz/NeuralTrainingDemo/SparseMarkovViz in line with the shared visual language (npm run build passes)."}
{"id":"babygpt-9ed8","title":"UI: Investigate + redesign the 'abhorrent' component from Screenshot 2025-12-21 18.01.11","description":"Context\n- Screenshot 2025-12-21 18.01.11: A component redesign looks visually out-of-family (\"abhorrent\"), compared to the rest of the site’s sleek glassy components.\n- Goal: bring it back into the BabyGPT visual language (glass, subtle borders, balanced spacing, consistent typography).\n\nWhere\n- UNKNOWN from screenshot alone.\n\nWhat to do\n1) Identify the component.\n   - Run `npm run dev`.\n   - Navigate through Chapter 1 and Chapter 2 until you find the exact UI shown in Screenshot 2025-12-21 18.01.11.\n   - Once found, record:\n     - Route / chapter / section number\n     - Component file path(s)\n\n2) Redesign to match the rest of the design system.\n   - Use existing patterns: `VizCard`, `card-glass`, `ambient-glow`, mono labels.\n   - Avoid harsh backgrounds, inconsistent font sizes, or mismatched paddings.\n\n3) Provide before/after evidence.\n   - Add a short note in the PR/issue comment describing what changed.\n\nDefinition of done\n- The component visually matches surrounding vizzes/components.\n- No layout regressions on mobile.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:37.108719-05:00","updated_at":"2025-12-22T17:50:14.081469-05:00","closed_at":"2025-12-22T17:50:14.081469-05:00","close_reason":"The previously out-of-family redesign was brought back into the shared visual language (VizCard surface + semantic cyan/magenta + consistent controls)."}
{"id":"babygpt-9j5","title":"2.8 Validate Ch2 invariant: Embeddings give learnable geometry","description":"AUDIT: Read sections 2.1/2.4, verify 'embeddings provide learnable geometry' is established. IF missing: create ticket to add visual showing geometry emerging from training. IF unclear: add Grassmann connection.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.348171-05:00","updated_at":"2025-12-20T19:42:48.676355-05:00","closed_at":"2025-12-20T19:42:48.676355-05:00","close_reason":"Closed"}
{"id":"babygpt-9qo","title":"2.3 Validate Ch1 invariant: Tokenization via stoi/itos","description":"AUDIT: Read sections 1.2.x, verify stoi/itos pattern is established with code examples. IF missing code: create ticket to add TokenizerDemo or code block. IF pattern unclear: create ticket to add explicit naming.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:00.570455-05:00","updated_at":"2025-12-20T19:32:32.628999-05:00","closed_at":"2025-12-20T19:32:32.628999-05:00","close_reason":"Closed"}
{"id":"babygpt-9vf","title":"2.10 Validate Ch2 invariant: Embedding lookup is E[ix]","description":"AUDIT: Read sections 2.5.x, verify embedding lookup as row selection E[ix] is shown with code. IF missing code: create ticket to add NumPy example. IF unclear: add visual showing row extraction.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.932113-05:00","updated_at":"2025-12-20T19:44:30.92558-05:00","closed_at":"2025-12-20T19:44:30.92558-05:00","close_reason":"Closed"}
{"id":"babygpt-a2s","title":"6.7 KNOWN: Audit notation shift Ch1-\u003eCh2","description":"KNOWN ISSUE: Document ALL notation changes (probability notation, variable names, tensor shapes). Create ticket to add 'Notation Bridge' section at Ch2 start if significant differences found.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:16:12.505195-05:00","updated_at":"2025-12-20T20:30:03.290638-05:00","closed_at":"2025-12-20T20:30:03.290638-05:00","close_reason":"PASS: Notation is consistent across chapters. Variables introduced with same meaning: P(next|c), E[ix], D, T, B. Ch2 builds on Ch1 notation without conflict."}
{"id":"babygpt-a61","title":"4.1 Audit 2.1 Grassmann dependencies","description":"AUDIT: Read section 2.1, verify it refs Ch1 lookup table limitation. Check 'memorization vs generalization' connects to Ch1 sparsity. IF no Ch1 ref: add SectionLink to 1.6.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:20.532018-05:00","updated_at":"2025-12-20T19:49:30.927063-05:00","closed_at":"2025-12-20T19:49:30.927063-05:00","close_reason":"Closed"}
{"id":"babygpt-a7p","title":"10.6 Transition audit: 2.9-\u003e2.10","description":"AUDIT: Check bridge from 'synthesis: we have a map' to 'the nudge: let's make it move'. IF abrupt: create ticket to add motivating question like 'But how do we make the map accurate?'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:28.568598-05:00","updated_at":"2025-12-20T20:19:47.08151-05:00","closed_at":"2025-12-20T20:19:47.08151-05:00","close_reason":"Closed"}
{"id":"babygpt-ae7","title":"2.12 Validate Ch2 invariant: Dot product is similarity metric","description":"AUDIT: Read section 2.6, verify dot product established as primary similarity metric with formula and example. IF missing formula: add. IF missing example: add WorkedExample.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:20.503823-05:00","updated_at":"2025-12-20T19:46:16.79037-05:00","closed_at":"2025-12-20T19:46:16.79037-05:00","close_reason":"Closed"}
{"id":"babygpt-agje","title":"UI: Exercise Hint/Solution should be tabs (mutually exclusive)","description":"Context\n- Screenshot 2025-12-21 18.03.34: Hint and Solution should not be independently expandable.\n- Desired UX: tabs where only one of {none, hint, solution} is visible at a time.\n\nWhere\n- src/components/Exercise.tsx\n- src/components/Exercise.module.css\n\nWhat to do\n1) Replace the current two independent toggles (`showHint`, `showSolution`) with a single state:\n   - activePanel: 'none' | 'hint' | 'solution'\n\n2) UI behavior\n- Clicking “Hint” shows hint and hides solution.\n- Clicking “Solution” shows solution and hides hint.\n- Clicking the active tab again collapses to none.\n\n3) Accessibility\n- Use buttons with clear labels.\n- Add aria state (aria-pressed or aria-selected) and role=\"tablist\" if you implement true tabs.\n\n4) Styling\n- Match BabyGPT style: glassy, subtle borders, consistent fonts.\n\nDefinition of done\n- Only one panel visible at a time.\n- Existing Exercise call sites require no changes.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:37.655384-05:00","updated_at":"2025-12-22T15:27:12.113829-05:00","closed_at":"2025-12-22T15:27:12.113829-05:00","close_reason":"Replaced side-by-side Hint/Solution collapsibles with mutually-exclusive tab UI (Hint/Solution/none)."}
{"id":"babygpt-ahx","title":"9.4 Redundancy audit: sum-to-1 rule","description":"AUDIT: Compare 'probabilities sum to 1' in 1.1.1 vs 2.7. IF Ch2 ref is unnecessary: create ticket to add proper callback 'Recall from Ch1 that...' instead of re-explaining.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:09.182071-05:00","updated_at":"2025-12-20T20:29:55.921523-05:00","closed_at":"2025-12-20T20:29:55.921523-05:00","close_reason":"PASS: Ch1 establishes sum-to-1 as probability axiom (1.1.1), Ch2 references it as motivation for softmax (2.7 line 804). Callback reinforces learning."}
{"id":"babygpt-amk7","title":"Ch4 §4.1: clarify entropy vs cross-entropy vs NLL (one crisp bridge sentence)","description":"Context\n- In Chapter 4, we explain \"average surprise\" and correctly use cross-entropy/NLL as the thing we can actually measure.\n- Readers commonly get confused right here:\n  - Entropy: expected surprise under the TRUE distribution.\n  - Cross-entropy / NLL: surprise when you score samples from the true distribution using YOUR model.\n- We do not need a full formal derivation, but we do need a single crisp sentence that prevents the classic confusion.\n\nWhere\n- Find the first place Chapter 4 introduces: entropy, cross-entropy, negative log-likelihood (NLL), perplexity, \"average surprise\".\n- Likely file: `src/chapters/Chapter4.tsx` (if chapters are split), otherwise search in `src/chapters` for \"cross-entropy\" / \"NLL\" / \"entropy\".\n\nTask\n1) Add a short paragraph (1–3 sentences) that explicitly distinguishes:\n   - True entropy of English (unknown / not directly computable).\n   - Cross-entropy/NLL we estimate by scoring held-out text under our model.\n2) Proposed sentence (adapt tone, keep it humble):\n   - \"We can’t compute the true entropy of English; what we can compute is cross‑entropy: we take held‑out text and score it under our model.\" \n3) Ensure terminology stays consistent throughout the chapter:\n   - If you use \"average surprise\" as cross-entropy, say so once and then keep the same name.\n\nAcceptance criteria\n- A reader can answer: \"What number are we computing in practice, and why isn’t it the true entropy of English?\"\n- No new forward references; the sentence should fit where the terms first appear.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T20:33:27.490163-05:00","updated_at":"2025-12-24T21:57:24.127822-05:00","closed_at":"2025-12-24T21:57:24.127822-05:00","close_reason":"Chapter4 file doesn't exist yet; added the crisp entropy vs cross-entropy/NLL bridge sentence at the first cross-entropy introduction in Chapter2; build passes"}
{"id":"babygpt-ar2","title":"1.2 Gap: Why counting models can't share information","description":"Line ~574-575: 'Counting model can't share information across contexts unless literal overlap' — but WHY? What prevents generalization at data structure level? FIX: Add explanation: a hash table maps exact keys to values. There's no notion of 'similar keys'. Hash('cat sat') and Hash('dog sat') are unrelated integers. The data structure has no geometry — keys are just addresses, not locations in a space.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:32.990534-05:00","updated_at":"2025-12-20T21:09:03.018087-05:00","closed_at":"2025-12-20T21:09:03.018087-05:00","close_reason":"FIXED: Added data structure explanation - hash tables map exact keys to unrelated integers with no geometry for similarity"}
{"id":"babygpt-ars","title":"CrossEntropyViz: Polish sweep","description":"Audit CrossEntropyViz for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (curve rendering, guide lines, axis labels)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:05.323407-05:00","updated_at":"2025-12-20T22:54:57.553713-05:00","closed_at":"2025-12-20T22:54:57.553713-05:00","close_reason":"Wrong - this IS a gold standard (Dec 19), not needing sweep"}
{"id":"babygpt-az5m","title":"Ch2 Softmax: make Boltzmann/temperature intro more gradual (reduce Greek dump)","description":"User feedback (Screenshot 2025-12-21 21.04.45): the physics/temperature path in Chapter 2 Softmax section drops symbols too abruptly. Make the transition into the Boltzmann distribution gentler and more first‑principles, so a non-formally-trained reader can follow.\\n\\nScope: src/chapters/Chapter2.tsx Section 2.7 around the paragraphs introducing Boltzmann + the MathBlock with P(state)=...\\n\\nApproach: break the single equation into a two-step recipe (unnormalized weight then normalize), explicitly define what each symbol means (E, T, k, Z) in plain language, and connect back to the already-learned 'exp + normalize' pattern (without mic-drop tone).\\n\\nAcceptance: improved readability and pacing; build passes (npm run build).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:32:02.401606-05:00","updated_at":"2025-12-21T21:54:21.25256-05:00","closed_at":"2025-12-21T21:54:21.25256-05:00","close_reason":"Superseded by more detailed Boltzmann/physics tasks: babygpt-g2yu (atom-connection + gentle unfolding), babygpt-qdvh (expand physics bridge + citations), babygpt-8xcp (first-principles exp+normalize derivation)."}
{"id":"babygpt-b36k","title":"Ch1 tone: replace 'This isn't a trick or a formula to memorize' with more humble wording","description":"Context\n- User request: avoid language that minimizes the reader's effort. “This isn’t a trick…” can read as dismissive if the concept is hard.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: \"This isn't a trick or a formula to memorize\"\n\nTask\n1) Replace that sentence with wording that is humble and acknowledges this may take a moment to digest.\n2) Keep the meaning: multiplication is just bookkeeping for repeated “of those” filtering; do not add preachy/mic-drop phrasing.\n\nAcceptance\n- Sentence is rewritten with kinder tone, same technical meaning.\n- No other copy changes.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:08.121241-05:00","updated_at":"2025-12-21T14:50:57.206183-05:00","closed_at":"2025-12-21T14:50:57.206183-05:00","close_reason":"Replaced minimizing phrasing with gentler 'looks like a formula at first' wording while keeping the bookkeeping point; build passes.","dependencies":[{"issue_id":"babygpt-b36k","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.899537-05:00","created_by":"daemon"}]}
{"id":"babygpt-b6ub","title":"Fix: restore Generalization Wall crossover marker + correct color label","description":"Context (screenshots 2025-12-23 12:05:36):\n- A color control is labeled 'Color Name' but the UI should show the selected color name (e.g., 'Red') or label simply 'Color'.\n- The Generalization Wall viz lost its 'Crossover' marker/detail (the point where data coverage crosses into sparse regime). This detail is pedagogically important and should be visible again.\n\nTask\n1) Identify the affected components:\n   - Search for the literal label 'Color Name' and fix copy/label.\n   - Find the Generalization Wall / context-length slider viz and restore the 'Crossover' marker/annotation on the spectrum bar.\n2) Ensure the crossover marker updates as T changes (or is computed from coverage ratio) and remains visible at common widths.\n3) Verify no horizontal overflow/clipping.\n\nAcceptance\n- Color label reads correctly.\n- 'Crossover' marker is present and legible.\n- npm -s run build passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T12:08:39.389799-05:00","updated_at":"2025-12-23T12:15:12.68076-05:00","closed_at":"2025-12-23T12:15:12.68076-05:00","close_reason":"Updated AbstractionChainViz example to include a human-readable color name, and restored/clarified the Generalization Wall crossover marker label (Crossover, ratio=1) with centered layout; build passes."}
{"id":"babygpt-bgc","title":"8.6 Gap audit: one-hot encoding","description":"AUDIT: Verify one-hot defined before matrix multiplication explanation in 2.5. IF gap: create ticket to add one-hot intro earlier.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:47.356896-05:00","updated_at":"2025-12-20T20:13:53.369802-05:00","closed_at":"2025-12-20T20:13:53.369802-05:00","close_reason":"Closed"}
{"id":"babygpt-bpm","title":"1.1.7.1 Gap: Why pointer overhead is bad (cache misses)","description":"Line ~1030-1038: Memory calculation concrete but doesn't explain WHY pointer overhead hurts. FIX: Add 1-2 sentences on CPU cache hierarchy. Following a pointer = random memory access = cache miss = ~100 cycles. Array traversal = sequential access = cache hit = ~1 cycle. Pointer-chasing is 100x slower per access. At millions of lookups/second, this dominates runtime.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:40.398536-05:00","updated_at":"2025-12-20T21:10:02.264407-05:00","closed_at":"2025-12-20T21:10:02.264407-05:00","close_reason":"FIXED: Added hardware mechanism explanation for pointer overhead in KenLM section. Cache miss (~100 cycles) vs L1 cache hit (~1 cycle) makes 100× difference at millions of lookups/second."}
{"id":"babygpt-bs5","title":"6.2 Terminology audit: token vs character","description":"AUDIT: find all 'token'/'character' occurrences. IF confusing transitions: add clarifying phrases like 'in our character-level model, each token is a single character'. Document where explicit clarification needed.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.081601-05:00","updated_at":"2025-12-20T20:29:37.892023-05:00","closed_at":"2025-12-20T20:29:37.892023-05:00","close_reason":"PASS: 'character' used for tutorial's char-level focus (91 uses). 'token' used when discussing general concepts (44 uses). Clear distinction maintained."}
{"id":"babygpt-bzz","title":"1.2 Validate Ch1 map waypoint 1.1.5 Chain Rule","description":"AUDIT: Navigate to 1.1.5, verify description 'Turn P(text) into a product of P(next|context)' matches section. IF mismatch: update description to reflect actual section focus. Document what section actually covers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:23.150647-05:00","updated_at":"2025-12-20T19:31:18.711913-05:00","closed_at":"2025-12-20T19:31:18.711913-05:00","close_reason":"Closed"}
{"id":"babygpt-c0k","title":"2.9 Validate Ch2 invariant: Similar = similar predictive role","description":"AUDIT: Read section 2.3, verify 'similar = similar next-char distributions' is explicit. IF missing: create ticket to add CharacterClusterViz or fingerprint comparison. IF implicit: add explicit definition.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.635791-05:00","updated_at":"2025-12-20T19:43:30.661081-05:00","closed_at":"2025-12-20T19:43:30.661081-05:00","close_reason":"Closed"}
{"id":"babygpt-c0y2","title":"Softmax formula card: make explanation organic (step-by-step, first principles)","description":"Problem: Softmax formula currently shown with a compressed 1-line subtitle ('Step 1: Exponentiate... Step 2: Divide...') which feels abrupt and not earned. Goal: rewrite the UI copy around the displayed softmax equation so it leads the reader thought-to-thought: scores -\u003e need probabilities -\u003e ensure positive weights -\u003e normalize to sum 1. Include the missing but important numerical-stability note (shift by max) in a gentle optional aside (or in a short line). Keep tone humble/clear. Keep visuals consistent with VizCard/MathBlock styling; avoid adding more vertical competition.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T15:41:48.570893-05:00","updated_at":"2025-12-23T15:48:05.479756-05:00","closed_at":"2025-12-23T15:48:05.479756-05:00","close_reason":"Rewrote softmax equation caption to be a gentle, step-by-step weights→normalize explanation"}
{"id":"babygpt-c3ic","title":"UI bug: fix clipped text in a viz/card (responsive overflow/wrapping)","description":"Context (screenshot 2025-12-23 00:04:09): some UI text is visibly clipped (cut off instead of wrapping/ellipsis). This is almost always caused by fixed heights + `overflow: hidden`, or a flex/grid child missing `min-width: 0`.\n\nTask\n1) Identify the exact component and selector:\n   - Reproduce in the running app by navigating to the section shown in the screenshot.\n   - Use DevTools inspector to locate the clipped element.\n   - Find the owning file (usually a `src/components/*.module.css` + corresponding `.tsx`).\n   - If you can’t find it visually, search by visible text from the UI via `rg -n \"\u003cexact phrase\u003e\" src/chapters src/components`.\n2) Fix the CSS so the text never clips:\n   - Avoid `height` on containers holding text; use `min-height`.\n   - Avoid `overflow: hidden` unless paired with deliberate ellipsis.\n   - In flex/grid rows, ensure text nodes have `min-width: 0` when you want ellipsis.\n   - Add responsive breakpoints if a desktop 2-column layout collapses and squeezes text.\n3) Verify on common widths:\n   - ~390px (mobile), ~768px (tablet), ~1280px (desktop).\n\nAcceptance criteria\n- No clipped text; it either wraps or truncates intentionally.\n- Visual language remains in-family (`docs/visual-language.md`).\n- `npm -s run build` passes.\n\nImplementation note\nIn the final PR/commit description, explicitly state:\n- which component was broken,\n- what caused clipping,\n- what CSS rule(s) fixed it.\n","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T02:18:02.16194-05:00","updated_at":"2025-12-23T03:11:39.943631-05:00","closed_at":"2025-12-23T03:11:39.943631-05:00","close_reason":"Prevented content clipping inside VizCards by making VizCard content/footer horizontally scroll when needed; added small-screen wrapping tweaks for AxiomViz; build passes."}
{"id":"babygpt-c6g","title":"9.2 Redundancy audit: chain rule explanations","description":"AUDIT: Compare 1.1.5 and 1.1.8 chain rule content. IF 1.1.8 doesn't add value: create ticket to merge into 1.1.5 OR convert 1.1.8 to pure application/example.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:08.60991-05:00","updated_at":"2025-12-20T19:50:14.742334-05:00","closed_at":"2025-12-20T19:50:14.742334-05:00","close_reason":"Closed"}
{"id":"babygpt-c8j","title":"1.3 Validate Ch1 map waypoint 1.1.7.2 Sparsity Trap","description":"AUDIT: Navigate to 1.1.7.2, verify 'counting hits zeros' description. IF mismatch: update to match actual section content. Note if section covers more than just zero-probability problem.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:23.455896-05:00","updated_at":"2025-12-20T19:31:18.997429-05:00","closed_at":"2025-12-20T19:31:18.997429-05:00","close_reason":"Closed"}
{"id":"babygpt-cax","title":"2.2 Validate Ch1 invariant: Probabilities sum to 1","description":"AUDIT: Read section 1.1.1, verify 'probabilities sum to 1' is explicitly stated and demonstrated. IF missing: create ticket to add axiom statement. IF implicit only: create ticket to make explicit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:00.276732-05:00","updated_at":"2025-12-20T19:32:32.328595-05:00","closed_at":"2025-12-20T19:32:32.328595-05:00","close_reason":"Closed"}
{"id":"babygpt-cb02","title":"ContextExplosionViz: restore overlap/sharing explanation","description":"Context (screenshot 2025-12-23 12:56:07)\n- In Fig. 2.2 “The Context Explosion” (ContextExplosionViz), the redesign removed the ‘overlap/sharing’ explanation (“overlap is the escape hatch” / why embeddings generalize by reusing the same row across many contexts).\n- User feedback: “you lost the OVERLAP shit”. This is a regression: the viz needs to explicitly call out overlap/sharing as the key difference, not just show parameter counts.\n\nWhere\n- src/components/ContextExplosionViz.tsx\n- src/components/ContextExplosionViz.module.css\n\nGoal\nBring back a clear, compact ‘overlap’ explanation while preserving the new 2-column layout.\n\nRequirements\n1) Add a small, always-visible ‘Overlap / Sharing’ micro-panel (not a large callout) that explains:\n   - Lookup table: each length-T context is a separate key → no sharing between contexts.\n   - Embedding table: the same token row is reused across all contexts → overlap creates sharing.\n2) Keep it compact:\n   - Desktop: place as a thin row under the two panels (or as a small strip inside the panels).\n   - Mobile: stack cleanly; no extra full-screen scroll.\n3) Keep voice precise and non-preachy.\n4) No clipping/overflow at ~390/768/1280 widths.\n\nAcceptance\n- Reader can answer “what does overlap mean here, and why does it matter?” from the viz alone.\n- npm -s run build passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T12:57:44.820052-05:00","updated_at":"2025-12-23T13:01:31.858661-05:00","closed_at":"2025-12-23T13:01:31.858661-05:00","close_reason":"Restored the missing overlap/sharing explanation in ContextExplosionViz as a compact always-visible micro-panel under the comparison, without reverting the redesigned layout; build passes."}
{"id":"babygpt-cbs","title":"2.11 Validate Ch2 invariant: Shape [B,T] -\u003e [B,T,D]","description":"AUDIT: Read section 2.8, verify shape transformation [B,T]-\u003e[B,T,D] is explicitly shown. IF missing: create ticket to add TensorShapeBuilder visual. IF present but unclear: add step-by-step shape trace.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:20.216665-05:00","updated_at":"2025-12-20T19:45:33.564581-05:00","closed_at":"2025-12-20T19:45:33.564581-05:00","close_reason":"Closed"}
{"id":"babygpt-cejs","title":"MathBlock: long equations get clipped (fix overflow/wrapping)","description":"Problem (from Screenshot 2025-12-21 21.09.07, 1800×1740): a KaTeX-rendered equation inside a MathBlock overflows the container and the right side of the text is CUT OFF (clipped). The reader cannot see the full equation.\n\nWhere to look\n- Component: `src/components/MathBlock.tsx` + `src/components/MathBlock.module.css`\n- Repro: open Chapter 2 (Softmax/Boltzmann area) and find a long equation (e.g. a normalization equation that includes both a fraction and a summation). On desktop and especially narrower widths, the equation extends past the MathBlock and is clipped.\n\nWhat to do\n1. Update MathBlock styling so long display equations are never clipped.\n2. Prefer a graceful behavior:\n   - Option A: allow horizontal scroll for long equations (common math UX)\n   - Option B: responsive scale-down (only if it remains readable)\n   - Option C: wrap/break lines (harder with KaTeX; scroll is usually best)\n3. Ensure the fix applies to all MathBlocks (not only this one equation).\n\nImplementation notes\n- Likely fix is CSS:\n  - add `overflow-x: auto; overflow-y: hidden; max-width: 100%` on the equation wrapper.\n  - consider targeting KaTeX display: `.equation :global(.katex-display)` and/or `.equation :global(.katex)`.\n\nAcceptance criteria\n- No equation text is clipped at common widths (mobile ~390px, tablet, desktop).\n- Long equations remain fully readable (via scroll or other solution).\n- No regression: short equations still centered and look the same.\n- `npm run build` passes.\n","status":"closed","priority":2,"issue_type":"bug","created_at":"2025-12-21T21:48:23.71722-05:00","updated_at":"2025-12-22T15:24:45.951679-05:00","closed_at":"2025-12-22T15:24:45.951679-05:00","close_reason":"MathBlock now horizontally scrolls and centers safely (no clipping) for long KaTeX equations."}
{"id":"babygpt-cgpf","title":"Add more micro-exercises + prediction moments (interactive rails)","description":"Context\n- The interactive demos are a core differentiator, but the chapter still has too few moments where the reader must commit to a prediction or computation.\n- We want to convert passive reading into active understanding with short, low-friction prompts.\n\nWhere\n- Identify the chapter(s) covering:\n  - Chain rule / decomposing P(sequence)\n  - Sparsity / |V|^T scaling\n  - Surprise / cross-entropy / perplexity\n  - KenLM engineering detour\n- Likely files: `src/chapters/Chapter1.tsx` and/or `src/chapters/Chapter2.tsx` and/or `src/chapters/Chapter4.tsx`.\n\nWhat to build\nA) Add 2–3 micro-exercises (each should be a true \"commit before answer\" moment) near the relevant concept:\n1) After the chain-rule example (e.g., \"cat\"):\n   - Prompt: \"If one conditional probability is mis-estimated by half, what happens to the sequence probability?\" \n   - Provide a tiny numeric instance with explicit numbers so the reader can compute (no algebra required).\n2) After the sparsity wall / generalization wall:\n   - Prompt: \"With vocab size 27, what happens to |V|^T when T increases by 1?\" \n   - Answer: \"Multiply by 27.\" Make the point visceral.\n3) (Optional third) After defining surprise / bits:\n   - Prompt: \"If p drops from 1/2 to 1/8, how many extra bits of surprise is that?\" (Answer: +2 bits.)\n\nB) Add three structural \"rails\" (lightweight, not a rewrite):\n1) Early payoff:\n   - Before the big sparsity discussion, insert a tiny loop: count bigrams on a toy corpus → score a tiny held-out string → compute surprise.\n   - Goal: reader sees what the metric is *before* the scaling wall.\n2) Engineering detour labeling:\n   - Wrap KenLM section(s) in a clearly-labeled collapsible/banner that signals: \"engineering limits\" and that it can be skipped without breaking the main story.\n   - Keep tone respectful (no \"optional for amateurs\" language).\n3) End-of-chapter compression:\n   - Right before code/tokenization begins, add a short recap box: \"what we proved\" as inevitabilities.\n   - Example bullet list:\n     - We need decomposition (chain rule).\n     - We score with log-probability of the truth.\n     - We average surprise on held-out text.\n     - We need a finite context window.\n     - We need a mapping to integers.\n\nAcceptance criteria\n- At least two new exercises require the reader to compute/predict before revealing the result.\n- KenLM detour is clearly labeled as skippable without sounding condescending.\n- Recap box cleanly bridges into the upcoming code section.\n- `npm -s run build` passes.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-23T20:35:46.493732-05:00","updated_at":"2025-12-23T20:35:46.493732-05:00"}
{"id":"babygpt-chlj","title":"Design: Define callout color semantics + audit usage (Screenshot 2025-12-21 18.07.02)","description":"Context\n- Screenshot 2025-12-21 18.07.02: Callout colors feel arbitrary.\n- We need an intentional mapping: which variant/color means what.\n\nWhere\n- src/components/Callout.tsx (if exists) + Callout.module.css\n- Callout usage in:\n  - src/chapters/Chapter1.tsx\n  - src/chapters/Chapter2.tsx\n\nWhat to do\n1) Define semantics\n- info: neutral clarification / note\n- insight: key insight / “this is the point”\n- warning: pitfalls / common mistakes\n\n2) Update styles if needed\n- Ensure border-left colors and optional background gradients match the semantics.\n\n3) Audit usage\n- Walk through both chapters and change variants where they don’t match the semantics.\n- Keep diffs small and local.\n\nDefinition of done\n- Variants are used consistently.\n- Visual palette feels structured, not random.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:39.034322-05:00","updated_at":"2025-12-22T15:25:56.43712-05:00","closed_at":"2025-12-22T15:25:56.43712-05:00","close_reason":"Defined callout color semantics in docs and updated Callout styling to make variants visibly distinct and consistent."}
{"id":"babygpt-crly","title":"Ch2 §2.1: Expand the q/u blend intuition (make mixture prediction explicit)","description":"Problem\n- In `src/chapters/Chapter2.tsx` (Section 2.1), we say:\n  - “Purple is half red, half blue.”\n  - “And a blend of 'q' and 'u' predicts a mix of what tends to follow each.”\n- This is a powerful idea, but the current text is too short. Readers won’t automatically connect “blend embeddings” → “blend next-token bets”.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.1 Grassmann's Insight`\n- Nearby text includes: “Colors aren't numbers… And a blend of 'q' and 'u' predicts…”\n\nWhat to write\n- Add ~2–4 short paragraphs (humble, clear tone) that explicitly walks the reader through:\n  1) We can represent each character by a measurable fingerprint: its next-character distribution (from Chapter 1 counts).\n  2) If you average two fingerprints, you literally get a mixture distribution.\n  3) So blending coordinates is not a vibe — it’s a concrete claim about predicted next-character probabilities.\n\nConcrete mini-example (required)\n- Add a tiny numeric example with just 2–3 successor options to show the mixture:\n  - Example format: “After q: u=0.9, other=0.1. After u: vowel=0.6, other=0.4. Blend 50/50 → u=0.45, vowel=0.30, other=0.25.”\n- Keep the numbers simple and obviously illustrative.\n\nAcceptance criteria\n- A reader who has never seen embeddings can explain, in their own words, why blending q and u produces a blended “what comes next” distribution.\n- No new jargon introduced without explanation.\n- `npm run build` passes.\n","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:48:54.496189-05:00","updated_at":"2025-12-22T16:04:05.44602-05:00","closed_at":"2025-12-22T16:04:05.44602-05:00","close_reason":"Expanded the q/u blend intuition with a concrete mixture equation (why blended coordinates yield blended next-token bets)."}
{"id":"babygpt-cv3u","title":"Ch1: Recap 'what we earned' should be callout-quality (special recap boxes)","description":"Context\n- Recaps are important pacing moments. Right now the recap “Before we touch code…” is a plain bullet list.\n- Request: make recap content feel like a deliberate, special moment (callout-style boxes), not just another list.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Find: “Before we touch code, let's pin down what we already earned in Section 1.1:” (around line ~1276)\n\nWhat to do\n- Convert the recap into a more intentional UI.\n  - Good option: use the existing \u003cInvariants\u003e component.\n  - Alternative: a single Callout with a recap title.\n- Ensure the recap has:\n  - The “facts learned so far” list\n  - A clear “what you will build by end of chapter” list\n\nConstraints\n- Keep the content identical unless you are fixing factual errors.\n\nDefinition of done\n- The recap is visually distinct and readable.\n- It matches the visual language of the rest of the book.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:38.487035-05:00","updated_at":"2025-12-23T01:57:02.365823-05:00","closed_at":"2025-12-23T01:57:02.365823-05:00","close_reason":"Replaced the 'what we earned' recap + 'what we build' list with Invariants recap boxes in Ch1 §1.2 for clearer, callout-quality foundations."}
{"id":"babygpt-cxl","title":"2.1 Gap: Abrupt jump from colors to language","description":"Line ~113-119: MASSIVE jump from 'abstract relationships can be coordinates' to language. What makes language LIKE colors? Answer comes 120 lines later. FIX: Add bridge sentence immediately: 'Colors have measurable relationships (wavelength ratios). Language has measurable relationships too (co-occurrence statistics). If we can measure it, we can coordinatize it. That's the move.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:04.826141-05:00","updated_at":"2025-12-20T21:26:25.553877-05:00","closed_at":"2025-12-20T21:26:25.553877-05:00","close_reason":"FIXED: Replaced vague 'measurable relationships' with specific procedures — Grassmann's knob-matching experiment, our counting. Both produce numbers; previous paragraph establishes numbers=coordinates."}
{"id":"babygpt-d3x3","title":"ContextExplosionViz: fundamental layout redesign","description":"Context\n- User feedback (2025-12-23): “I need a fundamental re-arrangement. We can't just shrink a layout that's not made for this.”\n- This refers to Fig. 2.2 “The Context Explosion” viz.\n\nWhere\n- src/components/ContextExplosionViz.tsx\n- src/components/ContextExplosionViz.module.css\n\nProblem\n- Current layout is fundamentally vertical and then “shrunk”. Even compacted, it still reads like stacked blocks rather than a purpose-built, glanceable comparison.\n\nGoal\nRedesign the viz so the comparison is the primary visual structure:\n- Left column: lookup-table growth (V^T × V)\n- Right column: embedding-table size (V × D)\n- Slider for T should feel like a small control surface that updates both columns.\n\nDesign requirements\n1) Desktop (~1024–1440px):\n   - Use a 2-column grid.\n   - Top row spanning both columns: compact slider + T readout.\n   - Next row: two comparison panels side-by-side (Lookup vs Embedding), each with:\n     - small label\n     - compact formula line\n     - large number\n     - small delta line (+1 T ⇒ ×V vs unchanged)\n   - Bottom: a single compact bar comparison row (or tiny bars embedded inside each panel), but avoid a full separate big section.\n2) Mobile (~390px):\n   - Stack the two panels, but keep the slider compact at top.\n   - Avoid “2 scrolls to get past”; aim for ~1 screenful + a bit.\n3) Pedagogy:\n   - Keep “What is T?” as a collapsible detail, but integrate it so it doesn’t feel like a large callout wedged in the middle.\n4) No horizontal overflow/clipping. Verify at ~390/768/1280 widths.\n\nAcceptance\n- The redesign feels intentionally composed, not compressed.\n- The comparison is immediately legible without scrolling through multiple distinct blocks.\n- npm -s run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T12:00:27.631872-05:00","updated_at":"2025-12-23T12:05:33.9166-05:00","closed_at":"2025-12-23T12:05:33.9166-05:00","close_reason":"Redesigned ContextExplosionViz into a purpose-built 2-column comparison (top control surface + side-by-side panels with embedded bars) instead of stacked blocks; build passes."}
{"id":"babygpt-d7ie","title":"UI: Fix code-prefix aesthetic mismatch (Screenshot 2025-12-21 18.08.46)","description":"Context\n- Screenshot 2025-12-21 18.08.46: A “prefix” label + dark background + blue text combination feels off compared to the rest of the theme.\n- The prefix styling should be consistent with our mono labels elsewhere (subtle background, consistent font size).\n\nWhere\n- UNKNOWN from text alone.\n- Likely candidates:\n  - src/components/CodeChallenge.module.css (phase pill + inline code styling)\n  - src/components/CodeWalkthrough.module.css (filename/prefix areas)\n  - src/components/CodeBlock.module.css (if present)\n\nWhat to do\n1) Identify the exact UI in the screenshot.\n   - Find the route/section and component file.\n\n2) Adjust styling\n- Ensure font sizes match nearby text.\n- Avoid overly dark backgrounds behind bright cyan text (unless it matches code-block styling).\n- Prefer the site’s existing glass/mono label patterns.\n\nDefinition of done\n- Prefix/header area looks intentional and consistent.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:39.587759-05:00","updated_at":"2025-12-22T17:49:28.135945-05:00","closed_at":"2025-12-22T17:49:28.135945-05:00","close_reason":"The prefix input styling in CorridorDemo is now less harsh (lighter surface, aligned typography), matching the project’s mono label patterns."}
{"id":"babygpt-def","title":"Extract Slider reusable component (styled range input with label, used in 19 files)","description":"Create a reusable Slider component (styled \u003cinput type=range\u003e) with label, optional value display, min/max/step, and onChange; centralize the CSS used across ~19 files. Export from src/components/index.ts and migrate call sites.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:22.889167-05:00","updated_at":"2025-12-21T00:24:44.480982-05:00","closed_at":"2025-12-21T00:24:44.480982-05:00","close_reason":"Added Slider component and migrated all range inputs to it"}
{"id":"babygpt-dfs","title":"CSS: Extract ambientGlow pattern to shared utility (13 files)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:10.282355-05:00","updated_at":"2025-12-20T23:28:50.945953-05:00","closed_at":"2025-12-20T23:28:50.945953-05:00","close_reason":"Closed"}
{"id":"babygpt-diik","title":"Ch1 restructure: fold Benchmark + Train/Test callout content into core narrative","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.39.25.png: Several important ideas are split across multiple callouts. User wants these to be core content, not fragmented into boxes.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for callout titles around the KenLM/benchmark section (e.g., “Concrete Numbers: The One Billion Word Benchmark”, “Train vs. Test”).\n\nTask\n1) Decide which facts must be in the main narrative (not optional sidebars).\n2) Convert those callouts into normal \u003cParagraph\u003e flow (and possibly a single compact callout for citations only).\n3) Keep the section structure readable: clear subheadings or short paragraphs; avoid wall-of-text.\n\nAcceptance\n- Benchmark + train/test points read as core story, not side quests.\n- The chapter feels more linear and less “boxy”.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:08.94939-05:00","updated_at":"2025-12-21T13:33:24.666338-05:00","closed_at":"2025-12-21T13:33:24.666338-05:00","close_reason":"Folded benchmark/train-test/timeline callouts into main narrative (less boxy), keeping flow linear; build passes.","dependencies":[{"issue_id":"babygpt-diik","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.613028-05:00","created_by":"daemon"}]}
{"id":"babygpt-dkh8","title":"Ch1: fix LaTeX fractions rendering in corridor/multiplication example","description":"Bug: In Chapter 1, several MathInline equations used String.raw with double backslashes (e.g. \\frac), which KaTeX interprets as a newline then plain text, rendering 'frac14' etc instead of true fractions. Fix: replace \\frac/\\log/\\approx/etc with single-backslash LaTeX commands inside String.raw (e.g. \\frac, \\log, \\approx, \\, \\text). Touchpoints: src/chapters/Chapter1.tsx around the corridor multiplication paragraph and the earlier surprise example. Acceptance: fractions render as actual fractions, not 'frac…'; build passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-22T22:46:08.842148-05:00","updated_at":"2025-12-22T22:46:35.296761-05:00","closed_at":"2025-12-22T22:46:35.296761-05:00","close_reason":"Replaced double-backslash LaTeX commands in String.raw with correct single-backslash forms; fractions/logs render properly; build passes."}
{"id":"babygpt-dsb3","title":"Ch1 structure: establish surprise + perplexity earlier (before KenLM benchmark mention)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.37.05.png: Perplexity shows up later as if the reader already understands it. Reader needs surprise/perplexity established earlier and more organically.\n\nCurrent state\n- Chapter 1 mentions “perplexity 67.6” in the KenLM/benchmark callout before a full, intuitive foundation is laid.\n\nTask\n1) Identify the first time perplexity is mentioned in src/chapters/Chapter1.tsx (search: \"perplexity\").\n2) Add an earlier, plain-language introduction that connects:\n   - probability → surprise (−log₂ p) → average surprise (entropy) → perplexity = 2^{average surprise}\n3) Add a tiny numeric example (2–3 steps) so the reader can compute a perplexity-like number by hand.\n4) Ensure the later benchmark section can reference perplexity without re-teaching basics.\n\nAcceptance\n- Reader encounters surprise + perplexity foundations before any benchmark name-drops.\n- Later perplexity references feel earned, not abrupt.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:08.689512-05:00","updated_at":"2025-12-21T13:32:35.79162-05:00","closed_at":"2025-12-21T13:32:35.79162-05:00","close_reason":"Introduced surprise→cross-entropy→perplexity (with tiny numeric example) before first benchmark mention; build passes.","dependencies":[{"issue_id":"babygpt-dsb3","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.378231-05:00","created_by":"daemon"}]}
{"id":"babygpt-dut","title":"1.7 Audit Ch1 ChapterMap missing sections","description":"AUDIT: List all 18 Ch1 sections, compare to 6 ChapterMap waypoints. IDENTIFY: critical sections missing from map (tokenization 1.2? context length 1.3.1?). CREATE TICKET to add missing waypoints if navigation UX suffers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:24.620052-05:00","updated_at":"2025-12-20T19:31:20.132315-05:00","closed_at":"2025-12-20T19:31:20.132315-05:00","close_reason":"Closed"}
{"id":"babygpt-dwyf","title":"Ch2 §2.7: Simplex viz intro ordering + above/below language audit","description":"Problem\n- In the Softmax section, we refer to “The triangle above…” and then explain the probability simplex.\n- User request: introduce the simplex concept *before* the viz, and ensure “above/below” language matches the actual ordering.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.7 From Scores to Probabilities (Softmax)`\n- Around `\u003cSoftmaxSimplexViz /\u003e` and the paragraphs that explain the simplex.\n\nWhat to do\n1. Reorder content so it goes:\n   - Intro text: what the simplex is, what corners/center mean\n   - THEN render `\u003cSoftmaxSimplexViz /\u003e`\n   - THEN follow-up text that refers back to what they just saw\n2. Fix “triangle above/below” wording so it is correct.\n3. Audit nearby vizzes for the same issue (a viz appearing before the reader is told what it is).\n\nAcceptance criteria\n- No viz appears before it is introduced (at least for this Softmax/Simplex block).\n- Above/below references are correct.\n- `npm run build` passes.\n","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:50:59.305499-05:00","updated_at":"2025-12-22T19:38:18.160162-05:00","closed_at":"2025-12-22T19:38:18.160162-05:00","close_reason":"Rewrote Ch2 §2.7: microstate/atom bridge + first-principles exp/normalize derivation (incl 2-state example), introduced simplex before viz, added citations (7–11); build passes."}
{"id":"babygpt-dx8","title":"PHASE 9: Redundancy Detection","description":"Find excessive repetition vs useful reinforcement (4 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:11:00.385827-05:00","updated_at":"2025-12-20T20:30:44.386463-05:00","closed_at":"2025-12-20T20:30:44.386463-05:00","close_reason":"Redundancy detection audit complete. All 4 issues passed: sparsity builds progressively, chain rule sections complement, embeddings motivation scaffolds, sum-to-1 callback reinforces learning."}
{"id":"babygpt-dy0e","title":"Ch2 Softmax: Rethink from first principles (avoid 'yeet Greek'; motivate before equations)","description":"Context\n- Screenshots 2025-12-21 18.14.26 and 18.15.19: Softmax section introduces heavy math too abruptly.\n- Reader is smart but may not have formal training; we must build from first principles.\n\nWhere\n- File: src/chapters/Chapter2.tsx\n- Section 2.7 “From Scores to Probabilities (Softmax)” (starts around line ~884)\n- Current flow uses a large Callout (“Deriving Softmax”) with equations and exp.\n\nWhat to do\n1) Rewrite the softmax introduction to be more bottom-up.\n- Start with: “we have scores (logits), need probabilities”\n- Walk through naive attempts:\n  - divide by sum (fails with negatives)\n  - clamp negatives to 0 (fails with ties / scale sensitivity)\n  - shift by +constant then normalize (still scale problems)\n  - square/abs (biases sign/magnitude weirdly)\n\n2) Only after motivation, introduce:\n- exponentiation for positivity + gap amplification\n- normalization to sum to 1\n- then show the final formula\n\n3) Reduce “boxiness”.\n- Move essential text out of giant callouts; keep callouts only for true asides.\n\nConstraints\n- No forward references.\n- Keep the tone humble and precise.\n\nDefinition of done\n- A reader can explain why softmax is used before seeing the final equation.\n- Section feels linear and gentle.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:40.409233-05:00","updated_at":"2025-12-21T20:04:28.454552-05:00","closed_at":"2025-12-21T20:04:28.454552-05:00","close_reason":"Softmax §2.7 rewritten bottom-up (naive attempts → exp+normalize), while preserving key training intuition (softmax+cross-entropy signal) and keeping Boltzmann/maximum-entropy as optional collapsible; build passes."}
{"id":"babygpt-dzx","title":"2.7 Gap: Why exponentiation amplification is useful","description":"Line ~818-819: 'Exponentiation amplifies differences' — but WHY is this useful? What would happen with a different function? FIX: Add explanation: we want the model to be CONFIDENT — put most probability on one answer. Linear scaling preserves relative gaps. Exponential WIDENS gaps, making the winner dominate. This is 'soft argmax' — we approximate max() but keep it differentiable.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:24.587271-05:00","updated_at":"2025-12-20T21:48:11.854582-05:00","closed_at":"2025-12-20T21:48:11.854582-05:00","close_reason":"Fixed: Concrete numbers [2.0,1.5]→[7.4,4.5]→[0.62,0.38]. WHY: hedging wastes probability, amplification rewards commitment."}
{"id":"babygpt-e0am","title":"ConditioningShiftViz: Make the probability-mass chart draggable/interactive (Screenshot 2025-12-21 18.07.38)","description":"Context\n- Screenshot 2025-12-21 18.07.38: The probability-mass viz is promising but static; users want to “play with it”.\n- This will become a core visualization pattern, so it’s worth leveling up.\n\nWhere\n- src/components/ConditioningShiftViz.tsx\n- src/components/ConditioningShiftViz.module.css\n\nWhat to do\n1) Add drag interaction\n- Allow click+drag (pointer events) over the SVG/chart panel to rotate the pseudo-3D perspective.\n- Implementation idea:\n  - Maintain state for view angles or for depth parameters (DEPTH_X / DEPTH_Y) derived from drag deltas.\n  - Update the prism projection accordingly.\n\n2) Add UI affordance\n- Small hint text: “Drag to rotate” (and keep existing hover-to-inspect).\n\n3) Keep it stable\n- Clamp angles/values so it never inverts or becomes unreadable.\n- Must work on touch devices.\n\nDefinition of done\n- Dragging changes the perspective in a noticeable, smooth way.\n- Hover inspection still works.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:39.316688-05:00","updated_at":"2025-12-21T19:48:18.197229-05:00","closed_at":"2025-12-21T19:48:18.197229-05:00","close_reason":"Added pointer drag to rotate pseudo-3D perspective (depthX/depthY) with clamps; added 'Drag to rotate' hint; preserved hover inspection; touch-safe via pointer events; build passes."}
{"id":"babygpt-e0e","title":"Restore missing Chapter 2 sections","description":"Add: Grassmann's Vision Finally Built, Sort characters by score, Sort characters by this coordinate, Solution 1: Decomposition, Overlap Isn't Meaning","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T18:34:15.036781-05:00","updated_at":"2025-12-20T18:48:42.893995-05:00","closed_at":"2025-12-20T18:48:42.893995-05:00","close_reason":"Closed","comments":[{"id":7,"issue_id":"babygpt-e0e","author":"andrewlouis","text":"Verified: Grassmann's Vision content exists at lines 1647-1665 (within Section 2.10 The Nudge). Sort characters buttons are component labels, not sections. Overlap section exists as 2.6. All content present.","created_at":"2025-12-20T23:48:39Z"}]}
{"id":"babygpt-e1gk","title":"Ch1 ending: add stronger motivation for exercises; visualize generalization gap (avoid anti-climax)","description":"Context\n- User feedback: The end of the Chapter 1 “theory” arc feels anti-climactic and does not motivate why the exercises matter or what the reader will implement. They also want a visual demonstration of “intent vs gap” for generalization, ideally using our probability distribution concepts.\n\nWhere\n- Primary file: src/chapters/Chapter1.tsx\n- Look for the Chapter 1 exercises section (search for: title=\"Exercises\" or \u003cExercise).\n- Also look at transitions into exercises and the last paragraphs before the exercises start.\n\nTask\n1) Add a short pre-exercises “Why these exercises exist” block immediately before the exercises section. It must answer explicitly:\n   - What the reader is about to implement (tokenization, sliding window training pairs, etc.).\n   - Why implementation matters: it operationalizes the chain rule + conditioning + surprise into real data a model can train on.\n   - Why we care: the step from memorization (exact match) to generalization requires representing shared structure; this chapter builds the pipeline that makes later learning possible.\n2) Strengthen motivation with a small visual (preferred):\n   - If ConditioningShiftViz (issue babygpt-3538) exists, reuse it or extend it.\n   - Otherwise create a minimal lightweight viz inside Chapter 1 that contrasts:\n       (a) “counting/lookup” behavior: sharp, brittle distribution with zeros\n       vs\n       (b) “generalizing model” intent: smoother distribution that assigns non-zero mass to plausible unseen continuations.\n   - The viz does NOT need to be mathematically perfect; it must be conceptually correct and help intuition.\n3) Avoid hype; use humble, supportive tone. Do not imply the reader “should already get it”.\n\nAcceptance\n- The chapter no longer ends soft: reader feels a clear reason to do exercises and a concrete expectation of what they'll build.\n- If a viz is added, it clearly communicates the generalization gap in one glance.\n- The exercises feel like the natural next move, not a disconnected appendix.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T11:01:43.960899-05:00","updated_at":"2025-12-21T14:47:41.929622-05:00","closed_at":"2025-12-21T14:47:41.929622-05:00","close_reason":"Added pre-exercises motivation block + new GeneralizationGapViz (interactive smoothing slider) to show why P=0 is disastrous and why exercises matter; build passes.","dependencies":[{"issue_id":"babygpt-e1gk","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T11:02:03.868115-05:00","created_by":"daemon"}]}
{"id":"babygpt-eat8","title":"Typography: unify Term + Highlight styling with code tokens","description":"Problem: some inline token highlights are pill-style (\u003ccode\u003e) while others are flat (\u003cTerm\u003e/\u003cHighlight\u003e), creating inconsistent visual language between prose and code panels. Goal: make Term render using the same code-chip styling and give Highlight a subtle background so emphasis feels like the same system. Scope: src/components/Typography.tsx and Typography.module.css only; keep overrides in code blocks (pre code) intact. Acceptance: build passes; tokens look consistent across chapters.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-22T20:00:43.958237-05:00","updated_at":"2025-12-22T20:01:07.953198-05:00","closed_at":"2025-12-22T20:01:07.953198-05:00","close_reason":"Implemented: \u003cTerm\u003e now renders as \u003ccode\u003e for consistent chip styling; \u003cHighlight\u003e gets subtle background/inner border. Build passes."}
{"id":"babygpt-eeaa","title":"Ch2 recap: replace 'permission slip' phrasing with precise statement of what Grassmann gives us","description":"Problem\n- In the Chapter 2 recap we say: “Grassmann showed that abstract things can be coordinates. We need that ‘permission slip’…”\n- User request: when stating facts, be precise and concrete. “Permission slip” is cute but underspecified.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: the recap list just before the Chapter 2 invariants (around the end of the chapter flow map / recap)\n\nWhat to do\n- Rewrite that recap bullet so it states, explicitly:\n  1) What Grassmann demonstrated (coordinates for abstract attributes via measurable mixing/scaling rules).\n  2) What we are borrowing (represent tokens by coordinates so “near/far” and interpolation have meaning).\n  3) What we are NOT claiming (the symbol isn’t a vector; the embedding is the coordinate representation).\n\nAcceptance criteria\n- The recap bullet can stand alone as a clear “earned fact” with no slang placeholders.\n- Tone stays humble and clear.\n- `npm run build` passes.\n","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:51:56.754181-05:00","updated_at":"2025-12-22T19:46:58.543066-05:00","closed_at":"2025-12-22T19:46:58.543066-05:00","close_reason":"Rewrote Ch2 §2.9 synthesis bullet to precisely state Grassmann’s contribution and what embeddings claim (coordinates for mixable/scalable attributes; symbol vs embedding distinction)."}
{"id":"babygpt-eeb","title":"3.6 Audit 1.1.5 Chain Rule dependencies","description":"AUDIT: Read section 1.1.5, verify all prereqs present: joint prob from 1.1.2, conditional prob from 1.1.4. IF any missing: add SectionLink backward reference.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:32.718033-05:00","updated_at":"2025-12-20T19:42:01.781533-05:00","closed_at":"2025-12-20T19:42:01.781533-05:00","close_reason":"Closed"}
{"id":"babygpt-egk","title":"4.3 Audit 2.3 What Can We Measure dependencies","description":"AUDIT: Read section 2.3, verify fingerprint/similarity concept is established BEFORE dot product in 2.6. Check P(next|c) connects to Ch1. IF no Ch1 connection: add SectionLink.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.099299-05:00","updated_at":"2025-12-20T20:25:32.224877-05:00","closed_at":"2025-12-20T20:25:32.224877-05:00","close_reason":"FIXED: Added SectionLink to Chapter 1.1.5 (Chain Rule) in Section 2.3 when introducing P(next|c)","comments":[{"id":12,"issue_id":"babygpt-egk","author":"andrewlouis","text":"NEEDS_FIX: Section 2.3 establishes P(next|c) fingerprint concept but lacks explicit reference to Chapter 1 counting/probability foundations. Recommendation: Add SectionLink to Chapter 1 when introducing P(next|c) to show where this counting concept comes from.","created_at":"2025-12-21T00:51:28Z"}]}
{"id":"babygpt-eguz","title":"Docs: Create a concept flow map (section → concepts → dependencies) and version control it","description":"Context\n- We need a clear meta‑structure of the book so we can avoid:\n  - forward references\n  - repeating the same explanation twice\n  - re‑introducing ideas after they were already introduced\n- This should be version-controlled so edits stay coherent over time.\n\nWhere\n- Add a new documentation file (create docs/ if missing), e.g.:\n  - docs/concept-map.md\n\nWhat to do\n1) Create a map for Chapter 1 and Chapter 2.\n   - For each section number (e.g. 1.1, 1.2, …, 2.7, 2.10), list:\n     - Concepts introduced (first appearance)\n     - Concepts used but not introduced\n     - Associated components/vizzes (if any)\n\n2) Add a dependency graph.\n   - Prefer Mermaid so it renders in GitHub.\n   - Nodes = section numbers.\n   - Edges = “depends on”.\n\n3) Add an “anti‑regression checklist”.\n   - A short list of what must remain true (no forward refs, etc).\n\nDefinition of done\n- docs/concept-map.md exists and is readable.\n- Mermaid graph renders (no syntax errors).\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:36.56279-05:00","updated_at":"2025-12-21T19:39:54.770621-05:00","closed_at":"2025-12-21T19:39:54.770621-05:00","close_reason":"Added docs/concept-map.md with per-section concept inventory + Mermaid dependency graph + anti-regression checklist; build passes."}
{"id":"babygpt-eq37","title":"Ch2 §2.3: tighten 'embeddings beat |V|^T' wording — separate sharing vs context composition","description":"Context\n- The current Chapter 2 narrative correctly sells *parameter sharing* via embeddings, but parts of the wording can accidentally imply a stronger claim than is true:\n  - It can sound like embeddings alone handle multi-token contexts.\n- The correct picture is:\n  - Embeddings buy sharing across all occurrences of a token (one row per token).\n  - Generalization across unseen *contexts* also requires a composition rule / architecture that combines token vectors into logits.\n\nWhere\n- `src/chapters/Chapter2.tsx` around Sections 2.2–2.4 (the \"reuse\" / \"context explosion\" / \"embedding table\" setup).\n- Search for language like: \"any context containing them produces similar predictions\" or similar.\n\nTask\n1) Edit the few sentences that overshoot:\n   - Keep the punch, but make it honest:\n     - \"Embeddings give us reusable parts (shared vectors).\"\n     - \"We still need assembly instructions: a function that combines vectors from the context into logits for the next token.\" \n2) Add one tiny concrete example of the simplest possible embedding LM to anchor the claim:\n   - Bigram-style:\n     - `e_i = E[i]`\n     - `ℓ = e_i W_out + b`\n     - `P(next|i) = softmax(ℓ)`\n   - One sentence: \"This already generalizes a little (similar embeddings → similar logits), but it's still only 1-token context.\" \n3) Make sure we explicitly say where multi-token context handling will come from later (without forward-referencing transformers).\n   - Acceptable: mention simple composition options (sum/concat) as preview, or point to the coming section that introduces the composition.\n\nAcceptance criteria\n- Readers don't walk away thinking `|V|^T → |V|` just because we stored an embedding table.\n- The chapter makes a clean distinction between: (a) sharing and (b) composition.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T20:39:06.084539-05:00","updated_at":"2025-12-24T21:46:36.870982-05:00","closed_at":"2025-12-24T21:46:36.870982-05:00","close_reason":"Rewrote embeddings explanation to separate shared-memory benefit from the separate need for a context-composition rule; build passes"}
{"id":"babygpt-esr","title":"3.1 Audit 1.1 Physics dependencies","description":"AUDIT: Read section 1.1, check for forward references to later concepts. Verify Shannon 1.3 bits/char target is introduced. IF forward refs found: create ticket to remove or add 'we'll see later' framing. IF Shannon missing: add.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:31.302577-05:00","updated_at":"2025-12-20T19:42:00.260142-05:00","closed_at":"2025-12-20T19:42:00.260142-05:00","close_reason":"Closed"}
{"id":"babygpt-ewbx","title":"Redesign: From Histograms to Coordinates viz (UX polish)","description":"Context (screenshot 2025-12-23 12:56:27): The \"From Histograms to Coordinates\" interactive viz looks cluttered and visually weak compared to the rest of the site. User wants a serious UX polish pass to make it feel like a ‘hero’ viz.\n\nWhere\n- Identify the component by searching for the title text \"From Histograms to Coordinates\" and the tab labels \"Counts (Data)\", \"Postcard (2D)\", \"Similarity (Score)\".\n- Likely files: src/components/*Viz*.tsx + corresponding *.module.css.\n\nGoal\nMake the viz beautiful, clear, and consistent with the project’s visual language (docs/visual-language.md): fewer nested boxes, better hierarchy, consistent spacing, and purposeful surfaces.\n\nConcrete requirements\n1) Top controls (A/B selects):\n   - Align and style them consistently with other vizzes (pill controls, clear labels A/B).\n   - Reduce wasted vertical space.\n2) Step tabs:\n   - Redesign the step selector so it reads as a single component (segmented control / stepper), not three separate pills.\n   - Keep the active state obvious.\n3) Content panel:\n   - Improve hierarchy inside the active step: one primary panel, minimal sub-panels.\n   - Reduce the ‘two scrolls’ feeling within the card.\n4) Histogram panels:\n   - Tighten spacing; improve readability of bars and counts.\n   - Ensure labels align and don’t feel like random boxes.\n5) The bottom ‘link’ callout:\n   - Keep it, but style it as a compact insight strip consistent with callout colors.\n6) Responsiveness:\n   - Verify at ~390/768/1280 widths; no clipping/overflow.\n\nAcceptance\n- The component feels intentional and on-par with the best Chapter 1 vizzes.\n- npm -s run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T13:05:37.251381-05:00","updated_at":"2025-12-23T13:34:51.332701-05:00","closed_at":"2025-12-23T13:34:51.332701-05:00","close_reason":"Redesigned CharacterClusterViz to align with VizCard visual language; added gold audit screenshot."}
{"id":"babygpt-f94","title":"3.3 Audit 1.1.2 Problem With Sequences dependencies","description":"AUDIT: Read section 1.1.2, verify joint probability P(x1,x2,...) is introduced AFTER single-token probability in 1.1.1. IF out of order: create ticket to add bridging sentence or reorder.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:31.86495-05:00","updated_at":"2025-12-20T19:42:00.870741-05:00","closed_at":"2025-12-20T19:42:00.870741-05:00","close_reason":"Closed"}
{"id":"babygpt-f94r","title":"Ch2 Softmax: put Boltzmann history back in main flow + make max-entropy argument rigorous (no overclaim)","description":"Context\n- Softmax section 2.7 is part of the core narrative of BabyGPT: math as a historical object, not just a tool.\n- The Boltzmann/temperature story is a major differentiator and should be part of the main reading path (not hidden behind an optional collapsible).\n- The “max entropy / exp is inevitable” claim must be correct and not over-stated.\n\nWhere\n- File: src/chapters/Chapter2.tsx\n- Section: 2.7 “From Scores to Probabilities (Softmax)”\n- Current: physics + max-entropy live inside a \u003cdetails className=\"collapsible\"\u003e block.\n\nTask\n1) Move the physics narrative (marble-in-bowl intuition + short Boltzmann/Mach/Einstein historical grounding) into the main 2.7 flow.\n   - Keep it readable and not too long.\n   - Keep one key equation (Boltzmann distribution) and a short mapping to softmax.\n   - Avoid burying it as “optional”.\n\n2) Keep the deeper/technical parts optional, but make them rigorous:\n   - Rewrite the maximum-entropy argument precisely:\n     - State the setup: maximize Shannon entropy subject to constraints (sum to 1, expected energy/score).\n     - Result: exponential family p_i ∝ exp(β·score_i) (or exp(-β·E_i)), where β relates to temperature.\n   - Do NOT claim softmax is the only possible mapping without stating assumptions.\n   - If including the derivation, put it in a collapsible and keep it short.\n\n3) Ensure the “softmax+cross-entropy gives a clean training signal” intuition remains present.\n\nAcceptance\n- Physics story is in the main reading path of 2.7.\n- Max-entropy claim is accurate and scoped (no misleading inevitability).\n- Section still feels linear and gentle.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T20:12:58.508659-05:00","updated_at":"2025-12-21T20:15:35.376479-05:00","closed_at":"2025-12-21T20:15:35.376479-05:00","close_reason":"Moved Boltzmann/temperature history + marble intuition into main 2.7 flow; kept a more rigorous max-entropy argument in optional collapsible (states assumptions, gives exponential-family form) and avoids overclaim; build passes."}
{"id":"babygpt-fa1","title":"5.1 Validate SectionLink in 1.2 to 1.1","description":"AUDIT: Line 1058 in Ch1, find SectionLink to 1.1. Verify 'sparsity problem' text accurately reflects 1.1 content. IF inaccurate: update link text to match what 1.1 actually covers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:39.346986-05:00","updated_at":"2025-12-20T19:43:40.154249-05:00","closed_at":"2025-12-20T19:43:40.154249-05:00","close_reason":"Closed"}
{"id":"babygpt-fgj","title":"2.5 Gap: Why one-hot leads to gradients","description":"Line ~610-611: 'One-hot decides which row gets credit' — but WHY does this lead to gradients? Gradient flow unexplained. FIX: Add explanation of gradient routing. One-hot selects row i. Loss depends on row i. Gradient ∂L/∂E[i] is non-zero ONLY for row i. One-hot acts as a 'router' — it tells backprop which embedding to update. All other rows have zero gradient for this example.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:09.295596-05:00","updated_at":"2025-12-20T21:42:32.33788-05:00","closed_at":"2025-12-20T21:42:32.33788-05:00","close_reason":"Fixed: Explained gradient routing from first principles - if row 3 is used and prediction is wrong, only row 3 can be adjusted. No backprop jargon."}
{"id":"babygpt-fho1","title":"EmbeddingGradientViz: Redesign (too clunky / too much real estate) + improve interaction (Screenshot 2025-12-21 18.16.57)","description":"Context\n- Screenshot 2025-12-21 18.16.57: EmbeddingGradientViz feels clunky and oversized.\n- This is a core concept; the viz should be as immediate and beautiful as Chapter 1’s best vizzes.\n\nWhere\n- src/components/EmbeddingGradientViz.tsx\n- src/components/EmbeddingGradientViz.module.css\n- Used in src/chapters/Chapter2.tsx (search for \u003cEmbeddingGradientViz /\u003e)\n\nWhat to do\n1) Reduce UI complexity\n- Keep only the controls needed for the main story.\n- Avoid multiple panels competing for attention.\n\n2) Improve interaction\n- Make the “path” of the update easy to see.\n- Prefer direct manipulation where possible (e.g., drag points / drag the gradient arrow) if it stays stable.\n\n3) Visual polish\n- Match VizCard patterns and spacing.\n- Ensure text labels are consistent and not cramped.\n\nConstraints\n- No new heavy dependencies.\n- Keep performance smooth.\n\nDefinition of done\n- The viz is simpler to use and more visually in-family.\n- It communicates the concept quickly.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:41.240144-05:00","updated_at":"2025-12-22T15:56:28.958454-05:00","closed_at":"2025-12-22T15:56:28.958454-05:00","close_reason":"Redesigned EmbeddingGradientViz into compact 2-column layout with direct manipulation (drag context ring), clear preview of next step, and simplified controls/probability view."}
{"id":"babygpt-fp5","title":"1.9 Validate Ch2 map waypoint 2.2 Reuse Question","description":"AUDIT: Navigate to 2.2, verify 'context explosion' description. IF mismatch: update. Check if section clearly establishes the combinatorial explosion problem.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:37.51601-05:00","updated_at":"2025-12-20T19:31:56.502713-05:00","closed_at":"2025-12-20T19:31:56.502713-05:00","close_reason":"Closed"}
{"id":"babygpt-fwkp","title":"Ch2 Softmax: add connective tissue bridges (physics pivot + temperature meaning + training payoff)","description":"Context\n- User feedback: even with the softmax rewrite, the section needs more connective tissue.\n- Specifically: (A) why we pivot into physics/history, (B) what temperature really does in an LLM (sampling vs training), and (C) why softmax is worth the ceremony (it enables a clean training signal).\n\nWhere\n- File: src/chapters/Chapter2.tsx\n- Section 2.7 “From Scores to Probabilities (Softmax)” around the temperature paragraph and the Boltzmann history block.\n\nTask\n1) Bridge A (physics pivot): add 1–2 short paragraphs that explicitly connect logits↔energies as the same underlying problem: unnormalized scores → normalized probability distribution, without adding arbitrary thresholds.\n2) Bridge B (temperature meaning): after the T→0/1/∞ bullets, add a paragraph explaining:\n   - temperature is applied at sampling/inference time by rescaling logits before softmax\n   - it does not change what the model learned; it changes how often it takes second-best options\n   - training usually keeps T fixed (often 1) and learns the logits themselves\n3) Bridge C (training payoff): in the “softmax + cross-entropy” callout, add one closing sentence tying back to the earlier “smoothness” requirement: softmax gives a differentiable probability map with a stable learning signal.\n\nConstraints\n- Keep tone humble/clear; no overclaims like “softmax is the only way” unless assumptions are stated.\n- Keep bridges short; don’t bloat the section.\n\nAcceptance\n- Reader never wonders why we switched frames.\n- Reader can explain temperature as a sampling knob.\n- Softmax’s training relevance feels earned.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T20:21:57.227901-05:00","updated_at":"2025-12-21T20:23:58.368418-05:00","closed_at":"2025-12-21T20:23:58.368418-05:00","close_reason":"Added explicit bridges in Ch2 §2.7: logits↔energies connection before Boltzmann; simplex/temperature tie-back before viz; clarified temperature as sampling knob vs training; added smoothness→stable learning signal payoff in softmax+cross-entropy callout; build passes."}
{"id":"babygpt-g2yu","title":"Ch2 §2.7: Boltzmann story needs better atom-connection + gentler formula unfolding","description":"Problem\n- User feedback: the Boltzmann/temperature section currently jumps too fast and doesn’t clearly explain:\n  1) how this story historically relates to atoms (why talking about “states” implies discrete microstates)\n  2) how to unfold the formula organically from first principles\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.7 From Scores to Probabilities (Softmax)`\n- The part that introduces the marble-in-a-bowl analogy and then the Boltzmann distribution.\n\nWhat to change\n1. Add missing bridge: explain why Boltzmann’s move depends on *many possible micro-configurations* (the implicit atom hypothesis), and why that was controversial in the 1800s.\n2. “Reverse telescope” the math:\n   - Start with a plain-language recipe:\n     - “lower energy → more likely”\n     - “temperature controls how strongly energy differences matter”\n     - “normalize so probabilities sum to 1”\n   - Then show a tiny 2‑state example with real numbers (two energies, two probabilities).\n   - Only then show the general formula.\n3. Make the reader understand each symbol before seeing it (avoid “Greek dump”).\n4. Add citations (required):\n   - Boltzmann’s statistical mechanics / distribution\n   - Mach’s skepticism about atoms\n   - Einstein/Brownian motion (as the later evidence thread)\n\nAcceptance criteria\n- A smart non-physicist can explain what a “state” is and why the math needs normalization.\n- The formula appears only after the intuition + 2‑state example.\n- At least 3 citations added (Boltzmann, Mach, Einstein/Brownian motion).\n- `npm run build` passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T21:50:32.325106-05:00","updated_at":"2025-12-22T19:38:18.152972-05:00","closed_at":"2025-12-22T19:38:18.152972-05:00","close_reason":"Rewrote Ch2 §2.7: microstate/atom bridge + first-principles exp/normalize derivation (incl 2-state example), introduced simplex before viz, added citations (7–11); build passes."}
{"id":"babygpt-g31","title":"Extract DataRow reusable component (key-value display pattern)","description":"Extract the repeated key/value row pattern (label + value + optional monospace styling) into a DataRow component and migrate call sites.","status":"blocked","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:23.72084-05:00","updated_at":"2025-12-21T14:58:27.955576-05:00"}
{"id":"babygpt-ghl","title":"1.1.1 Gap: Why rare events NEED high surprise","description":"Line ~214-219: Log formula shown but the NECESSITY isn't derived. What breaks with a different function? FIX: Add a Callout showing that log is the UNIQUE function satisfying: (1) additive for independent events, (2) monotonic in probability, (3) continuous. Cite Shannon's uniqueness theorem or derive from first principles.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:35.057347-05:00","updated_at":"2025-12-20T20:53:26.119947-05:00","closed_at":"2025-12-20T20:53:26.119947-05:00","close_reason":"FIXED: Added explanation of Shannon's uniqueness theorem showing log is the unique function satisfying additivity, monotonicity, and continuity requirements for surprise"}
{"id":"babygpt-glfs","title":"Ch1 rendering: audit and remove stray spacing artifacts from {' '} and similar patterns","description":"Context\n- User request: there are visible weird spacing artifacts in rendered prose. They pointed to patterns like: sat\"\u003c/code\u003e{' '} and said there are multiple spots like this.\n\nWhere\n- Primary file: src/chapters/Chapter1.tsx\n- Search for: {' '} (literal {' '} in JSX)\n\nTask\n1) Audit each {' '} insertion in Chapter1 and verify whether it produces visible double spaces / awkward wrapping.\n2) Replace where possible with normal spaces inside text nodes (or rewrite JSX so spaces occur naturally).\n3) Prefer using \u003cTerm\u003e over raw \u003ccode\u003e for inline tokens so spacing is simpler and code color is consistent.\n4) Visually verify the problem areas in the browser.\n\nAcceptance\n- No obvious extra blank areas or weird spacing around inline code/tokens in Chapter1.\n- No semantic changes to content; only whitespace/rendering polish.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:09.745719-05:00","updated_at":"2025-12-21T14:59:30.434537-05:00","closed_at":"2025-12-21T14:59:30.434537-05:00","close_reason":"Removed all {' '} spacers in Chapter1; replaced a couple inline \u003ccode\u003e tokens with \u003cTerm\u003e and rewrote JSX to keep natural spacing; build passes.","dependencies":[{"issue_id":"babygpt-glfs","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.416811-05:00","created_by":"daemon"}]}
{"id":"babygpt-gmw","title":"2.10 Gap: Why similar stats → nearby points (not proven)","description":"Line ~1129-1134: 'Similar statistics → similar gradients → nearby points' is CLAIMED but not PROVEN. It's a restatement, not a derivation. FIX: Add rigorous explanation. If tokens a,b have similar P(next|a) ≈ P(next|b), they make similar errors on similar examples. Similar errors = similar gradients. Gradient descent moves them in similar directions. Over many steps, they converge to nearby locations. Show this with the training viz.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:40.48566-05:00","updated_at":"2025-12-20T21:53:06.0745-05:00","closed_at":"2025-12-20T21:53:06.0745-05:00","close_reason":"Fixed: Concrete derivation - 'a' and 'e' both precede 'n', both get nudged toward E['n']. Same targets → same directions → nearby endpoints."}
{"id":"babygpt-gs7l","title":"Generalization Wall: restore crossover/overflow feedback marker","description":"Context\n- The \"Generalization Wall\" viz (component `src/components/ExplosionDemo.tsx`) has a log-scale bar meant to communicate when training data stops covering the combinatorial space.\n- Reviewer feedback: the CENTER feedback (\"crossover\" / \"overflow\" threshold) has gotten lost in recent iterations. The viz needs an explicit, unmissable sense of: (a) where the ratio=1 threshold is, and (b) where the current setting sits relative to it.\n\nWhere\n- `src/components/ExplosionDemo.tsx`\n- `src/components/ExplosionDemo.module.css`\n\nWhat to build\n1) Keep the existing fixed crossover line at ratio=1, but add a second marker for the CURRENT position:\n   - A small dot/handle sitting on the bar track at `barPosition`.\n   - A label anchored to that dot that shows the current ratio in plain terms, e.g. `ratio = 4.9e-7` (or `coverage = 0.000049%`).\n   - The label should visually indicate which side of crossover we are on: left side = \"more data than possibilities\"; right side = \"most sequences unseen\".\n2) Make the crossover/overflow meaning explicit in 1 sentence in-viz (not a long paragraph), near the bar:\n   - Example wording: \"Crossover (ratio=1): data count equals number of possible contexts.\" (Adjust wording to match existing tone.)\n3) Preserve visual language:\n   - Use `VizCard` outer surface (already used).\n   - Use semantic accents (`--accent-cyan` for crossover; green/yellow/red consistent with the rest).\n   - No new random hex colors.\n\nAcceptance criteria\n- At any T=1..10, a reader can immediately answer: \"Are we left or right of crossover, and by roughly how much?\"\n- No layout overflow on mobile (~390px) or desktop.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T20:32:19.385161-05:00","updated_at":"2025-12-23T21:33:02.563437-05:00","closed_at":"2025-12-23T21:33:02.563437-05:00","close_reason":"Added a movable 'Current' marker (dot + anchored label) on the bar, keeping crossover line; makes left/right-of-crossover obvious"}
{"id":"babygpt-gsy8","title":"Implement Hardcore Chapter 3 Plan","description":"Spec created in CHAPTER_3_SPEC.md. Ready for implementation phase.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-23T21:38:08.212796-05:00","updated_at":"2025-12-23T21:42:35.954612-05:00"}
{"id":"babygpt-gxpp","title":"Ch2: Cross-entropy feels unintroduced when first mentioned (re-sequence so it's earned)","description":"Problem\n- User feedback: “cross-entropy comes out of absolutely nowhere.”\n- We mention “softmax + cross-entropy” in the Softmax section before the reader has a grounded definition of cross-entropy (average surprise / −log probability).\n- This violates the project ethos: no unexplained expert vocabulary, no forward references without scaffolding.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.7` callout “Why the math plays nicely with training (softmax + cross-entropy)”\n- Section: `2.10` currently introduces loss/surprise/cross-entropy more fully.\n\nWhat to do\nChoose ONE clean fix (do not half-fix):\nOption A (recommended):\n- Remove/relocate the “softmax + cross-entropy” gradient callout from 2.7.\n- Reintroduce it after cross-entropy is defined in 2.10 (or immediately after the first time we define the loss).\n\nOption B:\n- Keep the idea in 2.7 but remove the term “cross-entropy”.\n- Phrase it as: “When we score predictions by −log(probability of the truth), the gradient becomes…”\n- Only do this if −log and “loss” have been introduced by that point (no new jargon).\n\nAcceptance criteria\n- The first time the reader sees “cross-entropy”, they also get the definition and a tiny example.\n- No forward references are required to understand the sentence.\n- `npm run build` passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T21:51:28.266042-05:00","updated_at":"2025-12-22T19:43:56.469228-05:00","closed_at":"2025-12-22T19:43:56.469228-05:00","close_reason":"Cross-entropy is now introduced where it first appears (Ch2 §2.10) with definition + tiny surprise example; Ch2 §2.7 no longer mentions cross-entropy."}
{"id":"babygpt-gzap","title":"Ch4 §4.3: tighten sourcing for numeric perplexity benchmark claims","description":"Context\n- The text cites a specific perplexity value for an (unpruned) Kneser–Ney 5-gram on the One Billion Word Benchmark.\n- Because this is a precise numeric claim, it will get fact-checked. The citation must nail the exact experimental setup (tokenization, vocabulary, pruning, etc.).\n\nWhere\n- Locate the paragraph that mentions:\n  - One Billion Word Benchmark\n  - Kneser–Ney 5-gram\n  - a specific perplexity number (e.g. ~67.6)\n- Likely file: `src/chapters/Chapter4.tsx` (or `src/chapters/Chapter2.tsx` if the benchmark is currently in Ch2). Use ripgrep to find \"One Billion Word\" and the numeric perplexity.\n\nTask\n1) Find the authoritative source for the exact number used.\n   - Prefer primary sources: benchmark paper, official report, or well-cited reproduction.\n2) Update citations so they are unambiguous:\n   - If the number is from a particular configuration, add a parenthetical like: \"(KN 5-gram, modified Kneser–Ney, [pruned/unpruned], [tokenization], [vocab], [interpolation])\".\n3) If the exact configuration cannot be verified quickly:\n   - Either soften the claim (e.g. \"~60–80\") OR\n   - Replace with a verified number and cite it.\n\nAcceptance criteria\n- A reader (or nitpicky reviewer) can trace the number to a source and see the setup matches.\n- No unsourced numeric benchmark remains.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T20:34:32.594717-05:00","updated_at":"2025-12-24T22:14:01.035873-05:00","closed_at":"2025-12-24T22:14:01.035873-05:00","close_reason":"Deferring: Chapter 4 content/files not in repo yet; will revisit sourcing once Ch4 exists"}
{"id":"babygpt-h150","title":"Ch2 §2.2: paint vs light — add one-line guardrail to avoid pedantic color-mixing derail","description":"Context\n- Chapter 2 uses a color-mixing analogy (paint/light) to justify coordinates and blending.\n- Current wording can invite avoidable pedantic derailments (e.g., \"yellow + blue light reads as white\") because spectral reality is messier than simplified RGB reasoning.\n\nWhere\n- `src/chapters/Chapter2.tsx` → Section 2.2 (paint vs light / color mixing). Find the sentence that describes blue+yellow (or complementary) light yielding white.\n\nTask\n1) Keep the analogy, but add a short, humble guardrail line so the claim is clearly scoped:\n   - Example: \"Under an RGB-style additive model (yellow ≈ red+green), adding blue gives white.\" \n   - Or: \"With complementary primaries, blue + yellow can be perceived as white.\" \n2) Do NOT turn this into a long physics digression; one sentence is enough.\n\nAcceptance criteria\n- The analogy still works, but a knowledgeable reader can't easily nitpick it into the ground.\n- Tone stays humble and precise.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T20:37:59.250215-05:00","updated_at":"2025-12-24T21:30:12.425991-05:00","closed_at":"2025-12-24T21:30:12.425991-05:00","close_reason":"Added a one-paragraph guardrail clarifying additive/RGB assumption for blue+yellow→white to prevent pedantic derail"}
{"id":"babygpt-hm0w","title":"Ch1: Move GeneralizationGapViz out of Exercises (it’s narrative, not an exercise)","description":"Context\n- The “Generalization Gap” visualization currently appears inside the Exercises section.\n- Feedback: it doesn’t belong there; it’s core narrative motivation.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Section 1.8 \"Exercises\" currently includes: \u003cGeneralizationGapViz /\u003e (around line ~1880)\n\nWhat to do\n- Move the viz (and its intro sentence if needed) to an earlier narrative section:\n  - Good candidates: end of Section 1.6 \"The Limit\" or right before Exercises begin.\n- Exercises section should begin with exercises, not a new concept.\n\nConstraints\n- Keep the copy around the exercises intact unless it no longer reads correctly after moving.\n\nDefinition of done\n- GeneralizationGapViz is in narrative flow, not in Exercises.\n- Exercises section is focused.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:40.141647-05:00","updated_at":"2025-12-23T01:51:48.308907-05:00","closed_at":"2025-12-23T01:51:48.308907-05:00","close_reason":"Moved GeneralizationGapViz into narrative flow (end of 1.7) and removed from Exercises section."}
{"id":"babygpt-hmy","title":"CorridorDemo UI: remove/replace 'Type to see the corridor narrow...' box; reuse that style for the input","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.30.36.png: the bottom prompt box (\"Type to see the corridor narrow...\") is not necessary, but the style of that box is good. User suggests using that style for the input textbox instead.\n\nWhere\n- Component: src/components/CorridorDemo.tsx (search for \"Type to see the corridor narrow...\")\n- Styles: src/components/CorridorDemo.module.css (.formula is the styled box with border-left accent)\n\nTask\n1) Remove or significantly de-emphasize the empty-state prompt inside the .formula area. Options: hide the formula block until input exists; or replace with a subtle hint near the input.\n2) Apply the “formula box” styling (background + left accent border) to the input area, so the input feels like the primary interaction target.\n3) Keep accessibility: input should remain clearly focusable and readable.\n4) Verify mobile layout (CorridorDemo uses a responsive grid).\n\nAcceptance\n- The demo no longer shows a big bottom prompt box when empty, or it is clearly secondary.\n- Input styling adopts the nice accent-left style and remains readable.\n- No layout regressions; npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:07.857136-05:00","updated_at":"2025-12-21T15:23:08.037218-05:00","closed_at":"2025-12-21T15:23:08.037218-05:00","close_reason":"Moved corridor prompt into input placeholder; hide formula box until input; styled input with left-accent formula treatment; npm run build passes.","dependencies":[{"issue_id":"babygpt-hmy","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.638852-05:00","created_by":"daemon"}]}
{"id":"babygpt-hn3","title":"GeometricDotProductViz: Polish sweep","description":"Sweep GeometricDotProductViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Compare against GradientDescentViz as gold standard","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:51:30.304017-05:00","updated_at":"2025-12-20T22:52:15.874626-05:00","closed_at":"2025-12-20T22:52:15.874626-05:00","close_reason":"Recreating with expanded gold standards"}
{"id":"babygpt-hp1","title":"1.13 Validate Ch2 map waypoint 2.10 The Handoff","description":"AUDIT: Navigate to 2.10, check if ChapterMap says 'The Handoff' but section is titled 'The Nudge'. IF title mismatch: decide which name is better and update for consistency. Verify 'updating when wrong' description.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:38.709081-05:00","updated_at":"2025-12-20T20:23:37.219635-05:00","closed_at":"2025-12-20T20:23:37.219635-05:00","close_reason":"FIXED: ChapterMap title changed from 'The Handoff' to 'The Nudge' to match Section 2.10 title","comments":[{"id":9,"issue_id":"babygpt-hp1","author":"andrewlouis","text":"NEEDS_FIX: Title mismatch - ChapterMap says 'The Handoff' but Section 2.10 is titled 'The Nudge'. Need to choose one title and update both places for consistency. Recommendation: check which title better captures the content (gradient updates vs. handoff to next layer).","created_at":"2025-12-21T00:37:56Z"}]}
{"id":"babygpt-hsy","title":"4.9 Audit 2.9 Synthesis dependencies","description":"AUDIT: Read section 2.9, verify it properly synthesizes 2.1-2.8. Check 'arc' summary is accurate to actual content. IF inaccurate: update synthesis to match reality.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:22.838707-05:00","updated_at":"2025-12-20T19:55:46.76411-05:00","closed_at":"2025-12-20T19:55:46.76411-05:00","close_reason":"Closed"}
{"id":"babygpt-hxa","title":"1.1.2 Gap: Why probabilities must sum to 1","description":"Line ~351-357: 'Probabilities must sum to 1' stated as requirement but WHY? Is it definition, constraint, or convention? FIX: Add explanation that sum-to-1 is the DEFINITION of exhaustive mutually exclusive outcomes. If you're certain SOMETHING happens, the total certainty must be 100%. This enables the 'probability mass' metaphor.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:44.802076-05:00","updated_at":"2025-12-20T20:54:58.273538-05:00","closed_at":"2025-12-20T20:54:58.273538-05:00","close_reason":"FIXED: Added explanation that sum-to-1 is the definition of exhaustive mutually exclusive outcomes, not just a constraint. Introduced 'probability mass' metaphor and conservation of certainty concept."}
{"id":"babygpt-i1kl","title":"Ch2 payoff: ensure a minimal implementable embedding LM is built (wire lookup → logits → loss)","description":"Context\n- Chapter 2 teaches the parts: embedding lookup, dot product, softmax, tensors.\n- Risk: without a clear \"synthesis\" where we wire these into a minimal language model (context → logits → loss), the chapter feels like a toolbox without showing the machine.\n\nWhere\n- `src/chapters/Chapter2.tsx` → Sections 2.9 (Synthesis) and 2.10 (The Nudge), plus any code walkthrough areas.\n- Confirm whether the chapter already includes an explicit end-to-end forward pass and training objective.\n\nTask\n1) Add (or strengthen) a short, explicit \"minimal model\" section that shows:\n   - Inputs: context token id(s)\n   - Embedding lookup: `E[i]`\n   - Compute logits over next tokens with a linear layer: `ℓ = h W_out + b`\n   - Softmax to get probabilities\n   - Loss as negative log-prob of the correct next token\n2) Keep the model intentionally small and inspectable:\n   - If using 1-token context (bigram-style), say so.\n   - If using multi-token context, pick a simple composition rule (sum or concat) and show it plainly.\n3) Provide one micro worked example so the reader can trace the numbers:\n   - Doesn't need huge matrices; use a tiny vocab (e.g. 3 tokens) and tiny D.\n   - Goal is mechanical understanding, not realism.\n\nAcceptance criteria\n- A reader can state: \"Given token IDs, here's how we compute P(next token) and here's what we minimize in training.\" \n- This appears before or alongside the start of code implementation in the chapter.\n- `npm -s run build` passes.","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-23T20:40:10.838146-05:00","updated_at":"2025-12-23T20:40:10.838146-05:00"}
{"id":"babygpt-i3d","title":"3.5 Audit 1.1.4 Conditional Probability dependencies","description":"AUDIT: Read section 1.1.4, verify P(X|Y) 'given' notation is introduced BEFORE chain rule in 1.1.5 uses it. IF missing: create ticket to add conditional probability primer.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:32.427399-05:00","updated_at":"2025-12-20T19:42:01.496245-05:00","closed_at":"2025-12-20T19:42:01.496245-05:00","close_reason":"Closed"}
{"id":"babygpt-i3g","title":"7.4 Difficulty audit: Ch1 internal flow","description":"AUDIT: Map difficulty curve through Ch1 (theory-\u003eimpl-\u003etheory). IF abrupt jumps: create tickets to add transition paragraphs at specific locations. Document the curve.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:29.199543-05:00","updated_at":"2025-12-20T19:44:56.133937-05:00","closed_at":"2025-12-20T19:44:56.133937-05:00","close_reason":"Closed"}
{"id":"babygpt-i7a","title":"CSS: Refactor large files (DotProductViz 997 LOC, CharacterClusterViz 952 LOC)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:13.535944-05:00","updated_at":"2025-12-20T23:31:12.937696-05:00","closed_at":"2025-12-20T23:31:12.937696-05:00","close_reason":"Skip - files are well-structured, refactoring is high-risk low-reward"}
{"id":"babygpt-iesn","title":"Gradient intro: add a derivative visual (slope/tangent) + align with reader questions","description":"Context (what the reader sees): In Chapter 2's gradient introduction, the prose jumps to derivative/gradient language without a concrete picture of \"derivative = slope\". At that moment, the reader is likely staring at symbols and asking: \"what is this measuring, mechanically?\"\n\nGoal: Add a small, in-family visual stepping stone that makes slope/tangent/\"downhill\" tangible *before* we generalize to gradients in many dimensions.\n\nPlan / Requirements:\n1) Add a new mini-viz (new component) showing a simple 1D curve f(x) with an interactive point x. Render:\n   - the point (x, f(x))\n   - a tangent line at that point\n   - numeric readouts for x and slope f'(x)\n2) Optional but valuable: a toggle to show a secant line (finite difference) with Δx and Δf, so the reader sees \"slope\" become \"derivative\" as Δx→0.\n3) Place the viz directly in the Chapter 2 gradient intro, immediately before the first time we use gradient notation.\n4) Keep copy gentle and concrete: define what changes, what stays fixed, and what a negative slope means.\n5) Follow visual-language.md (VizCard + panel-dark/inset-box). Responsive at 390px; no clipping/overflow.\n\nAcceptance:\n- A reader can explain why \"step opposite the slope\" reduces the function in 1D, and then accept the jump to many dimensions.\n- Build passes.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-23T16:13:08.072168-05:00","updated_at":"2025-12-24T21:02:37.369726-05:00","closed_at":"2025-12-24T21:02:37.369726-05:00","close_reason":"Added DerivativeViz (slope/tangent + finite difference) and integrated into Ch2 §2.10; build passes"}
{"id":"babygpt-igj","title":"2.6 Validate Ch2 invariant: Replace tables with shared matrices","description":"AUDIT: Read section 2.2, verify 'replace context tables with shared matrices' claim. IF missing: create ticket to add explicit comparison diagram. IF present but unclear: add transition paragraph.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:18.757813-05:00","updated_at":"2025-12-20T19:41:00.558501-05:00","closed_at":"2025-12-20T19:41:00.558501-05:00","close_reason":"Closed"}
{"id":"babygpt-imn","title":"6.5 Terminology audit: logits vs scores","description":"AUDIT: grep 'logit'/'score' in Ch2. IF used interchangeably without explanation: add parenthetical '(also called scores)' at first logits usage. Document findings.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.928108-05:00","updated_at":"2025-12-20T20:29:53.058318-05:00","closed_at":"2025-12-20T20:29:53.058318-05:00","close_reason":"PASS: 'logits' consistently used for raw outputs before softmax (line 821: 'dot product gives us logits'). 'scores' used as general term for similarity scores. Relationship clear."}
{"id":"babygpt-imo","title":"CodeWalkthrough: Polish sweep","description":"Audit CodeWalkthrough for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (code highlighting, step transitions)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:06.164784-05:00","updated_at":"2025-12-20T22:54:57.804254-05:00","closed_at":"2025-12-20T22:54:57.804254-05:00","close_reason":"Wrong - this IS a gold standard (Dec 19), not needing sweep"}
{"id":"babygpt-io8m","title":"Ch1 pedagogy: strengthen the transition into context length (Markov assumption → what gets lost)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.43.21.png. The section setting up context length needs a stronger, more intuitive mental model before we move on.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Area: after decomposition/Markov assumption intro and before later “Sliding Window” mechanics.\n- Look for the paragraph that starts: “What dies when you truncate context?”\n\nTask\n1) Rewrite/expand this transition so the reader can clearly answer:\n   - What “context length” means operationally (what the model can see)\n   - What information is thrown away when n is small\n   - Why increasing context helps but explodes data needs\n2) Consider adding a tiny illustrative example with 2 different context windows (n=2 vs n=5) using the same sentence.\n3) Keep it gentle: explain in words first; keep math optional.\n\nAcceptance\n- Reader has a robust mental model of context length before the chapter moves on.\n- No abrupt jumps; tone stays supportive.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:09.998251-05:00","updated_at":"2025-12-21T14:29:06.381459-05:00","closed_at":"2025-12-21T14:29:06.381459-05:00","close_reason":"Expanded Markov/context-length transition with concrete n-gram window example (keys/cabinet sentence) and explicit memory↔data trade-off; build passes.","dependencies":[{"issue_id":"babygpt-io8m","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.635655-05:00","created_by":"daemon"}]}
{"id":"babygpt-j2e","title":"5.2 Validate SectionLink in 1.3 to 1.1","description":"AUDIT: Line 1164 in Ch1, find SectionLink to 1.1. Verify 'Decomposition Strategy' matches what 1.1 calls it. IF different name: update to match actual terminology used in 1.1.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:39.640729-05:00","updated_at":"2025-12-20T19:43:40.445855-05:00","closed_at":"2025-12-20T19:43:40.445855-05:00","close_reason":"Closed"}
{"id":"babygpt-j3v","title":"10.5 Transition audit: 2.4-\u003e2.5","description":"AUDIT: Check bridge from 'vectors are storage' concept to 'how lookup works' mechanics. IF abrupt: create ticket to add 'Now let's see how to actually retrieve these vectors.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:28.276122-05:00","updated_at":"2025-12-20T20:18:36.111869-05:00","closed_at":"2025-12-20T20:18:36.111869-05:00","close_reason":"Closed"}
{"id":"babygpt-j5q","title":"2.4 Gap: Circular reasoning on coordinates","description":"Line ~444-445: 'Coordinates make sense because of statistical relationships' — but this is circular. We ASSERT embeddings should exist because stats are countable. WHY do countable stats = valid coordinates? FIX: Add the missing step: coordinates are valid if operations on them (add, scale, dot) correspond to meaningful operations on the original objects. Stats form a vector space (can add, scale). That's the justification.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:51.738121-05:00","updated_at":"2025-12-20T21:38:06.397128-05:00","closed_at":"2025-12-20T21:38:06.397128-05:00","close_reason":"Fixed: replaced vague 'vector operations mirror statistical operations' with concrete example - averaging 'a' and 'e' vectors gives high value in followed-by-n slot because both precede n"}
{"id":"babygpt-j6l","title":"3.13 Audit 1.3.2 Free Lunch dependencies","description":"AUDIT: Read section 1.3.2, verify causal masking 'looking right is forbidden' is adequately motivated. IF unclear why: add 'you can't know the future when predicting' callout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:55.221605-05:00","updated_at":"2025-12-20T19:42:29.111412-05:00","closed_at":"2025-12-20T19:42:29.111412-05:00","close_reason":"Closed"}
{"id":"babygpt-jhyq","title":"Redesign: reduce extreme vertical real estate for a core explanation block/viz","description":"Context (screenshots 2025-12-23 00:02:27 and 00:04:52): a core explanation block/viz takes an excessive amount of vertical space and the concept ramp feels abrupt. Feedback: “redesign this; it can be as impactful without taking an incredible amount of space” + “go first principles… a beautiful organic descent into clarity”.\n\nTask\n1) Identify the exact block/viz:\n   - Reproduce by navigating the chapter until you find the section matching the screenshots.\n   - Record: chapter + section number + component name + file paths.\n2) Redesign goals (layout + pedagogy):\n   - Shorter/denser without becoming busy.\n   - Desktop: prefer 2-column layout (explanation left, viz right).\n   - Mobile: stack cleanly.\n   - Remove redundant headers/padding layers.\n   - Use the visual system: `VizCard` for figures, `.panel-dark` / `.inset-box` for nested panels; semantic colors via `--accent-*`.\n3) First-principles ramp:\n   - If the block introduces new notation/math, add a minimal “plain language → notation” lead-in before symbols.\n4) Validate:\n   - No overflow/clipping.\n   - Interactions still work.\n\nAcceptance criteria\n- The block is materially shorter (less scroll) while preserving clarity.\n- No clipping/overflow at ~390/768/1280 widths.\n- `npm -s run build` passes.\n\nImplementation note\nBecause screenshots aren’t machine-readable in CI, include in your PR notes what you changed relative to the observed problems:\n- excessive vertical spacing,\n- abrupt conceptual jump,\n- any specific elements that were too large.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T02:17:57.35532-05:00","updated_at":"2025-12-23T03:21:10.11384-05:00","closed_at":"2025-12-23T03:21:10.11384-05:00","close_reason":"Redesigned EmbeddingGradientViz to reduce vertical real estate by tabbing the right panel (Controls vs Probabilities) while keeping the main plot visible; build passes."}
{"id":"babygpt-jmf0","title":"SoftmaxLandscapeViz: first-principles onboarding (guided stepping stones)","description":"Context (what the reader sees): `SoftmaxLandscapeViz` (Fig. 2.6) shows a 3D field of pillars. The reader can rotate the view, select which token's probability is used as height (P('e') / P('a') / P('i')), and adjust temperature. There is also a readout that shows Δℓ(e), Δℓ(a), ℓ(i)=0 and the resulting probabilities at the hovered/pinned pillar.\n\nProblem: The viz is conceptually dense. The current UI assumes the reader already knows what \"Δℓ(e)\" means, why one logit is fixed to 0, and how the 3D grid corresponds to changing scores. The reader needs stepping stones in the *main path* of the visualization (not a hidden/optional side quest), and the wording must not talk down to them.\n\nGoal: Rework the visualization's in-viz explanation so it leads the reader thought-to-thought, using concrete definitions they can verify by moving one control at a time. This is not optional; it's the primary way the viz teaches.\n\nPlan / Requirements:\n1) Replace the current footer text + single-line axis hint with a short, structured onboarding inside the chart (or immediately above it) that is always visible and very compact.\n2) Step the reader through *definitions*, not slogans:\n   - Start from: \"We have three scores ℓ(e), ℓ(a), ℓ(i).\"\n   - Define the plane explicitly: \"We fix ℓ(i)=0 and vary Δℓ(e)=ℓ(e)−ℓ(i), Δℓ(a)=ℓ(a)−ℓ(i).\" (Explain why by showing it's just choosing a reference.)\n   - Define height: \"For each (Δℓ(e), Δℓ(a)) pair, compute softmax and take P(selectedToken). That's the pillar height.\"\n   - Define temperature effect: \"Changing T reshapes the same surface (sharper vs flatter).\"\n3) Tie each definition to an action: \"drag to rotate\", \"click to pin\", \"change T\" with one sentence each.\n4) Keep it visually calm: 3–5 short lines max, no big callout boxes, no mic-drop tone.\n5) Remove expert-y wording like \"guided mode\" / \"smart amateur\". This is just the viz speaking clearly.\n\nAcceptance:\n- A reader can explain, in plain language, what each axis means and what a pillar represents after 30 seconds of interaction.\n- Build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T16:13:08.789844-05:00","updated_at":"2025-12-23T20:58:03.656904-05:00","closed_at":"2025-12-23T20:58:03.656904-05:00","close_reason":"Replaced patronizing/optional framing with always-on first-principles overlay + focused readout; no guided-mode language"}
{"id":"babygpt-jpz","title":"Rewrite chain rule + log-prob explanation; reframe naive index; clarify KenLM tokens","description":"Update chapter prose: explain why joint probability multiplies via conditional filtering example; show why logs help compute; reframe naive approach as brute-force index of sequences; insert a simple graph viz (reuse NgramGraphViz); clarify that KenLM is an n-gram LM typically word-level but can treat any tokenization (chars/words).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:20:38.153799-05:00","updated_at":"2025-12-21T10:45:34.282017-05:00","closed_at":"2025-12-21T10:45:34.282017-05:00","close_reason":"Updated Ch1 joint-prob explanation: infinite-resources lookup framing + NgramGraphViz, added 'Why AND multiplies' callout and clearer logs example, clarified KenLM tokenization"}
{"id":"babygpt-k85","title":"1.1.7.1 Gap: Backoff mathematical justification","description":"Line ~1064: Backoff introduced with ladder metaphor but mathematical mechanism missing. WHY is shortening context valid? FIX: Add explanation: if P('sat'|'the cat') is undefined (zero count), we ASSUME it's similar to P('sat'|'cat') or even P('sat'). This is the smoothing assumption — shorter contexts are less precise but non-zero. We trade accuracy for coverage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:48.40969-05:00","updated_at":"2025-12-20T21:17:05.959569-05:00","closed_at":"2025-12-20T21:17:05.959569-05:00","close_reason":"FIXED: Added backoff ladder explanation + made limit precise: 'the cat sat' and 'the dog sat' remain separate keys, sharing requires exact substring match"}
{"id":"babygpt-kcu","title":"2.2 Gap: HOW modeling tokens solves reuse","description":"Line ~306-309: 'Stop memorizing contexts, start modeling tokens' — but HOW does this solve reuse? Claim stated, not derived. FIX: Add derivation: if 'cat' and 'dog' have similar embeddings, then contexts containing them produce similar predictions. One learned embedding serves ALL contexts containing that token. V embeddings serve V^T contexts. That's the compression: V \u003c\u003c V^T.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:19.739476-05:00","updated_at":"2025-12-20T21:28:49.181037-05:00","closed_at":"2025-12-20T21:28:49.181037-05:00","close_reason":"FIXED: Added derivation showing compression math (V embeddings serve V^T contexts). One embedding for 'cat' serves all contexts containing it. Concrete example: vocab 27, T=3 → 19,683 entries vs 27. Claim now derived from first principles."}
{"id":"babygpt-kic","title":"PHASE 7: Difficulty Curve","description":"Audit difficulty spikes and transitions (5 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:59.794166-05:00","updated_at":"2025-12-20T20:30:35.430159-05:00","closed_at":"2025-12-20T20:30:35.430159-05:00","close_reason":"Difficulty curve audit complete. All issues passed. Collapsibles used appropriately for math spikes."}
{"id":"babygpt-kks","title":"PHASE 3: Ch1 Dependency Audit","description":"Audit conceptual dependencies for each Chapter 1 section (15 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:58.613251-05:00","updated_at":"2025-12-20T20:30:21.076498-05:00","closed_at":"2025-12-20T20:30:21.076498-05:00","close_reason":"Ch1 dependency audit complete. 36/37 issues passed. Fixed: perplexity definition added before first use."}
{"id":"babygpt-kmd","title":"8.2 Gap audit: dot product","description":"AUDIT: Find first usage of dot product, verify definition precedes it. IF gap: create ticket to add conceptual intro before formula.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:46.23742-05:00","updated_at":"2025-12-20T20:09:28.205004-05:00","closed_at":"2025-12-20T20:09:28.205004-05:00","close_reason":"Closed"}
{"id":"babygpt-ko7","title":"PHASE 4: Ch2 Dependency Audit","description":"Audit conceptual dependencies for each Chapter 2 section (11 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:58.911021-05:00","updated_at":"2025-12-20T20:30:21.969548-05:00","closed_at":"2025-12-20T20:30:21.969548-05:00","close_reason":"Ch2 dependency audit complete. Fixed: SectionLinks corrected, Ch1 reference added in 2.3."}
{"id":"babygpt-kp5","title":"Softmax: Elevate from standard to exceptional","description":"Unique insight: Softmax is the ONLY answer to 'least-biased distribution given scores' (max entropy theorem). Add: (1) 'Why exp?' callout explaining inevitability, (2) SoftmaxSimplexViz showing 3-logit sliders → point on triangle, (3) Boltzmann connection as optional collapsible, (4) gradient cancellation explanation.","status":"closed","issue_type":"feature","created_at":"2025-12-20T22:13:13.034021-05:00","updated_at":"2025-12-20T22:25:16.177739-05:00","closed_at":"2025-12-20T22:25:16.177739-05:00","close_reason":"Implemented SoftmaxSimplexViz with probability triangle + temperature, added max-entropy callout (why exp is ONLY answer), gradient callout, and Boltzmann physics connection. Accessible and democratic."}
{"id":"babygpt-kr02","title":"Ch1: Make context-length scaling painfully precise (sliding window → |V|^T contexts)","description":"Context\n- In Chapter 1 we jump from a long-range dependency example (\"The boy who lived…\" → later pronoun) straight to a formula (Possible Sequences = Vocab^Context).\n- The reader is missing the *mechanics* of what’s being counted: a sliding window of length T, a lookup table keyed by that context, and why the table size explodes.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Section: 1.6 \"The Limit\"\n- Find the paragraph containing: \"English has long-range dependencies. If a sentence starts with \\\"The boy who lived...\\\"\"\n- Current code is around line ~1764 and the MathBlock right below it is:\n  - equation=\"\\\\text{Possible Sequences} = \\\\text{Vocab}^{\\\\text{Context}}\"\n\nWhat to change\n1) Add the missing bridge: define *context length* T as a sliding window.\n   - For each position t, the model conditions on the previous T characters/tokens and predicts the next one.\n   - Make it explicit that we are not predicting whole 100‑char sequences at once; we are predicting 1 step at a time with a length‑T context.\n\n2) Make the counting precise:\n   - Number of possible contexts of length T over a vocabulary |V| is |V|^T.\n   - A lookup table is not “just |V|^T things”. It maps each context to a *distribution* over next tokens (|V| probabilities).\n   - So the table stores on the order of |V|^T × |V| numbers (plus overhead).\n\n3) Add a tiny concrete toy example (keep it small and readable):\n   - Example vocabulary size |V|=3 (e.g. {a,b,␣}) and context length T=2 → 3^2=9 contexts.\n   - For each context, the table stores a 3‑way next‑token distribution.\n\n4) Keep the big punchline (80^100 ≈ 10^190), but explain where “80” comes from (typical character set including punctuation/space).\n   - Ensure the prose stays humble/clear (no mic‑drop sentences).\n\nConstraints\n- Keep the diff localized to this section; do not reformat the whole file.\n- No forward references; define terms before using them.\n\nDefinition of done\n- The section explicitly explains sliding window → contexts → table size.\n- Math and prose agree on what is being counted.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:36.004027-05:00","updated_at":"2025-12-21T19:14:50.161682-05:00","closed_at":"2025-12-21T19:14:50.161682-05:00","close_reason":"Added explicit sliding-window → context keys → |V|^T contexts and |V|^{T+1} stored values; kept 80^100 punchline with explanation; build passes"}
{"id":"babygpt-ktx8","title":"Components: Extract DataRow (label/value pair) reusable component","description":"Context\n- Multiple components render the same UI pattern: a small mono label + a colored value, aligned in a row (label/value display).\n- This duplication makes it hard to keep spacing/colors consistent.\n\nWhere\n- Search for duplicated patterns in src/components:\n  - Common classnames: statLabel/statValue, numberLabel/numberValue, label/value pairs.\n  - Use ripgrep for \"statLabel\" and \"numberLabel\".\n\nTask\n1) Identify at least 3 components that use the same label/value row pattern.\n2) Create a new component DataRow:\n   - File: src/components/DataRow.tsx\n   - Styles: src/components/DataRow.module.css\n   - Export it from src/components/index.ts\n3) API (suggested):\n   - label: ReactNode\n   - value: ReactNode\n   - emphasis?: \"default\" | \"cyan\" | \"yellow\" | \"muted\" (or similar)\n   - align?: \"left\" | \"right\" (optional)\n4) Refactor the identified components to use DataRow, keeping visuals identical or better.\n\nAcceptance\n- Less duplicated markup/CSS in the chosen components.\n- No visual regressions.\n- npm run build passes.","status":"closed","priority":3,"issue_type":"task","created_at":"2025-12-21T19:34:28.654275-05:00","updated_at":"2025-12-23T02:12:44.940019-05:00","closed_at":"2025-12-23T02:12:44.940019-05:00","close_reason":"Added reusable DataRow component (label/value) + exported from components barrel; refactored DecoderDemo controls to use it; build passes; will sync/push."}
{"id":"babygpt-kua","title":"Extract StepDots reusable component (step indicator pattern)","description":"Extract the repeated step indicator UI (dots, active state, click handlers) into a StepDots component and migrate call sites.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:24.591116-05:00","updated_at":"2025-12-21T18:13:53.171544-05:00","closed_at":"2025-12-21T18:13:53.171544-05:00","close_reason":"Extracted reusable StepDots component (mini + numbered variants) and migrated SlidingWindowDemo + GradientTraceDemo; build passes"}
{"id":"babygpt-kydh","title":"Ch2 Embeddings: properly introduce e_q and e_u notation + intuition","description":"Context (screenshot 2025-12-23 00:01:37): reader feedback: “we don’t really introduce e_q and e_u properly”. The chapter uses embedding notation (`e_c`, `e_q`, `e_u`) before giving a crisp definition.\n\nWhere to edit\n- `src/chapters/Chapter2.tsx` around Sections 2.3–2.6.\n  - Use `rg -n \"e_q|e_u|e_c\" src/chapters/Chapter2.tsx` to find first use.\n- Also check visual labels/captions in: `src/components/CharacterClusterViz.tsx`, `src/components/DotProductViz.tsx`, `src/components/EmbeddingInspector.tsx`, `src/components/EmbeddingGradientViz.tsx`.\n\nGoal\nAdd a short first-principles “notation + meaning” ramp before the first time `e_q`/`e_u` appears.\n\nConcrete requirements\n1) Add a small “Notation” paragraph + `MathBlock` (or 3–4 bullet list) defining:\n   - `E` is the embedding table (matrix).\n   - `E[c]` means “row lookup”: the D numbers for token/character `c`.\n   - `e_c` is shorthand for that row (a vector).\n   - Example instantiation: `e_q = E['q']`, `e_u = E['u']`.\n2) Immediately tie it to an existing chapter example:\n   - In “qu-heavy” text, `q` is often followed by `u`, so training tends to pull `e_q` toward `e_u` (or make `dot(e_q, e_u)` large).\n3) Keep the ontology precise:\n   - The vector is not “inside the letter”; it is learned from behavior in the corpus.\n4) Ensure vizzes use the same notation string-for-string.\n\nAcceptance criteria\n- No instance of `e_q` or `e_u` appears before the definitions.\n- Reader can answer: “what is `e_q` (in code and in meaning)?” after reading the ramp.\n- `npm -s run build` passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T02:18:06.961305-05:00","updated_at":"2025-12-23T02:33:45.832385-05:00","closed_at":"2025-12-23T02:33:45.832385-05:00","close_reason":"Added early Chapter 2 notation ramp for embeddings (E, E[c], e_c) and explicit e_q/e_u examples tied to 'qu' intuition; build passes."}
{"id":"babygpt-kymi","title":"Ch2 Gradient: make intro more gradual (gentler descent into ∇, η, update rule)","description":"Context (screenshot 2025-12-23 00:00:52): the gradient section ramps too fast into symbols/updates. Reader feedback: “Gentler descent into this”.\n\nWhere to edit\n- Primary: `src/chapters/Chapter2.tsx` → Section 2.10 “The Nudge”. Find the paragraph that starts with “So we have a scoreboard (loss). Now we need a steering wheel…” and the line “In 1D, you call it slope…”, plus the first appearance of `∇` and `η`.\n- Secondary: labels/captions in `src/components/NeuralTrainingDemo.tsx`, `src/components/GradientTraceDemo.tsx`, `src/components/GradientDescentViz.tsx` if they reference gradient/η and need to match the new ramp.\n\nGoal\nRewrite the intro to gradient descent so a smart reader without calculus can follow without whiplash.\n\nConcrete requirements\n1) Introduce the idea in three steps (no heavy notation at first):\n   - Step A (1D): “slope” with one tiny numeric example (e.g., f(x)=x^2 at x=2 → slope positive → step left decreases f).\n   - Step B (2D): “slope in x + slope in y” → gradient as an arrow pointing uphill.\n   - Step C (many dims): same idea scales; now introduce `∇` (gradient) and `η` (learning rate).\n2) Define `η` in plain English BEFORE any equation uses it.\n3) Connect explicitly to the training loop: predict → compute loss → gradient tells “which parameter nudges increase loss fastest” → step opposite → repeat.\n4) Keep voice humble/clear; avoid minimizing language (“toy”, “easy”, “no magic”).\n5) No forward references.\n\nAcceptance criteria\n- The first appearance of `∇` and `η` is after the plain-English explanation.\n- The update rule reads as a summary, not as a jump cut.\n- `npm -s run build` passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T02:17:52.456987-05:00","updated_at":"2025-12-23T02:28:59.439271-05:00","closed_at":"2025-12-23T02:28:59.439271-05:00","close_reason":"Rewrote Section 2.10 gradient intro with 1D→2D→many-D ramp; define η before ∇ and update rule; build passes."}
{"id":"babygpt-ld77","title":"Ch1 explain: introduce Kneser–Ney smoothing from first principles (avoid name-drop)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.44.37.png. The chapter currently name-drops “Kneser–Ney 5-gram” without explaining what it is or why it matters.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: “Kneser–Ney”\n\nTask\n1) Add a first-principles explanation before/near the first mention:\n   - Why raw counts produce zeros (sparsity)\n   - What smoothing/backoff is trying to do (share probability mass)\n   - What makes Kneser–Ney special at a high level (continuation probability intuition)\n2) Keep the explanation concrete and avoid long math derivations. Use one small example (e.g., unseen bigram but common word).\n3) After the explanation, it is okay to mention the paper and benchmark.\n\nAcceptance\n- “Kneser–Ney” is no longer a pure name-drop; reader gets an intuition.\n- Explanation is short, concrete, and non-scary.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:10.253331-05:00","updated_at":"2025-12-21T13:34:00.913052-05:00","closed_at":"2025-12-21T13:34:00.913052-05:00","close_reason":"Added first-principles smoothing/backoff + Kneser–Ney intuition (continuation contexts) near first mention; removed name-drop feel; build passes.","dependencies":[{"issue_id":"babygpt-ld77","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.862719-05:00","created_by":"daemon"}]}
{"id":"babygpt-lfn","title":"4.8 Audit 2.8 Tensors dependencies","description":"AUDIT: Read section 2.8, verify it requires embedding lookup from 2.5. Check [B,T,D] notation has adequate context. IF shape notation unclear: add dimension explanation table.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:22.541482-05:00","updated_at":"2025-12-20T19:55:05.509377-05:00","closed_at":"2025-12-20T19:55:05.509377-05:00","close_reason":"Closed"}
{"id":"babygpt-lgn","title":"Extract VizCard reusable component (header + content + footer pattern from 11 vizzes)","description":"Create src/components/VizCard.tsx + VizCard.module.css to unify the common viz wrapper: ambient glow container, glass card, header (title + optional figNum/subtitle), content slot, optional footer. Export from src/components/index.ts and refactor initial viz components to use it.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:22.055457-05:00","updated_at":"2025-12-21T00:24:44.23236-05:00","closed_at":"2025-12-21T00:24:44.23236-05:00","close_reason":"Added VizCard component and migrated key vizzes"}
{"id":"babygpt-lhx","title":"1.1.5 Gap: What Markov assumption loses","description":"Line ~489-495: Markov Assumption introduced as solution to sparsity, but doesn't explain what we're LOSING. FIX: Add concrete example of information death. E.g., 'The doctor said she would...' — with n=2, we only see 'would' and lose 'doctor' and 'she'. Gender agreement, long-range coherence, topic — all die. This is WHY we eventually need attention.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:17.217057-05:00","updated_at":"2025-12-20T21:06:49.151068-05:00","closed_at":"2025-12-20T21:06:49.151068-05:00","close_reason":"FIXED: Added concrete example showing information loss from Markov truncation - 'The doctor said she would...' example demonstrates loss of gender agreement, professional roles, and topic when context window is too small"}
{"id":"babygpt-ljn9","title":"CodeWalkthrough: Add breathing room so highlight bar doesn’t crowd code (Screenshot 2025-12-21 18.10.16)","description":"Context\n- Screenshot 2025-12-21 18.10.16: The magenta/red highlight sits too close to the code characters.\n- It should feel like an annotation, not like it’s touching the text.\n\nWhere\n- src/components/CodeWalkthrough.module.css\n- Look at `.newLine` styling (inset bar + background)\n\nWhat to do\n- Increase left padding/indent for highlighted lines so the inset bar has breathing room.\n- Keep code alignment consistent between highlighted and non-highlighted lines.\n- Ensure the highlight still spans the full line width.\n\nDefinition of done\n- Highlight bar is visually separated from code glyphs.\n- No horizontal jitter when stepping through CodeWalkthrough.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:39.866807-05:00","updated_at":"2025-12-22T15:26:48.925015-05:00","closed_at":"2025-12-22T15:26:48.925015-05:00","close_reason":"Added per-line padding in CodeWalkthrough so magenta highlight bar doesn’t crowd code."}
{"id":"babygpt-ljve","title":"Ch2 GradientStepViz: make one gradient step feel inevitable (sign + p−y)","description":"Context\n- We added `DerivativeViz` to ground slope, but the chapter still needs an Encore-style “one step you can feel” loop: commit a guess about direction, manipulate one knob, see loss change, and connect to p−y.\n- Existing NeuralTrainingDemo is strong but reads as a dashboard; this bead is about the micro “single logit slice” that bridges intuition to the update rule.\n\nGoal\n- Add a compact interactive mini-viz near Chapter 2 “The Nudge” section that:\n  (a) forces the reader to predict the sign of the update,\n  (b) shows how increasing/decreasing one score affects loss,\n  (c) introduces p−y as the direction signal (without dumping calculus).\n\nWhere\n- `src/chapters/Chapter2.tsx` Section 2.10 “The Nudge” (near DerivativeViz / gradient descent rule).\n- Implement as a new component in `src/components/*` (name suggestion: `GradientStepViz.tsx`). Export via `src/components/index.ts`.\n\nDesign spec\n1) Prompt (commit)\n   - Prompt: “If the model under-predicts the true token, should its score go up or down?”\n   - Provide two choices: “Up” / “Down” + “Lock guess”.\n\n2) Action (manipulate)\n   - ONE primary control: a slider for the “true token logit” (or score).\n   - Everything else is fixed (keep it 1D).\n\n3) State (see it)\n   - Show a curve of loss vs that logit (1D plot).\n   - Show the current point on the curve and a tangent arrow indicating slope direction.\n   - As the slider moves, the point moves; loss updates.\n\n4) Metric (score it)\n   - Show: p(true) and loss = -log p(true).\n   - Show p−y explicitly with y=1 for the true token: (p−1).\n   - After reveal: explain in 1–2 sentences: “If p is too small, p−1 is negative, so the update pushes the logit up.”\n\nVisual constraints\n- Must be compact (no scroll trap), in-family (VizCard/panels), semantic colors (cyan for true token).\n- No condescension: do not label as “optional” or “advanced”; this is the main bridge.\n\nAcceptance criteria\n- Reader can answer: “Why does the gradient push the true token's score up when it was underpredicted?” after doing the interaction once.\n- Works on mobile, no clipping.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T22:47:43.044139-05:00","updated_at":"2025-12-25T00:17:43.032905-05:00","closed_at":"2025-12-25T00:17:43.032905-05:00","close_reason":"Added GradientStepViz (commit→lock→reveal) showing loss vs true logit, tangent slope, and p−y sign; integrated into Ch2 §2.10 after DerivativeViz; build passes"}
{"id":"babygpt-lwe","title":"4.6 Audit 2.6 Dot Product dependencies","description":"AUDIT: Read section 2.6, verify it requires fingerprint from 2.3. Check SectionLink to 2.1 Grassmann is accurate. IF inaccurate: fix link or description.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.962273-05:00","updated_at":"2025-12-20T19:53:40.269015-05:00","closed_at":"2025-12-20T19:53:40.269015-05:00","close_reason":"Closed"}
{"id":"babygpt-m4l","title":"6.3 Terminology audit: embedding vs vector","description":"AUDIT: check first usage establishes relationship. IF ambiguous: add definition where 'embedding' first appears. Create ticket to add term to glossary if none exists.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.362548-05:00","updated_at":"2025-12-20T20:29:38.77957-05:00","closed_at":"2025-12-20T20:29:38.77957-05:00","close_reason":"PASS: Relationship established in Ch2 Section 2.4 'Vectors Are Just Storage' - vectors store attributes, embeddings are learned vectors for tokens."}
{"id":"babygpt-mcb","title":"CSS: Add spacing scale tokens to replace ad-hoc values","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:15.095696-05:00","updated_at":"2025-12-20T23:31:50.097453-05:00","closed_at":"2025-12-20T23:31:50.097453-05:00","close_reason":"Closed"}
{"id":"babygpt-miu","title":"1.8 Validate Ch2 map waypoint 2.1 Grassmann","description":"AUDIT: Navigate to 2.1 via waypoint, verify 'colors to language' description matches Grassmann section. IF mismatch: update description to reflect actual historical narrative focus.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:37.19632-05:00","updated_at":"2025-12-20T19:31:16.181193-05:00","closed_at":"2025-12-20T19:31:16.181193-05:00","close_reason":"Closed"}
{"id":"babygpt-mvs","title":"Ch1 layout: remove huge whitespace + gentler transition after lookup-table section","description":"Context\n- User report + screenshot: Screenshot 2025-12-21 at 10.23.35.png shows a large blank vertical gap around the end of the lookup-table/phone-book discussion.\n- In Chapter 1 this is near the paragraph that ends with: \"It's a phone book: entries don't talk to each other.\"\n- Immediately after, the text jumps to: \"The solution? Decomposition—...\"\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for the exact string: \"It's a phone book: entries don't talk to each other.\"\n- Also note there is a nested \u003cSection number=\"1.1.3\" ...\u003e inside \u003cSection number=\"1.1.2\" ...\u003e. Layout.module.css sets .section { margin-bottom: 80px; } which can create huge gaps when sections are nested.\n\nTask\n1) Reproduce the issue locally (npm run dev) and scroll to the location shown in the screenshot.\n2) Remove the excessive whitespace. Prefer structural fixes over magic padding: either\n   - Stop nesting \u003cSection\u003e components (make 1.1.3 a sibling section, not a child), OR\n   - Add CSS rule(s) in src/components/Layout.module.css so nested sections have smaller bottom margin.\n3) Add a gentler transition sentence/paragraph between the phone-book conclusion and the decomposition/Markov assumption intro. The goal is: no abrupt topic switch.\n4) Verify anchors still work (section links in ChapterMap).\n\nAcceptance\n- The large blank vertical gap is gone and spacing matches surrounding content.\n- Transition reads smoothly (no whiplash).\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:07.043583-05:00","updated_at":"2025-12-21T13:16:31.143589-05:00","closed_at":"2025-12-21T13:16:31.143589-05:00","close_reason":"Unnested 1.1.3 from 1.1.2 to remove extra section margin; added bridge into decomposition; verified npm run build.","dependencies":[{"issue_id":"babygpt-mvs","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:57.90807-05:00","created_by":"daemon"}]}
{"id":"babygpt-mw5","title":"PHASE 1: ChapterMap Validation","description":"Validate all ChapterMap waypoints link correctly and descriptions match content (14 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:48.567941-05:00","updated_at":"2025-12-20T20:30:19.252676-05:00","closed_at":"2025-12-20T20:30:19.252676-05:00","close_reason":"All ChapterMap validation issues resolved. Fixed: Ch2 title mismatch (Handoff→Nudge), added missing waypoints (2.5, 2.7, 2.8, 2.9)."}
{"id":"babygpt-n1nm","title":"SoftmaxSimplexViz: Improve beauty + reduce real estate (Screenshots 2025-12-21 18.13.48 / 18.16.28)","description":"Context\n- Screenshots 2025-12-21 18.13.48 and 18.16.28: The softmax/simplex visualization feels less polished than Chapter 1 vizzes and uses too much space.\n\nWhere\n- src/components/SoftmaxSimplexViz.tsx\n- src/components/SoftmaxSimplexViz.module.css\n- Used in src/chapters/Chapter2.tsx Section 2.7\n\nWhat to do\n1) Make layout more compact\n- Reduce wasted vertical space.\n- Ensure slider panel + viz panel balance on desktop and stack cleanly on mobile.\n\n2) Improve visual polish\n- Typography, spacing, and glass styling should match Chapter 1 vizzes.\n- Consistent label sizing.\n\n3) Optional interaction upgrade (nice-to-have)\n- Allow dragging the point in the simplex to update probs/logits (or allow direct manipulation in a clear way).\n\nDefinition of done\n- Viz looks “premium” and is more space-efficient.\n- Mobile layout remains usable.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:40.693254-05:00","updated_at":"2025-12-22T17:47:33.080056-05:00","closed_at":"2025-12-22T17:47:33.080056-05:00","close_reason":"SoftmaxSimplexViz is now more compact and in-family; also supports direct manipulation (drag inside simplex) to explore distributions."}
{"id":"babygpt-n36","title":"10.2 Transition audit: 1.1.8-\u003e1.2","description":"AUDIT: Check bridge from chain rule application to 'let's build the tokenizer'. IF abrupt: create ticket to add transition like 'Theory in hand, time to build.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.382782-05:00","updated_at":"2025-12-20T19:53:45.624286-05:00","closed_at":"2025-12-20T19:53:45.624286-05:00","close_reason":"Closed"}
{"id":"babygpt-n3z","title":"2.14 Validate Ch2 invariant: Training nudges based on error","description":"AUDIT: Read section 2.10, verify training loop mechanism is clear (predict, measure error, nudge). IF missing: add diagram or pseudo-code. IF unclear: add step-by-step walkthrough.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:21.091193-05:00","updated_at":"2025-12-20T19:48:21.392133-05:00","closed_at":"2025-12-20T19:48:21.392133-05:00","close_reason":"Closed"}
{"id":"babygpt-n5zk","title":"Rewrite Chapter 3 to match the high-fidelity standard of Chapters 1 and 2","description":"Ensure narrative depth, visual language (custom components), formal rigor, and interactive verification match the established style.","status":"open","issue_type":"feature","created_at":"2025-12-23T21:04:53.491705-05:00","updated_at":"2025-12-23T21:04:53.491705-05:00"}
{"id":"babygpt-n5zk.1","title":"Implement 'The Physics of the Nudge' section (Derive p-y visually)","status":"closed","issue_type":"task","created_at":"2025-12-23T21:04:57.228769-05:00","updated_at":"2025-12-25T15:56:39.818001-05:00","closed_at":"2025-12-25T15:56:39.818001-05:00","close_reason":"Implemented SoftmaxNudgeViz (commit→lock→reveal) that derives p−y by finite-difference measurement and shows p, y, and p−y side-by-side; integrated into Ch2 §2.10; build passes"}
{"id":"babygpt-n5zk.2","title":"Add 'Numerical Stability' section (Max-subtraction derivation)","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-23T21:04:57.530018-05:00","updated_at":"2025-12-23T21:04:57.530018-05:00","dependencies":[{"issue_id":"babygpt-n5zk.2","depends_on_id":"babygpt-n5zk","type":"parent-child","created_at":"2025-12-23T21:04:57.530601-05:00","created_by":"daemon"}]}
{"id":"babygpt-n5zk.3","title":"Implement 'Chain of Blame' visual component","status":"in_progress","issue_type":"task","created_at":"2025-12-23T21:04:57.848707-05:00","updated_at":"2025-12-25T16:00:40.935649-05:00"}
{"id":"babygpt-n5zk.4","title":"Upgrade Worked Examples with interactive number stepping","status":"open","priority":1,"issue_type":"task","created_at":"2025-12-23T21:04:58.143725-05:00","updated_at":"2025-12-23T21:04:58.143725-05:00","dependencies":[{"issue_id":"babygpt-n5zk.4","depends_on_id":"babygpt-n5zk","type":"parent-child","created_at":"2025-12-23T21:04:58.144314-05:00","created_by":"daemon"}]}
{"id":"babygpt-n5zk.5","title":"Add 'The Gradient Audit' section with finite difference rigorous walkthrough","status":"open","issue_type":"task","created_at":"2025-12-23T21:04:58.432115-05:00","updated_at":"2025-12-25T15:46:43.399693-05:00"}
{"id":"babygpt-n6b","title":"Chain Rule: Add Jurafsky-Martin level formal rigor","description":"FormalRigor component: Collapsible with 'Formal' badge. Content: (1) P(B|A) definition → chain rule identity, (2) induction to n, (3) P(context)=0 edge case (formal smoothing motivation), (4) probability space Omega/F/P, (5) optional measure theory hook. Each part connects back to corridor intuition.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-20T22:13:14.680484-05:00","updated_at":"2025-12-20T22:21:24.652277-05:00","closed_at":"2025-12-20T22:21:24.652277-05:00","close_reason":"FormalRigor component implemented with accessible, democratic tone"}
{"id":"babygpt-na49","title":"SoftmaxLandscapeViz: make axes/orientation grokkable (labels, units, hover readouts)","description":"Context (what the reader sees): In `SoftmaxLandscapeViz` (Fig. 2.6), the main graphic is an isometric 3D grid of vertical pillars (a 15×15 field). Height is the selected probability (left panel \"Height shows…\" with tabs like P('e'), P('a'), P('i')). The base plane corresponds to two logit offsets (currently implemented as Δℓ(e) and Δℓ(a) with ℓ(i)=0).\n\nProblem: Even though there's a single-line hint (\"Left→right: Δℓ(e). Front→back: Δℓ(a). Height: P(token).\") and two small axis labels near the base, the orientation/axes are still hard to grok at a glance:\n- It's not obvious which direction is \"front→back\" vs \"left→right\" once the perspective changes.\n- There are no tick marks / corner anchors / \"you are here\" orientation cues.\n- The reader can't easily map a clicked pillar to the underlying (Δℓ(e), Δℓ(a)) coordinates and resulting P values without hunting.\n\nGoal: Make the plot readable as a coordinate system. A first-time reader should quickly answer: (1) what the two horizontal axes represent, (2) what height represents, (3) how to read a point.\n\nPlan / Requirements:\n1) Add an in-plot axis frame (inside the chart area) that remains stable under rotation: two axis arrows + labels: \"Δℓ(e)\" and \"Δℓ(a)\" with direction markers (e.g., \"−6\" at one end, \"+6\" at the other).\n2) Add minimal tick marks or corner labels on the base plane so front/back isn't ambiguous.\n3) Add a small HUD overlay (top-right) that shows the currently focused coordinates and value: Δℓ(e)=…, Δℓ(a)=…, and P(selectedToken)=… for the hovered or pinned pillar. (The component already computes these; surface them more prominently.)\n4) Ensure the axis/help overlays do not steal pointer events from the chart (use pointer-events: none except on buttons).\n5) Responsive: at 390px width, overlays must wrap and stay inside the card; no horizontal overflow.\n\nAcceptance:\n- A reader can interpret the axes without reading surrounding prose.\n- No clipping/bleed; build passes (`npm -s run build`).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T16:13:06.680956-05:00","updated_at":"2025-12-23T20:58:03.39622-05:00","closed_at":"2025-12-23T20:58:03.39622-05:00","close_reason":"Added axis arrows + in-plot overlay explaining base/height/T + focused readout; orientation is grokkable"}
{"id":"babygpt-ns40","title":"Ch2 SoftmaxSimplexViz: reuse bars controls, add simplex challenge + better labeling","description":"Context\n- The probability simplex viz is good, but surrounding layout/labels can be confusing and not as elegant as other vizzes.\n- We want a two-step stair: bars first, then simplex with the SAME controls so the mapping feels continuous.\n\nGoal\n- Refactor `SoftmaxSimplexViz` so it reuses the exact same score controls as the bars step, and make the simplex labels/legend first-principles readable.\n\nWhere\n- `src/components/SoftmaxSimplexViz.tsx` (+ CSS module).\n- Used in `src/chapters/Chapter2.tsx` softmax sections.\n\nDesign spec\n1) Control reuse\n   - The sliders for score(e), score(a), score(i) should match SoftmaxBarsViz (same order, same colors, same typography).\n   - Temperature is visible here (this is where temperature earns itself).\n\n2) Prompt (commit)\n   - Challenge text: “Find a setting where a is almost impossible, but e and i still compete.”\n   - Add “Lock attempt” to freeze and reveal scorecard.\n\n3) State (see it)\n   - Point moves in triangle as probabilities change.\n   - The triangle's corners must be labeled with token names (e/a/i), not confusing percent labels.\n   - The “uniform” center marker should be obvious and readable.\n   - Legend/labels must be on/over the graph (not detached).\n\n4) Metric (score it)\n   - Show P(e), P(a), P(i) numerically (mono).\n   - If perplexity/entropy isn't introduced yet in that section, do NOT introduce new jargon here. Keep metric to probabilities + ratio thresholds.\n\nVisual constraints\n- Reduce visual clutter: fewer nested boxes; consistent padding; no overflow beyond card bounds.\n- In-family surfaces + semantic colors.\n\nAcceptance criteria\n- Reader can point to: “this dot is my distribution” and explain corners vs center.\n- No clipping/overflow at narrow widths.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T22:46:35.753609-05:00","updated_at":"2025-12-25T00:05:31.45728-05:00","closed_at":"2025-12-25T00:05:31.45728-05:00","close_reason":"Updated SoftmaxSimplexViz with lockable challenge + reset, disables controls when locked, simplified vertex labels to token names, added clearer uniform center marker; build passes"}
{"id":"babygpt-p09k","title":"ExplosionDemo: Prevent line breaks in Generalization Wall numeric lines (Screenshot 2025-12-21 18.06.17)","description":"Context\n- Screenshot 2025-12-21 18.06.17: The new Generalization Wall is better, but key numeric lines wrap awkwardly.\n- This breaks the “one glance” readability.\n\nWhere\n- src/components/ExplosionDemo.tsx\n- src/components/ExplosionDemo.module.css\n- Focus areas:\n  - Possibilities line: `27^T ≈ …`\n  - Expansion line in \u003cdetails\u003e: `27^T = 27 × 27 × …`\n\nWhat to do\n- Adjust CSS so these expressions do not wrap mid-expression.\n- Preferred behavior:\n  - Keep on one line; if too long, allow horizontal scroll within the expansion area.\n  - Do not shrink font into illegibility.\n\nDefinition of done\n- No awkward wraps for T=10.\n- Mobile view still usable.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:38.759169-05:00","updated_at":"2025-12-22T15:32:25.497982-05:00","closed_at":"2025-12-22T15:32:25.497982-05:00","close_reason":"Prevented metric wrapping in ExplosionDemo by forcing coverage value to stay on one line (nowrap)."}
{"id":"babygpt-p74a","title":"Ch2 §2.1: Fix 'you get a vector space' claim (be precise about structure vs representation)","description":"Problem\n- In `src/chapters/Chapter2.tsx` Section 2.1, we currently say:\n  - “If your objects satisfy these rules — linearity and commutativity — you get a vector space.”\n- This is imprecise. A vector space is a *mathematical structure* (a set + operations satisfying axioms). We don’t “get” one automatically; we *choose* a representation and define operations that behave like vector addition and scalar multiplication.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.1 Grassmann's Insight`\n- Around the AxiomViz + the paragraph starting “If your objects satisfy these rules…”\n\nWhat to change\n- Rewrite that paragraph to be technically correct while staying readable.\n- Key points to include:\n  1) The axioms are about the *operations* (how mixing/scaling behaves), not about the objects having vectors “inside them.”\n  2) If mixing/scaling behaves like this, then it’s meaningful to *model the attributes* as coordinates and use vector math.\n  3) For characters: the symbol is still discrete; the embedding is the chosen coordinate representation.\n\nTone / style\n- Avoid “math mic-drop” phrasing.\n- Do not claim language itself is commutative; clarify we’re talking about mixing *attributes/representations*, not word order in sentences.\n\nAcceptance criteria\n- The revised paragraph would not make a math-trained reader cringe.\n- A non-math reader still understands the gist (“we’re allowed to treat attributes as coordinates when the algebra behaves”).\n- `npm run build` passes.\n","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:49:22.89886-05:00","updated_at":"2025-12-22T16:03:42.937479-05:00","closed_at":"2025-12-22T16:03:42.937479-05:00","close_reason":"Rewrote the 'you get a vector space' claim to be precise: we model attribute bundles as vectors; the objects aren’t inherently vectors."}
{"id":"babygpt-p78","title":"5.3 Validate SectionLink in 2.6 to 2.1","description":"AUDIT: Line 718 in Ch2, find SectionLink to 2.1. Verify Grassmann backward reference accurately reflects 2.1 content. IF inaccurate: update link text to match.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:39.923788-05:00","updated_at":"2025-12-20T20:01:39.580451-05:00","closed_at":"2025-12-20T20:01:39.580451-05:00","close_reason":"Closed"}
{"id":"babygpt-pqb","title":"3.14 Audit 1.6 The Limit dependencies","description":"AUDIT: Read section 1.6, verify it summarizes sparsity limit and properly sets up Ch2. IF Ch2 not foreshadowed: add explicit 'next chapter will...' statement. IF sparsity not summarized: add recap.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:55.507254-05:00","updated_at":"2025-12-20T19:42:29.404919-05:00","closed_at":"2025-12-20T19:42:29.404919-05:00","close_reason":"Closed"}
{"id":"babygpt-pzck","title":"Ch1: Reduce 'weird grouping' — avoid stacked callouts where content is core","description":"Context\n- Screenshot 2025-12-21 18.04.37: Some content feels awkwardly boxed/grouped (likely stacked callouts) instead of flowing as core narrative.\n- Goal: fewer boxes, clearer hierarchy.\n\nWhere (start here)\n- File: src/chapters/Chapter1.tsx\n- Find around the end of Section 1.1 where two callouts appear back-to-back:\n  - “This Becomes the Training Objective”\n  - “The Training Objective”\n- Current code around line ~1249.\n\nWhat to do\n- Decide the right structure:\n  Option A: merge into a single callout (one title, less repetition).\n  Option B: make the essential sentences core \u003cParagraph\u003e text and keep only the truly “callout-worthy” part boxed.\n- Remove repeated phrasing.\n- Maintain BabyGPT tone (humble, clear, not preachy).\n\nDefinition of done\n- The section reads smoothly without feeling like a stack of boxes.\n- Visual hierarchy is clearer.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:38.202336-05:00","updated_at":"2025-12-23T02:04:07.881114-05:00","closed_at":"2025-12-23T02:04:07.881114-05:00","close_reason":"Reduced stacked callouts in Ch1 by consolidating duplicated Training Objective callouts, merging the zero-probability + memorization example into one warning callout, and turning context-window/sampling callouts into core paragraphs."}
{"id":"babygpt-q4t","title":"2.4 Gap: Proper nouns vs adjectives conflation","description":"Line ~428-438: The metaphor conflates two claims: (1) integers are just labels, (2) vectors enable learning. These aren't the same. WHY can't labels learn? FIX: Separate the claims. Labels can't learn because they have no shared structure. 'cat'=17, 'dog'=42 — there's no operation that moves 17 toward 42. Vectors CAN be nudged: v_cat += 0.01 * gradient. Continuous space enables gradient descent.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:43.994118-05:00","updated_at":"2025-12-20T21:32:51.782488-05:00","closed_at":"2025-12-20T21:32:51.782488-05:00","close_reason":"FIXED: Added gradient descent mechanism - integers can't learn because no operation moves 17 toward 42; vectors enable v += 0.01*gradient in continuous space"}
{"id":"babygpt-q5cb","title":"KenLMDemo: redesign Linear Probing demo to match gold window motif (compact grid + slot inspector)","description":"Context: The interactive 'KenLM / Linear Probing' demo (src/components/KenLMDemo.tsx + KenLMDemo.module.css) currently renders 16 large slot cards (2-column grid) which makes the demo extremely tall and visually mismatched vs BabyGPT's 'gold' demos. User feedback: 'design this from first principles it looks so rubbish' (screenshot 2025-12-22 23.58.25).\n\nGoal: Keep the existing teaching interaction (query input + presets + step-by-step probe trace), but redesign the layout + surfaces to feel like BabyGPT's premium 'engineer's window' motif (see docs/assets/visual-audit/gold/audit-ch1-kenlm.png and gold/audit-ch1-corridor.png) and the visual spec in docs/visual-language.md.\n\nRequirements:\n- Outer container uses the same window header motif as CorridorDemo (top bar + 3 dots) with mono title.\n- Reduce vertical real estate:\n  - Memory should render as a compact 4×4 grid of small cells (or equivalent density), not 16 full cards.\n  - Empty slots must be visually minimal (not full '(empty)' cards).\n- Provide a 'Slot Inspector' panel that shows the packed-array fields (Hash/Prob/Backoff + key + home index) for the currently active/visited slot; optional hover-to-inspect is a plus.\n- Visual semantics:\n  - Cyan: home slot / primary signal\n  - Yellow: active probe step\n  - Magenta: collision\n  - Green: hit\n  - Red: empty/miss\n- Preserve existing behavior:\n  - Step 0: idle (Trace Lookup)\n  - Step 1: hash computed / home index known\n  - Step 2+: probe steps (collision/hit/empty)\n  - Reset behavior unchanged.\n- After changes: run npm -s run build, ensure TS build passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T00:19:30.656979-05:00","updated_at":"2025-12-23T00:48:24.866791-05:00","closed_at":"2025-12-23T00:48:24.866791-05:00","close_reason":"KenLMDemo redesigned to match gold visual language; compact 4×4 grid + slot inspector; build passing; merged and pushed."}
{"id":"babygpt-qdut","title":"Chapter 2: convert key callout into core narrative (no sidebar-only insight)","description":"Context (what the reader sees): In Chapter 2's softmax section, a foundational bridge currently lives inside a callout box. Specifically, there is a callout titled **\"Softmax only cares about differences (and that's a lifesaver)\"** that explains: adding the same constant to every logit leaves softmax unchanged, which permits the standard numerical-stability trick of subtracting the max logit.\n\nProblem: This idea is core to the story (it's both a conceptual bridge and a practical implementation fact), so it should be part of the main narrative flow. Keeping it boxed makes it feel optional, even though later content/code silently depends on it.\n\nGoal: Move the *core* explanation into the main prose, and leave only genuinely optional extras (full algebraic cancellation) in a callout/details block.\n\nPlan / Requirements:\n1) In `src/chapters/Chapter2.tsx`, locate the callout with title \"Softmax only cares about differences (and that's a lifesaver)\".\n2) Pull the key idea into 1–2 normal `Paragraph` blocks right after the softmax equation: \"Adding c to every logit doesn't change probabilities, so we can subtract max for stability.\"\n3) Keep the derivation (the algebra line-by-line) as optional: either a callout or a `\u003cdetails className=\"collapsible\"\u003e` so it's there for readers who want it.\n4) Ensure the code-facing instruction (subtract max logit) remains explicit in the main path.\n\nAcceptance:\n- Reader gets the \"subtract max\" trick as a natural part of the story without opening a callout.\n- Build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T16:13:10.193913-05:00","updated_at":"2025-12-24T00:26:30.070606-05:00","closed_at":"2025-12-24T00:26:30.070606-05:00","close_reason":"Moved softmax invariance (only differences matter + subtract max) into main prose; kept algebra as optional collapsible; build passes"}
{"id":"babygpt-qdvh","title":"Ch2 §2.7: Expand the physics connection for temperature (make it tactile + add citations)","description":"Problem\n- In `src/chapters/Chapter2.tsx` Section 2.7 (Softmax), we say:\n  - “Long before language models, people had the same problem… states with energies… probability mass…”\n- This needs more connective tissue and sources. The reader should feel the parallel, not just be told it.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.7 From Scores to Probabilities (Softmax)`\n- Around the paragraph beginning “If the word temperature feels oddly physical…”\n\nWhat to do\n1. Expand this mini-bridge by ~2–5 paragraphs.\n2. Make the analogy concrete:\n   - Explain “state” as a possible configuration (microstate) of a system.\n   - Explain “energy” as a score/cost: lower energy = more preferred.\n   - Explain that we need a rule that turns many unnormalized scores into a normalized probability distribution.\n3. Add citations (required). Use reputable primary or textbook sources for:\n   - Boltzmann distribution / Gibbs distribution\n   - Historical context around statistical mechanics (as needed)\n\nCitations mechanics\n- Use existing `\u003cCite n={...} /\u003e` and add entries to the Chapter 2 `\u003cCitations /\u003e` list.\n- Ensure numbering stays consistent and no Cite is missing an entry.\n\nAcceptance criteria\n- The physics connection reads as an “aha, same shape of problem” moment.\n- At least 2 citations added backing the physics claims.\n- `npm run build` passes.\n","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:49:51.37628-05:00","updated_at":"2025-12-22T19:38:18.159181-05:00","closed_at":"2025-12-22T19:38:18.159181-05:00","close_reason":"Rewrote Ch2 §2.7: microstate/atom bridge + first-principles exp/normalize derivation (incl 2-state example), introduced simplex before viz, added citations (7–11); build passes."}
{"id":"babygpt-qh0","title":"4.5 Audit 2.5 Embedding Lookup dependencies","description":"AUDIT: Read section 2.5, verify it requires embedding table concept from 2.4. Check row selection mechanics are adequately motivated. IF unclear: add visual or SectionLink.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.678121-05:00","updated_at":"2025-12-20T19:52:55.44936-05:00","closed_at":"2025-12-20T19:52:55.44936-05:00","close_reason":"Closed"}
{"id":"babygpt-qseb","title":"Ch2 DotOverlapViz: dot product as distribution overlap (interactive game)","description":"Context\n- Feedback: the current \"Dot Product: Overlap\" section/dialogue feels clunky and not first-principles.\n- We want Samwho-style tight loop: commit prediction -\u003e manipulate -\u003e visible state -\u003e metric.\n\nGoal\n- Rework the dot-product overlap viz into a small interactive \"overlap game\" that makes dot product feel inevitable as \"shared probability mass\".\n\nWhere\n- Existing component likely `src/components/DotProductViz.tsx` or similarly named (search \"Dot Product: Overlap\" / \"P(match)\").\n- It is referenced in `src/chapters/Chapter2.tsx` near dot product sections.\n\nDesign spec (must implement all)\n1) Prompt (commit)\n   - Add a short prompt: \"Pick A and B. Predict: will overlap be high or low?\"\n   - Provide a two-choice commit toggle: \"High overlap\" / \"Low overlap\" + \"Lock guess\".\n   - Until locked, hide the final dot value (show \"?\" or blurred).\n\n2) Action (manipulate)\n   - Primary controls: choose token A and token B (pills or dropdowns).\n   - Keep controls minimal; any presets (\"vowels\", \"rare pair\") can remain but should be secondary and consistent styling.\n\n3) State (see it)\n   - Show two aligned histograms: P(next | A) and P(next | B), same bins, stacked vertically or side-by-side.\n   - Hover/tap a bin highlights that bin in BOTH histograms and highlights the corresponding term in the sum (visual linkage).\n   - Display the top 3 contributing bins (by pA*pB) as chips/rows.\n\n4) Metric (score it)\n   - Show the dot formula in-place: dot(A,B) = Σ p_i(A) p_i(B)\n   - Show running value dot(A,B) (mono), plus baseline 1/V.\n   - After reveal, compare to guess with gentle feedback (no snark).\n\nVisual constraints\n- Legend must be inside/over the chart area (not floating off southwest).\n- Keep surfaces in-family: VizCard for outer; .panel-dark / .inset-box inside.\n- Use semantic colors: cyan/magenta for the two distributions; yellow for highlights; green/red only for correct/incorrect guess feedback.\n\nAcceptance criteria\n- Reader can say: \"dot is high when probability mass lands on the same next-characters\" after interacting once.\n- Works with keyboard + touch (hover equivalent via click/selection).\n- No overflow/clipping at narrow widths.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T22:42:37.411429-05:00","updated_at":"2025-12-24T23:38:41.182698-05:00","closed_at":"2025-12-24T23:38:41.182698-05:00","close_reason":"Reworked DotProductViz into commit→lock→reveal loop; added aligned-bin panel with hover-linked terms + top contributors; hid score until reveal; build passes"}
{"id":"babygpt-r1p","title":"4.10 KNOWN: Audit 2.10 loss function introduction","description":"KNOWN ISSUE: Cross-entropy loss may not be formally defined until this section. Check earlier loss refs","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:14:23.130719-05:00","updated_at":"2025-12-20T20:00:33.591449-05:00","closed_at":"2025-12-20T20:00:33.591449-05:00","close_reason":"Closed"}
{"id":"babygpt-r88","title":"2.2 Gap: WHY can't lookup tables generalize","description":"Line ~288: n-grams called 'islands' but WHY can't hash tables generalize? They can store ANY context... FIX: Add explanation: hash tables map keys to values with no notion of 'nearby keys'. Hash('dog sat') and Hash('cat sat') are unrelated integers. To generalize, you need GEOMETRY — a space where similar inputs are nearby. Hash tables have no geometry. This is the fundamental limit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:12.40026-05:00","updated_at":"2025-12-20T21:27:27.23295-05:00","closed_at":"2025-12-20T21:27:27.23295-05:00","close_reason":"FIXED: Added concrete explanation of hash table mechanism - hash functions map strings to unrelated integers (addresses/slots), not coordinates in a space. Change one letter and the hash jumps to a completely different memory location. No notion of distance built into the data structure."}
{"id":"babygpt-rbof","title":"SoftmaxLandscapeViz: interactive 3D softmax surface (prob vs logit gaps + temperature)","description":"Add a new viz to Chapter 2 softmax section that shows the global mapping from relative logits (Δℓ) to probabilities. This should *not* claim a continuous density over language; it visualizes softmax over 3 discrete options.\\n\\nImplementation: create src/components/SoftmaxLandscapeViz.tsx + src/components/SoftmaxLandscapeViz.module.css, export from src/components/index.ts, and render it in src/chapters/Chapter2.tsx within Section 2.7 (near SoftmaxWidget/SoftmaxSimplexViz).\\n\\nUX: A small pseudo‑3D 'landscape' made of pillars or a surface. Controls: temperature slider; toggle which token’s probability is shown as height (e.g. e/a/i); drag to rotate (pointer events like ConditioningShiftViz); click/hover a pillar to show readout (Δℓ_A, Δℓ_B, resulting P(e/a/i)).\\n\\nMath: Fix one logit to 0 (translation invariance), vary the other two over a range (e.g. [-6,6]); compute stable softmax with temperature. Show that T→0 sharpens and T→∞ flattens.\\n\\nAcceptance: build passes (npm run build), no perf issues (grid ~15–21). Styling consistent with other vizzes (VizCard, ambient glow, cyan/magenta/yellow palette).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T20:45:49.512673-05:00","updated_at":"2025-12-21T21:04:47.823549-05:00","closed_at":"2025-12-21T21:04:47.823549-05:00","close_reason":"Implemented SoftmaxLandscapeViz component + styles, exported via components barrel, embedded in Chapter 2 §2.7, and verified with npm run build."}
{"id":"babygpt-rd9","title":"1.11 Validate Ch2 map waypoint 2.4 Embedding Table","description":"AUDIT: Navigate to 2.4, verify 'D numbers per token' description matches 'Vectors Are Just Storage'. IF mismatch: update to reflect actual content.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:38.116683-05:00","updated_at":"2025-12-20T19:33:16.527827-05:00","closed_at":"2025-12-20T19:33:16.527827-05:00","close_reason":"Closed"}
{"id":"babygpt-res","title":"CodeWalkthrough: Polish sweep","description":"Sweep CodeWalkthrough for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (code highlighting, step transitions)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n\nReference gold standards:\n- GradientDescentViz (ambient glow, step interaction)\n- CrossEntropyViz (dynamic value display)\n- DotProductViz (panel layouts)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:57:34.641723-05:00","updated_at":"2025-12-20T23:01:57.716875-05:00","closed_at":"2025-12-20T23:01:57.716875-05:00","close_reason":"Polish sweep complete: ambient glow, multi-gradient backgrounds, header structure, transitions, hover states, mobile responsive breakpoints, ARIA labels"}
{"id":"babygpt-sa8n","title":"DotProductOverlapViz: rewrite dialogue/copy from first principles (clear + non-cramped)","description":"Problem (UI review screenshot 2025-12-23 12.59.12): The 'Dot Product: Overlap' visualization has confusing, cramped dialogue/copy. It throws formulas and terms without earning them, and the panel text feels like it’s fighting itself (too many mini-explanations, too many symbols, not a clean story).\n\nWhere:\n- Chapter 2 dot product overlap section (the card titled 'Dot Product: Overlap').\n- Component: locate via search for that title in src/components.\n\nReframe goal (first principles):\nExplain dot(A,B) as a probability of match in a way that a careful reader can reconstruct:\n1) A and B are probability distributions over the same alphabet (next-character outcomes).\n2) Imagine a two-step experiment: roll once from A, roll once from B.\n3) P(match) is the sum over outcomes i of P(i from A) * P(i from B).\n4) That sum is exactly the dot product when A and B are probability vectors.\n\nUX/copy requirements:\n- Replace the current top subtitle with a clean experiment statement (no 'dot(A,B) is P(match)' without context).\n- Replace 'TRY:' chip row copy with something calmer (e.g., 'Presets:' or remove entirely if redundant).\n- Right panel hierarchy:\n  - Big number: P(match)\n  - Small baseline: uniform baseline = 1/V\n  - One short sentence interpretation (what the number means)\n  - Optional details (formula expansion, top contributors) behind details/collapsible to avoid vertical competition.\n- Remove/rename the DOT/EUCLIDEAN toggle unless it is essential; if kept, explain the difference in one line.\n- Make the ‘Optional: how dot(A,B) becomes a probability’ section actually explain the above 4-step experiment, not just restate.\n\nVisual constraints:\n- Must match docs/visual-language.md: VizCard + panel-dark + inset-box; use semantic colors (cyan/magenta/yellow) intentionally.\n- No text overlap/clipping; responsive at 390/768/1280.\n\nAcceptance:\n- The copy reads like a single linear story with one 'aha' (dot = match probability), not a dashboard.\n- Build passes: npm -s run build.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T14:54:01.3678-05:00","updated_at":"2025-12-23T15:35:50.232239-05:00","closed_at":"2025-12-23T15:35:50.232239-05:00","close_reason":"Rewrote Dot Product overlap story; aligned styling to VizCard + inset panels; removed Euclidean toggle clutter"}
{"id":"babygpt-sbvf","title":"AbstractionChainViz: fix overflow/clipping + tighten abstraction chain story","description":"Problem (from UI review): The Abstraction Chain viz can break its card bounds: the rightmost station/glow and/or labels overflow the VizCard container on some viewports. Also re-check the abstraction chain labels/story for correctness and clarity (ensure each hop is justified, not hand-wavy).\\n\\nWhere: Chapter 2, section 2.1 figure (Abstraction Chain). Component: src/components/AbstractionChainViz.tsx + src/components/AbstractionChainViz.module.css.\\n\\nRepro:\\n1) Run dev server.\\n2) Open /chapter-02.\\n3) Scroll to “The Abstraction Chain (From Physics to Embeddings)”.\\n4) At common desktop widths (~1200–1400px) and small laptop widths (~900–1100px), verify that no station, glow ring, or label extends past the card edges or gets clipped awkwardly.\\n\\nFix goals:\\n- Ensure the entire rail + stations are visually contained in the VizCard, with consistent left/right padding and no clipped text.\\n- If glow rings cause overflow, constrain/scale them (e.g., reduce glow radius, add internal padding, set overflow hidden carefully).\\n- Audit the labels/hops: “Colors → Coordinates → Algebra → Structure → Embeddings/Learned Coordinates”. Confirm these are accurate and not implying invalid equivalences. Adjust microcopy to be first-principles clear.\\n\\nAcceptance:\\n- No overflow/clipping on mobile (~390px), tablet (~768px), and desktop (~1280px).\\n- Typography + colors match docs/visual-language.md (VizCard baseline).\\n- Chapter 2 build passes: npm -s run build.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T13:42:44.250471-05:00","updated_at":"2025-12-23T14:20:42.74173-05:00","closed_at":"2025-12-23T14:20:42.74173-05:00","close_reason":"Added overflow guard + safer station margins/glow; tightened microcopy for structure/embeddings."}
{"id":"babygpt-sbz","title":"6.6 Terminology audit: context_length vs block_size vs T","description":"AUDIT: find all three terms. IF not explicitly connected: add callout 'These three names mean the same thing: context_length, block_size, and T all refer to...'. Create ticket for fix.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:12.210875-05:00","updated_at":"2025-12-20T20:29:54.015098-05:00","closed_at":"2025-12-20T20:29:54.015098-05:00","close_reason":"PASS: T used consistently for context length. block_size only appears in code context. context_length explained in prose. Equivalence clear in Section 2.8."}
{"id":"babygpt-scr","title":"CSS: Create shared hover/focus state utilities","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:18.33111-05:00","updated_at":"2025-12-20T23:33:21.099617-05:00","closed_at":"2025-12-20T23:33:21.099617-05:00","close_reason":"Closed"}
{"id":"babygpt-sgx","title":"3.7 Audit 1.1.6 Building From Corpus dependencies","description":"AUDIT: Read section 1.1.6, verify it requires chain rule (1.1.5) and builds concrete counting examples. IF chain rule not referenced: add SectionLink. IF counting unclear: add FrequencyTable example.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:33.003599-05:00","updated_at":"2025-12-20T19:42:02.066115-05:00","closed_at":"2025-12-20T19:42:02.066115-05:00","close_reason":"Closed"}
{"id":"babygpt-sh2","title":"Restore misc missing sections","description":"Add: Sampling: Let It Talk, From Surprise to Loss, The Math: Chain Rule, Build a toy model, Build the pipeline, Define the goal, Hit the limit, What comes next, Pick the biggest term in the sum, Retrain on the current chapter corpus","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T18:34:15.938805-05:00","updated_at":"2025-12-20T18:49:01.787359-05:00","closed_at":"2025-12-20T18:49:01.787359-05:00","close_reason":"Closed","comments":[{"id":2,"issue_id":"babygpt-sh2","author":"andrewlouis","text":"Added: A Running Mini-Corpus, Telescoping With The Same Example, We Just Built a Markov Chain, What Chapter 1 Can't Do (Yet)","created_at":"2025-12-20T23:38:37Z"},{"id":3,"issue_id":"babygpt-sh2","author":"andrewlouis","text":"Verified: The Math: Chain Rule content already exists (Geometric Intuition callout + corridor metaphor)","created_at":"2025-12-20T23:39:04Z"},{"id":8,"issue_id":"babygpt-sh2","author":"andrewlouis","text":"Final status: Sampling: Let It Talk ADDED, From Surprise to Loss ADDED (previous session), The Math: Chain Rule exists as 'The Chain Rule', Build a toy model/Build the pipeline/Define the goal/Hit the limit are ChapterMap entries not content sections, What comes next exists as 'What's Next', Pick the biggest term is a button label, Retrain on corpus is a toggle label. All content verified present.","created_at":"2025-12-20T23:48:59Z"}]}
{"id":"babygpt-sly","title":"GradientTraceDemo: Polish sweep","description":"Sweep GradientTraceDemo for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (code highlighting, step transitions, output formatting)\n- Accessibility (aria labels, keyboard nav for step buttons)\n- Mobile responsiveness (two-panel layout on small screens)\n\nGold standards to reference:\n- GradientDescentViz (ambient glow, step-by-step interaction)\n- CrossEntropyViz (dynamic value display)\n- CodeWalkthrough (code panel styling, step highlighting)\n- NeuralTrainingDemo (training step visualization)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:52:31.766306-05:00","updated_at":"2025-12-20T23:13:41.555954-05:00","closed_at":"2025-12-20T23:13:41.555954-05:00","close_reason":"Closed"}
{"id":"babygpt-smnh","title":"ProbabilitySimplexViz: redesign UI for elegance (keep triangle, reduce clutter)","description":"Problem: Probability Simplex viz (Fig. 2.5) works functionally but layout feels boxy/cluttered vs other Chapter 2 vizzes (see screenshot 2025-12-23 13.01.48). Triangle itself is good; surrounding UI needs a more elegant, consistent visual language.\n\nWhere:\n- Component: locate by searching for title \"The Probability Simplex\" in src/components.\n- Styles: its CSS module in src/components.\n- Used in Chapter 2 softmax section.\n\nGoals (UX):\n- Keep the simplex triangle plot, but make the overall card calmer: fewer nested boxes, more whitespace, clearer hierarchy.\n- Left controls: more readable labels (avoid raw score('e') style), consistent typography with rest of Chapter 2.\n- Reduce visual competition: group controls/probabilities/helper text without stacking heavy bordered panels.\n- Probability readout: more legible and in-family (compact rows or mini-bars with consistent colors).\n- Responsive: 390/768/1280, no clipped text or awkward overflow.\n\nConstraints:\n- Follow docs/visual-language.md: VizCard + panel-dark/inset-box surfaces; semantic accent colors.\n- Preserve behavior (sliders, temperature, trail, clear trail).\n- Run npm -s run build.\n\nAcceptance:\n- Same functionality, but more elegant layout with better spacing/alignment and less dashboard feel.\n- Build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T15:51:27.720796-05:00","updated_at":"2025-12-23T16:10:03.249291-05:00","closed_at":"2025-12-23T16:10:03.249291-05:00","close_reason":"Redesigned Probability Simplex UI: calmer left panel sections, clearer labels, probability mini-bars, lighter surfaces while keeping triangle plot intact"}
{"id":"babygpt-syrk","title":"SoftmaxLandscapeViz: move legend overlay into plot (no bottom-left stray box)","description":"Context (what the reader sees): In `SoftmaxLandscapeViz` (Fig. 2.6) there's a \"legend\" worth of info split across two places: (a) left panel \"Height shows…\" token selector + temperature, and (b) a one-line axis hint above the SVG: \"Left→right: Δℓ(e). Front→back: Δℓ(a). Height: P(token).\" This reads like a legend, but it's outside the plot frame and easy to miss; it also looks visually detached (like it's south-west / outside the graph region).\n\nGoal: Make the legend feel like part of the plot by moving it into the chart area as an overlay.\n\nPlan / Requirements:\n1) Create a small overlay panel inside `.chartPanel` (top-right preferred) that contains the orientation legend (axis definitions + which token is height).\n2) Use in-family styling: `panel-dark` / `inset-box`, subtle translucent background, mono small-caps labels, semantic accent for the selected token.\n3) Ensure the overlay stays inside the SVG/container bounds at all widths. On small screens, allow it to collapse to a single-line summary (e.g., \"Δℓ(e)×Δℓ(a) → P('e')\") or stack.\n4) Overlay must not block chart interaction except on explicit interactive elements (buttons).\n\nAcceptance:\n- Legend never renders outside the plot boundary; no overlap with critical geometry; no horizontal scroll at 390/768/1280.\n- Build passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T16:13:07.385184-05:00","updated_at":"2025-12-23T20:58:03.12857-05:00","closed_at":"2025-12-23T20:58:03.12857-05:00","close_reason":"Legend is now an in-plot overlay (inside chart frame), not detached; verified on mobile+desktop"}
{"id":"babygpt-t4d","title":"10.4 Transition audit: Ch1-\u003eCh2","description":"AUDIT: Check ChapterNav + Section 1.7 adequately prepare reader for Ch2's philosophical style (Grassmann history). IF gap: create ticket to adjust 1.7 ending or add Ch2 preamble.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.97311-05:00","updated_at":"2025-12-20T20:16:39.15769-05:00","closed_at":"2025-12-20T20:16:39.15769-05:00","close_reason":"Closed"}
{"id":"babygpt-t50","title":"GeometricDotProductViz: Polish sweep","description":"Sweep GeometricDotProductViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n\nGold standards to reference:\n- GradientDescentViz (ambient glow, formula display, step interaction)\n- CrossEntropyViz (curve rendering, guide lines, axis labels)\n- CodeWalkthrough (panel layouts)\n- NeuralTrainingDemo (interactive training viz)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:52:29.993735-05:00","updated_at":"2025-12-20T23:06:35.831951-05:00","closed_at":"2025-12-20T23:06:35.831951-05:00","close_reason":"Polish sweep complete: vector gradients, keyboard nav (arrow keys + shift), touch support, ARIA labels, cubic-bezier transitions, focus states, mobile responsive, theme-consistent rgba colors"}
{"id":"babygpt-t74","title":"3.9 Audit 1.1.7.2 Sparsity Trap dependencies","description":"AUDIT: Read section 1.1.7.2, verify it builds on counting from 1.1.6. Check zero-probability problem is clearly introduced before cross-entropy formula. IF gap: add bridging explanation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:54.089992-05:00","updated_at":"2025-12-20T19:42:28.227795-05:00","closed_at":"2025-12-20T19:42:28.227795-05:00","close_reason":"Closed"}
{"id":"babygpt-tfi","title":"CSS: Add breakpoint tokens (currently hardcoded 600/700/768px)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:19.989928-05:00","updated_at":"2025-12-20T23:34:13.078193-05:00","closed_at":"2025-12-20T23:34:13.078193-05:00","close_reason":"Closed"}
{"id":"babygpt-tfpp","title":"AbstractionChainViz: fix overflow + tighten concept labels","description":"Context (screenshot 2025-12-23 11:37:28)\n- In Chapter 2, the “The Abstraction Chain” card (Fig. 2.1) overflows horizontally: the last node is clipped beyond the right edge of the card.\n- The chain also feels slightly off conceptually (duplicate “Coordinates” label; story may be confusing).\n\nWhere\n- Component: src/components/AbstractionChainViz.tsx and src/components/AbstractionChainViz.module.css\n- Used in: src/chapters/Chapter2.tsx near the start (Section 2.1).\n\nTask A — Fix the layout overflow\n1) Reproduce on common widths: ~390px, ~768px, ~1280px.\n2) Remove the clipping:\n   - The nodes row must either wrap gracefully into 2 rows on narrow widths OR scroll intentionally (but never clip).\n   - If you choose scrolling, add subtle affordance (e.g., gradient fade edges) so it feels intentional.\n3) Avoid fixed widths that force overflow. Ensure flex/grid children use min-width: 0 where needed.\n4) Verify the card looks correct inside VizCard and does not create horizontal page scroll.\n\nTask B — Tighten the conceptual chain\n1) Rename the final node to avoid repeating “Coordinates” twice.\n   - Suggested: last node label becomes “Embeddings” or “Learned Coordinates”.\n2) Ensure the chain reads linear left→right:\n   - Colors (measurable) → Coordinates (numbers) → Algebra (operations) → Structure (language patterns) → Embeddings (learned coordinates)\n3) Update the footer sentence so it matches the chain precisely.\n   - Keep voice humble + clear. Avoid mic-drop / minimizing language.\n\nAcceptance\n- No clipping/overflow at 390/768/1280 widths.\n- Labels + footer copy read coherently.\n- npm -s run build passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T11:39:12.248986-05:00","updated_at":"2025-12-23T11:46:31.73127-05:00","closed_at":"2025-12-23T11:46:31.73127-05:00","close_reason":"Fixed AbstractionChainViz node overflow by adding rail margins (stations no longer sit at 0%/100%); renamed final node to Embeddings + tightened copy for a clearer chain; build passes."}
{"id":"babygpt-tk7","title":"5.5 Audit implicit Ch2-\u003eCh1 references","description":"AUDIT: Search Ch2 for Ch1 concept reuse (P(next|c), tokenization, chain rule). Check if proper backward refs exist when reusing. IF no refs: add 'As established in Chapter 1...' callouts where appropriate.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:40.517066-05:00","updated_at":"2025-12-20T20:04:39.341073-05:00","closed_at":"2025-12-20T20:04:39.341073-05:00","close_reason":"Closed"}
{"id":"babygpt-tog7","title":"ContextExplosionViz: reduce vertical real estate","description":"Context (screenshot 2025-12-23 11:38:11)\n- In Chapter 2, Fig. 2.2 “The Context Explosion” card is too tall (takes ~2 scrolls to get past). User wants it smaller/tighter without losing clarity.\n\nWhere\n- Component: src/components/ContextExplosionViz.tsx + src/components/ContextExplosionViz.module.css\n- Used in: src/chapters/Chapter2.tsx around the early “reuse question / context explosion” section.\n\nGoal\nMake the viz materially shorter while keeping the key comparison intact:\n- lookup-table parameter count grows like V^T × V\n- embedding-table parameter count stays ~V×D (+ output layer) and doesn’t scale with T\n\nConcrete requirements\n1) Reduce vertical real estate:\n   - Remove/condense redundant padding blocks.\n   - Prefer a 2-column grid on desktop (left: lookup table, right: embedding) with a compact top row for the slider.\n   - On mobile, stack cleanly with smaller gaps.\n2) Make the explanatory callout less dominant:\n   - Either convert the “T is the context length” box into a smaller inline note, or tuck into a collapsible “What is T?” detail.\n3) Keep the “numbers stored” bar comparison but tighten it:\n   - Reduce bar height and spacing.\n   - Keep labels readable.\n4) Do not introduce horizontal overflow. Verify at ~390/768/1280 widths.\n\nAcceptance\n- The card is noticeably shorter (less scroll) while still immediately communicating the comparison.\n- No clipping/overflow.\n- npm -s run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-23T11:49:56.56375-05:00","updated_at":"2025-12-23T11:54:15.785418-05:00","closed_at":"2025-12-23T11:54:15.785418-05:00","close_reason":"Made ContextExplosionViz materially shorter by tightening spacing/typography and moving the 'T is the context length' explanation into a collapsible 'What is T?' section; build passes."}
{"id":"babygpt-tps","title":"8.7 KNOWN: Gap audit dot product-\u003eprobability","description":"KNOWN ISSUE: Connection between dot product and probability (via softmax) may not be foreshadowed in Ch1. AUDIT: check if Ch1 probability discussion sets up Ch2 dot product. IF gap: create ticket to add foreshadowing callout in Ch1.6.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:16:47.6341-05:00","updated_at":"2025-12-20T20:14:40.922591-05:00","closed_at":"2025-12-20T20:14:40.922591-05:00","close_reason":"Closed"}
{"id":"babygpt-trnf","title":"CorridorDemo: Improve Training Corpus panel layout + input styling (Screenshot 2025-12-20 23.52.06)","description":"Context\n- CorridorDemo is the interactive toy that shows counts_from_corpus.js and how the corridor narrows as you type a prefix.\n- Screenshot 2025-12-20 23.52.06: the Training Corpus panel looks heavy/boxy and takes a lot of space for a small list of sentences.\n- We want the demo to feel as premium as the other vizzes: tighter spacing, clearer hierarchy, less wasted area.\n\nWhere\n- Component: src/components/CorridorDemo.tsx\n- Styles: src/components/CorridorDemo.module.css\n\nTask\n1) Redesign the \"Training corpus (toy)\" panel so it is more compact and visually aligned with the rest of the demo.\n   - Keep the content (line numbers + sentences).\n   - Reduce unnecessary padding/margins and improve typography hierarchy.\n2) Ensure the highlight/dim behavior still reads clearly when the user types.\n3) Review the text input styling (class: input): match the visual language used in the rest of the demo (clean, readable, not overly dark/bright).\n\nAcceptance\n- Training Corpus panel uses less vertical space without losing readability.\n- Visual hierarchy: label, corpus lines, input, stats are easy to scan.\n- No regressions to interaction (typing, highlighting, suggestions).\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T19:33:09.711939-05:00","updated_at":"2025-12-22T17:48:27.18647-05:00","closed_at":"2025-12-22T17:48:27.18647-05:00","close_reason":"CorridorDemo training corpus panel is now more compact (2-column list on desktop) and input styling matches the rest of the demo UI."}
{"id":"babygpt-twpi","title":"Ch1 transition: add recap of foundations + what we build by end of chapter (before Tokenization)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.40.02.png. The “Okay, enough theory…” transition should recap what we've established and set expectations for what the reader will build by end of Chapter 1.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: \"Okay, enough theory. Let's build something.\"\n\nTask\n1) Before that sentence, add a short recap list of facts (3–6 bullets) that are now “known”:\n   - probability distributions sum to 1\n   - conditional probability is “filter worlds”\n   - chain rule decomposes sequence probability\n   - logs turn products into sums of surprise\n   - counting models fail due to sparsity / no sharing\n2) Add a clear “by the end of this chapter you will build…” preview. Be concrete (tokenization + sliding window training pairs).\n\nAcceptance\n- Reader feels oriented entering Tokenization.\n- Recap is factual (not hype) and preview is concrete.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:09.20324-05:00","updated_at":"2025-12-21T14:26:01.357195-05:00","closed_at":"2025-12-21T14:26:01.357195-05:00","close_reason":"Added recap (facts earned so far) + concrete 'what you'll build' preview right before Tokenization; build passes.","dependencies":[{"issue_id":"babygpt-twpi","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.893102-05:00","created_by":"daemon"}]}
{"id":"babygpt-u2a","title":"7.1 Difficulty audit: KenLM section 1.1.7.1","description":"AUDIT: Compare abstraction level (hashing, linear probing, pointer math) vs surrounding sections. IF spike: create ticket to move to appendix OR add 'optional deep dive' collapsible wrapper.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:28.330005-05:00","updated_at":"2025-12-20T19:44:55.839345-05:00","closed_at":"2025-12-20T19:44:55.839345-05:00","close_reason":"Closed"}
{"id":"babygpt-u4jn","title":"GeometricDotProductViz: side panel layout polish (avoid elements fighting for space)","description":"Problem (UI review screenshot 2025-12-23 12.58.47): In 'Dot Product: The Geometric View' figure, the right side panel feels cramped: the dot-product card, stats, formula, toggle, and conclusion fight for vertical space and alignment.\n\nWhere:\n- Component: src/components/GeometricDotProductViz.tsx + src/components/GeometricDotProductViz.module.css (confirm exact file via search for title 'Dot Product: The Geometric View').\n- Chapter 2 dot product section.\n\nFix goals:\n- Keep the visual (left) dominant; make the right panel read like a single coherent column.\n- Improve hierarchy: (1) dot value, (2) key stats table, (3) optional formula/projection toggle, (4) interpretation.\n- Align labels/values (2-column grid), reduce redundant containers, and prevent wrapping/clipping.\n- Add responsive behavior: on narrower widths, stack side panel below the plot or collapse optional blocks.\n\nAcceptance:\n- Looks in-family with VizCard/panel-dark/inset-box surfaces.\n- No text overlap or cramped spacing; panel feels intentional.\n- npm -s run build passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T14:43:41.785508-05:00","updated_at":"2025-12-23T14:50:26.218642-05:00","closed_at":"2025-12-23T14:50:26.218642-05:00","close_reason":"Consolidated dot-product side panel into a single details card, aligned label/value grid, reduced boxy competition, and tuned column sizing."}
{"id":"babygpt-u60q","title":"Ch1: Explain Causal Mask mechanics (not just the slogan)","description":"Context\n- The current Causal Mask explanation is a single sentence: “position i only sees positions 0 through i−1”.\n- This assumes the reader already understands what a “mask” is and how it is enforced.\n- We need a first‑principles explanation that matches the existing CausalMaskViz grid.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Section: 1.3 (training pairs / attention rule)\n- Find Callout title: “Enforcing the Rules”\n- Find text: “How do we stop the model from cheating? … The Causal Mask enforces this: position i only sees positions 0 through i−1.” (around line ~1564)\n- Viz immediately below: \u003cCausalMaskViz /\u003e\n\nWhat to change\n1) Expand the callout into a concrete explanation:\n   - Define a causal mask as a T×T grid/matrix of allowed vs blocked attention.\n   - For token at position i, allowed columns are 0..i (or 0..i−1 depending on whether “self” is allowed) — be explicit.\n   - Clarify the off‑by‑one: give a worked mini example for i=3.\n\n2) Explain enforcement in simple terms:\n   - In attention, we compute scores for every (query i, key j).\n   - The mask sets scores for “future” j\u003ei to −∞ before softmax, so their probability becomes 0.\n   - Keep it conceptual; 1 short pseudo‑code line is fine.\n\n3) Verify that the prose matches the CausalMaskViz axes and hover behavior.\n   - If the viz currently treats diagonal as visible vs not, align the wording.\n\nConstraints\n- Avoid heavy Greek / transformer jargon.\n- Keep the tone humble and precise.\n\nDefinition of done\n- A reader can explain what cells are blocked and why.\n- No axis mismatch between text and viz.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:36.285249-05:00","updated_at":"2025-12-21T19:08:36.803477-05:00","closed_at":"2025-12-21T19:08:36.803477-05:00","close_reason":"Expanded causal mask explanation (rows/cols, allowed triangle, -∞ masking) and fixed off-by-one to match CausalMaskViz; build passes"}
{"id":"babygpt-ud2","title":"PHASE 10: Missing Transitions","description":"Find abrupt section transitions (6 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:11:00.667977-05:00","updated_at":"2025-12-20T20:30:45.499015-05:00","closed_at":"2025-12-20T20:30:45.499015-05:00","close_reason":"Missing transitions audit complete. All 6 issues passed. Transition prose exists between major sections."}
{"id":"babygpt-ujl","title":"8.1 KNOWN: Gap audit cross-entropy","description":"KNOWN ISSUE: Cross-entropy formula in 1.1.7.2 Sparsity Trap but may not be formally defined until Ch2. AUDIT: trace first usage vs first definition. IF gap: create ticket to add definition before first use OR add forward-ref callout.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:16:45.94936-05:00","updated_at":"2025-12-20T19:48:40.747249-05:00","closed_at":"2025-12-20T19:48:40.747249-05:00","close_reason":"Closed"}
{"id":"babygpt-upu","title":"1.2 Gap: Why character-level helps connectivity","description":"Line ~540: 'Word graph is disconnected' stated but no explanation of WHY character-level helps. Space char as 'bridge' mentioned but mechanism unclear. FIX: Add explanation: at word level, 'cat' and 'dog' share zero characters. At char level, both contain common suffixes/patterns. Space char appears after EVERY word, creating a hub node. Character-level has inherent overlap that word-level lacks.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:24.292738-05:00","updated_at":"2025-12-20T21:08:05.551959-05:00","closed_at":"2025-12-20T21:08:05.551959-05:00","close_reason":"FIXED: Added concrete explanation of why character-level helps with sparsity. Explained that word-level 'cat' and 'dog' share zero structure, while character-level reveals overlap (consonant endings, CVC patterns). Crucially explained that space character creates a universal hub node with stable statistics from high frequency, enabling recombination of any word-ending with any word-beginning."}
{"id":"babygpt-v1z","title":"Ch1 design: standardize inline code/token color (prefer cyan) across chapters","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.28.41.png asks to standardize code colors across the document, including inline. User prefers the existing blue/cyan token color.\n\nCurrent state\n- Inline \u003cTerm\u003e uses var(--accent-cyan) (src/components/Typography.module.css).\n- Inline \u003ccode\u003e uses a different color/background (src/styles/global.css). This creates inconsistent “code” styling.\n\nTask\n1) Define a single visual style for inline code-like tokens (identifiers, short snippets, literal strings). Use var(--accent-cyan) as the main text color unless there is a strong reason not to.\n2) Update styling in src/styles/global.css and/or src/components/Typography.module.css so \u003ccode\u003e and \u003cTerm\u003e look consistent (color and font).\n3) Audit Chapter 1 + Chapter 2 for raw \u003ccode\u003e usage inside prose; where appropriate replace raw \u003ccode\u003e with \u003cTerm\u003e (Term is the documented component for code-like tokens).\n4) Ensure CodeBlock (syntax highlighted blocks) remains unchanged.\n\nAcceptance\n- Inline code tokens are consistently the same “blue” color across chapters.\n- No regressions to code blocks (\u003cpre\u003e\u003ccode\u003e should still render normally).\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:07.314007-05:00","updated_at":"2025-12-21T15:02:59.179104-05:00","closed_at":"2025-12-21T15:02:59.179104-05:00","close_reason":"Standardized inline \u003ccode\u003e to use accent-cyan and font-mono (matching \u003cTerm\u003e); kept pre code override so code blocks stay normal; build passes.","dependencies":[{"issue_id":"babygpt-v1z","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.164688-05:00","created_by":"daemon"}]}
{"id":"babygpt-v2em","title":"Ch2 RowSelectionViz: add prediction rail (commit-before-reveal)","description":"Context\n- Chapter 2 needs Encore/Samwho-style interactivity: the reader commits a prediction, manipulates one control, sees state change, and gets a metric.\n- Current Row Selection / one-hot → W row-selection viz explains the idea, but doesn't force the reader to predict first.\n\nGoal\n- Add a small “commit-before-reveal” interaction to the existing row-selection viz so the reader must guess what will happen before the UI shows the result.\n- Keep it in-family visually (VizCard / .panel-dark / .inset-box, semantic accents).\n\nWhere\n- Find the existing row-selection / one-hot demo component (search for labels like “Row Selection”, “one-hot”, or the matrix multiplication UI). Likely in `src/components/*Row*` or `src/components/*OneHot*`.\n- It is referenced in `src/chapters/Chapter2.tsx` around the embeddings lookup section (2.5.x).\n\nDesign spec (must implement all)\n1) Prompt (commit)\n   - Add a 1-sentence prompt above the control: “Before you click: which row of W do you think you’ll get when the 1 is on \u003ctoken\u003e?”\n   - Provide a tiny “Guess” UI that forces commitment before reveal.\n     - Option A (preferred): 3 pill choices (e.g., “row = h”, “row = e”, “row = l”) + a “Lock guess” button.\n     - Option B: dropdown guess + “Lock guess”.\n   - Until a guess is locked, the “Reveal” (or “Show result”) action is disabled and the result vector is hidden/blurred.\n\n2) Action (manipulate)\n   - Keep ONE primary control: selecting the token (the pill row). Do not add extra sliders.\n\n3) State (see it)\n   - When “Reveal” happens: animate/highlight in sequence\n     a) highlight the 1 in the one-hot vector\n     b) highlight the selected row in W\n     c) highlight the resulting output vector W[i]\n   - If the guess is wrong: show a gentle, non-snarky note: “Close — the 1 was on \u003ctoken\u003e, so we select row \u003ctoken\u003e.”\n\n4) Metric (score it)\n   - Display: “Selected index i = …” (mono) and “x^T W = W[i]”\n\nAcceptance criteria\n- A reader can answer: “Why does x^T W pick a row?” without reading surrounding prose.\n- Works on mobile widths (no clipping / overflow).\n- No new visual drift: uses existing tokens and shared components; no raw random hex.\n- `npm -s run build` passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-24T22:44:34.807331-05:00","updated_at":"2025-12-24T23:23:52.845814-05:00","closed_at":"2025-12-24T23:23:52.845814-05:00","close_reason":"Added commit-before-reveal flow (guess→lock→reveal) to MatrixRowSelectViz with highlight phases, gentle feedback, and hidden result until reveal; build passes"}
{"id":"babygpt-v523","title":"Ch2: Add a real first-principles intro to 'gradient' (currently too implicit)","description":"Context\n- Feedback: “Mate your introduction of the concept of gradient is… NON EXISTENT.”\n- We need a clear setup *before* derivations and vizzes.\n\nWhere\n- File: src/chapters/Chapter2.tsx\n- Find around the gradient section (look for “Softmax + cross-entropy has a famous gradient” and nearby).\n\nWhat to do\n1) Add a short gradient primer:\n- Gradient = slope (1D) / direction of steepest increase (multi-D).\n- For many parameters, it’s a vector of partial slopes.\n- To reduce loss, move opposite the gradient.\n- Define learning rate η as “how big a step”.\n\n2) Keep it intuitive:\n- One small picture analogy is ok (hill/bowl).\n- Avoid calculus symbols in the first paragraph.\n\n3) Then transition into the existing gradient derivation content.\n\nDefinition of done\n- A reader can answer: “what is a gradient and why do we subtract it?”\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T18:47:40.963037-05:00","updated_at":"2025-12-21T19:53:38.347231-05:00","closed_at":"2025-12-21T19:53:38.347231-05:00","close_reason":"Added gradient primer in Ch2 2.10 (slope → many-knob gradient → step opposite; introduced learning rate η) and removed early symbolic 'v += 0.01 * gradient' to avoid unexplained jargon; build passes."}
{"id":"babygpt-vcq","title":"1.5 Validate Ch1 map waypoint 1.6 The Limit","description":"AUDIT: Navigate to 1.6, verify description about scaling limits. IF mismatch: update. Check if section clearly establishes why memorization doesn't scale.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:24.024794-05:00","updated_at":"2025-12-20T19:31:19.565161-05:00","closed_at":"2025-12-20T19:31:19.565161-05:00","close_reason":"Closed"}
{"id":"babygpt-veo","title":"1.10 Validate Ch2 map waypoint 2.3 Ground Truth","description":"AUDIT: Navigate to 2.3, verify similarity definition description matches 'What Can We Measure?' section. IF mismatch: update. Ensure description captures P(next|c) fingerprint concept.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:37.819215-05:00","updated_at":"2025-12-20T19:32:34.288948-05:00","closed_at":"2025-12-20T19:32:34.288948-05:00","close_reason":"Closed"}
{"id":"babygpt-vp6","title":"1.1 Validate Ch1 map waypoint 1.1 The Physics","description":"AUDIT: Navigate to section 1.1 via waypoint, verify anchor works. Check description 'What we are predicting, why probability is the whole game' matches section content. IF broken link: fix section ID. IF description mismatch: update ChapterMap description.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:22.858935-05:00","updated_at":"2025-12-20T19:31:18.438925-05:00","closed_at":"2025-12-20T19:31:18.438925-05:00","close_reason":"Closed"}
{"id":"babygpt-vyy9","title":"Tone: replace 'toy' phrasing with 'tiny/demo/simplified example' (avoid trivializing)","description":"User feedback: calling examples a 'toy world/corpus' undercuts the project’s tone. Replace 'toy' language in narrative + UI with neutral terms like 'tiny', 'demo', 'simplified', 'practice' while keeping the intent (we’re shrinking the universe to make mechanics visible).\\n\\nScope (at least):\\n- src/chapters/Chapter2.tsx line with 'Back in our toy world (three options)' (Softmax section)\\n- src/chapters/Chapter2.tsx exercise text mentioning 'tiny toy' corpus preset\\n- src/chapters/Chapter1.tsx: 'toy hash rule' + 'Tiny toy example' phrasing\\n- src/components/NgramGraphViz.tsx subtitle 'toy corpus'\\n- src/components/CorridorDemo.tsx label 'Training corpus (toy)'\\n\\nAcceptance: wording feels serious but friendly; no 'toy' in user-facing prose/UI; build passes (npm run build).","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T21:09:42.316165-05:00","updated_at":"2025-12-21T21:17:22.487861-05:00","closed_at":"2025-12-21T21:17:22.487861-05:00","close_reason":"Replaced 'toy' language in Chapter 1/2 + Corridor/Ngram UI with 'tiny/demo/simple' phrasing to avoid trivializing examples; verified build."}
{"id":"babygpt-w7e","title":"2.7 Validate Ch2 invariant: Token IDs are arbitrary labels","description":"AUDIT: Read section 2.4, verify 'token IDs are arbitrary labels with no meaningful distance' is explicit. IF missing: create ticket to add Integer Distance Lie callout. IF implicit: make explicit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.057044-05:00","updated_at":"2025-12-20T19:42:02.475425-05:00","closed_at":"2025-12-20T19:42:02.475425-05:00","close_reason":"Closed"}
{"id":"babygpt-wblt","title":"UI bug: eliminate right-edge bleed / horizontal overflow (no content past card)","description":"Context (what the reader sees): On at least one Chapter 2 screen, a UI element visibly extends past the right edge of its intended container/card (you can see content \"bleeding\" beyond the rounded corner). This also tends to create horizontal scrolling on narrow viewports. The user has called this out repeatedly and wants it eliminated completely.\n\nGoal: Identify the exact offending component(s) and remove the horizontal overflow at the root cause (not by globally hiding overflow everywhere).\n\nPlan / Requirements:\n1) Reproduce the bleed at common widths (390px, 768px, 1280px). Confirm whether the page has horizontal scroll (`document.documentElement.scrollWidth \u003e clientWidth`).\n2) Use devtools to find the offender:\n   - Temporarily set `* { outline: 1px solid rgba(255,0,0,0.2); }` in devtools or use the layout panel to spot the element wider than its container.\n   - In console: walk up from the widest node: `([...document.querySelectorAll('*')].filter(el =\u003e el.scrollWidth \u003e el.clientWidth)).slice(0,10)` to locate culprits.\n3) Fix patterns at the component level (choose the correct fix):\n   - For flex/grid children: add `min-width: 0` and ensure children don't use `min-content` sizing unintentionally.\n   - For SVG/canvas: ensure width is responsive (`max-width: 100%`, correct viewBox, no hard-coded pixel width on wrapper).\n   - For long code/mono strings: ensure wrapping or controlled scrolling (`overflow-x: auto` in the right subcontainer, `overflow-wrap: anywhere` where appropriate).\n   - For positioned chips/badges: ensure they're not absolute outside bounds; use padding or containment.\n4) Add targeted regression guardrails only where needed (e.g., `max-width: 100%` on the specific wrapper).\n\nAcceptance:\n- No horizontal scroll on Chapter 2 at 390px width; nothing renders beyond card edges.\n- Build passes.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T16:13:09.515049-05:00","updated_at":"2025-12-23T19:33:26.709156-05:00","closed_at":"2025-12-23T19:33:26.709156-05:00","close_reason":"Prevent horizontal page scroll (html/body overflow-x hidden) + ensure KaTeX display blocks scroll within themselves"}
{"id":"babygpt-wcf","title":"4.2 Audit 2.2 Reuse Question dependencies","description":"AUDIT: Read section 2.2, verify it requires Ch1 n-gram concept. Check 'the cat' vs 'a cat' example refs counting model limits. IF no Ch1 connection: add explicit backward ref.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:20.811776-05:00","updated_at":"2025-12-20T19:50:14.742609-05:00","closed_at":"2025-12-20T19:50:14.742609-05:00","close_reason":"Closed"}
{"id":"babygpt-wk3","title":"1.1.1 Gap: Why independence matters for language modeling","description":"Line ~205: 'Surprise should be additive' is stated but WHY independence matters for text prediction isn't explained. The jump from 'two coins' to language is unmotivated. FIX: Add 1-2 sentences explaining that text characters ARE approximately independent given context, so additive surprise lets us score sequences by summing character-level surprises.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:26.503652-05:00","updated_at":"2025-12-20T20:52:03.052046-05:00","closed_at":"2025-12-20T20:52:03.052046-05:00","close_reason":"FIXED: Added explanatory bridge between 'additive surprise' and language modeling - explained that text characters are approximately independent given context, enabling sequence scoring via summed per-character surprises"}
{"id":"babygpt-wss","title":"7.3 Difficulty audit: Gradient derivation 2.10","description":"AUDIT: Check if calculus-based gradient derivation has adequate collapsible/optional wrappers. IF naked math: create ticket to wrap in expandable section with 'skip if not interested in math' note.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:28.913342-05:00","updated_at":"2025-12-20T20:06:54.898417-05:00","closed_at":"2025-12-20T20:06:54.898417-05:00","close_reason":"Closed"}
{"id":"babygpt-wsy6","title":"Ch1 content: fix combinatorics section to address 'words exist' misconception (dictionary still huge)","description":"Context\n- User request (quoted): “words exist! your answer is BOUND to be somewhere within the dictionary set yes?” The combinatorics section currently uses 27-character sequences and can accidentally imply that word-level modeling is constrained enough to avoid explosion.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: “Slide that to N=10” and the sentence about “205 trillion possible 10-character sequences”.\n\nTask\n1) Rewrite this explanation from first principles to address the misconception directly:\n   - Yes, tokens come from a finite vocabulary (dictionary / token set).\n   - But the number of possible sequences is still |V|^T (explodes with length).\n2) Add a concrete word-level example with realistic vocabulary size (e.g., 50,000 words): show that 10-word sequences is 50,000^{10}, an astronomically large number.\n3) Clarify that “valid English sequences” is a smaller subset than all sequences, but still far too large for counting/lookup coverage; the core problem is lack of sharing/generalization.\n4) Keep the tone organic and bottom-up; avoid sounding like you are arguing with the reader.\n\nAcceptance\n- Reader no longer leaves with “dictionary makes it tractable” intuition.\n- Character-level and word-level explosion are both explained clearly and concretely.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:10.544666-05:00","updated_at":"2025-12-21T14:22:40.092424-05:00","closed_at":"2025-12-21T14:22:40.092424-05:00","close_reason":"Reframed combinatorics to explicitly address finite-vocabulary misconception; added |V|^T word-level example (50k^10 ~ 1e47); build passes.","dependencies":[{"issue_id":"babygpt-wsy6","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:01.08506-05:00","created_by":"daemon"}]}
{"id":"babygpt-x1s","title":"CSS: Replace 28 hardcoded font-family with var(--font-*) (8 files)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:09.39879-05:00","updated_at":"2025-12-20T23:27:46.032006-05:00","closed_at":"2025-12-20T23:27:46.032006-05:00","close_reason":"Closed"}
{"id":"babygpt-x8o","title":"1.1.3 Gap: Derive 27^T explicitly","description":"Line ~407-408: The warehouse metaphor is vivid but 27^T isn't derived. WHERE does the exponent come from? FIX: Add explicit derivation: T positions × V choices per position = V^T total contexts. For V=27, T=5: 27^5 = 14,348,907. Show the combinatorial explosion as a concrete calculation, not just a claim.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:00.617686-05:00","updated_at":"2025-12-20T21:02:30.91003-05:00","closed_at":"2025-12-20T21:02:30.91003-05:00","close_reason":"FIXED: Deleted redundant paragraph. Extended MathBlock explanation with 'Multiplication, not addition — because each choice at position 1 can pair with any of 27 at position 2.'"}
{"id":"babygpt-xe9l","title":"Ch1/Ch2: Audit remaining non-LaTeX fractions/exponents in prose (Screenshot 2025-12-21 18.04.01)","description":"Context\n- Screenshot 2025-12-21 18.04.01: Some math in prose is still rendered as plain text (e.g., 1/27, 2^H) instead of proper fractions/superscripts.\n\nWhere\n- Chapters:\n  - src/chapters/Chapter1.tsx\n  - src/chapters/Chapter2.tsx\n\nWhat to do\n1) Find all remaining “math in prose” that uses slash fractions or caret exponents.\n- Examples to search for: `1/`, `^`, `^{` inside \u003cTerm\u003e or plain text.\n\n2) Convert to proper KaTeX\n- Use MathBlock for block equations.\n- Use MathInline for inline math (create it if missing; see related task).\n\n3) Check spacing\n- Ensure the line height doesn’t jump awkwardly.\n\nDefinition of done\n- No ugly caret/slash-fraction math remains in prose.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T18:47:41.521449-05:00","updated_at":"2025-12-22T15:25:05.272044-05:00","closed_at":"2025-12-22T15:25:05.272044-05:00","close_reason":"Converted remaining prose fractions/exponents to KaTeX (MathInline) so fractions/superscripts render consistently."}
{"id":"babygpt-xyl1","title":"Ch2 §2.1: Add missing bridge from 'redness' to coordinates (how color becomes a vector)","description":"Problem\n- User feedback: “HOW do you go from redness to some shit in vector space? You never get into this.”\n- We mention that color experience maps to a 3D vector space, but we need a more explicit, first-principles bridge showing how measurement produces coordinates.\n\nWhere\n- File: `src/chapters/Chapter2.tsx`\n- Section: `2.1 Grassmann's Insight`\n- Around the Callout “How colors turn into numbers” + the surrounding paragraphs.\n\nWhat to add\n1. A plain-language, step-by-step explanation of the measurement procedure:\n   - You have a target color patch.\n   - You have 3 adjustable primary lights (R/G/B) with knobs.\n   - You tune knobs until the mixed patch matches the target.\n   - The 3 knob values are the coordinates.\n2. A tiny worked example (required):\n   - Example: “Target patch matches with (R=0.2, G=0.7, B=0.1). That IS the vector.”\n3. Explain what this buys us:\n   - Once you have coordinates, mixing = adding/scaling vectors.\n4. Then explicitly connect it to language:\n   - We can’t measure “redness” for ‘q’, but we can measure “what follows q” from corpus counts.\n\nAcceptance criteria\n- A reader with zero math background can explain what a coordinate is and why color can be a vector.\n- The language-to-color analogy feels earned, not asserted.\n- `npm run build` passes.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T21:52:25.434431-05:00","updated_at":"2025-12-22T16:03:16.063923-05:00","closed_at":"2025-12-22T16:03:16.063923-05:00","close_reason":"Strengthened the redness→coordinates bridge with a clearer measurement story (basis lights/knob settings) and explicit character-mixture equation; clarified representation vs object."}
{"id":"babygpt-y2k","title":"Ch1 prose: gentler intro for joint probability; integrate 'AND multiplies' into core narrative","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.29.18.png: math notation appears too abruptly; readers who are smart but not fluent in math syntax get scared off.\n- User specifically asked to move the “Why AND multiplies” explanation out of a callout/textbox and into the main document flow.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Section: 1.1.2 “The Problem With Sequences” (MathBlock P(x_1, x_2, ..., x_t))\n- The current “Why “AND” multiplies” Callout is later near the corridor demo (search for title: \"Why “AND” multiplies\").\n\nTask\n1) Rewrite the lead-in around joint probability so it starts with plain language first, then introduces notation.\n2) Move the “AND multiplies” worlds/filtering example into the core narrative as normal \u003cParagraph\u003e content (not inside \u003cCallout\u003e).\n3) Ensure the reader learns: “AND in sequence” means “filter the world further” → multiplication of fractions → chain rule.\n4) Keep the math blocks, but make them feel like a summary of what the reader already understands.\n\nAcceptance\n- The joint-probability section reads smoothly for non-math-native readers.\n- The “AND multiplies” intuition is in the main narrative (not gated behind a callout).\n- No duplicated explanations; flow stays tight.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:07.583539-05:00","updated_at":"2025-12-21T11:11:46.403878-05:00","closed_at":"2025-12-21T11:11:46.403878-05:00","close_reason":"Rewrote 1.1.2 intro and integrated 'Why AND multiplies' into main text.","dependencies":[{"issue_id":"babygpt-y2k","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.387596-05:00","created_by":"daemon"}]}
{"id":"babygpt-y397","title":"KenLMDemo: fix squished 3-column layout (Query / Grid / Inspector)","description":"Problem (from UI review): In the KenLM / Linear Probing demo, the middle packed-array grid column becomes overly narrow/squished, making the layout feel unbalanced and hard to read. The slot inspector column can dominate, compressing the grid.\\n\\nWhere: Chapter 1 KenLM section (Linear Probing demo). Component: src/components/KenLMDemo.tsx + src/components/KenLMDemo.module.css.\\n\\nRepro:\\n1) Run dev server.\\n2) Open / (Chapter 1).\\n3) Scroll to the KenLM / Linear Probing viz.\\n4) On desktop widths ~1100–1500px, observe the 3-column layout: left Query panel, center packed array grid, right Slot Inspector. The center grid is too narrow compared to left/right.\\n\\nFix goals:\\n- Rebalance the CSS grid/flex ratios so the packed array grid stays readable (min width + appropriate fr ratios).\\n- Keep inspector usable, but don’t let it steal all horizontal space. Consider collapsing the inspector below the grid on narrower widths, or using a responsive breakpoint to switch from 3 columns → 2 rows.\\n- Validate no text clips inside the demo window and controls remain aligned with the gold “engineer’s window” motif (see docs/assets/visual-audit/gold/audit-ch1-kenlm.png).\\n\\nAcceptance:\\n- Center grid feels primary and readable on desktop; no squished middle column.\\n- Mobile/tablet responsive layout remains clean (no horizontal scroll).\\n- Build passes: npm -s run build.","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T13:42:45.777543-05:00","updated_at":"2025-12-23T14:21:09.900537-05:00","closed_at":"2025-12-23T14:21:09.900537-05:00","close_reason":"Rebalanced KenLMDemo grid column widths and added min-width:0 to prevent center grid squish."}
{"id":"babygpt-y3f","title":"7.2 Difficulty audit: Ch1-\u003eCh2 transition","description":"AUDIT: Compare difficulty at end of Ch1 vs start of Ch2 (Grassmann history). IF gap: create ticket to add bridging paragraph or adjust Ch1 ending tone.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:28.61759-05:00","updated_at":"2025-12-20T20:05:47.258521-05:00","closed_at":"2025-12-20T20:05:47.258521-05:00","close_reason":"Closed"}
{"id":"babygpt-ygf","title":"8.4 Gap audit: softmax","description":"AUDIT: Verify softmax not referenced before Section 2.7 definition. IF early reference found: create ticket to relocate or add forward-ref.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:46.79527-05:00","updated_at":"2025-12-20T20:12:43.20592-05:00","closed_at":"2025-12-20T20:12:43.20592-05:00","close_reason":"Closed"}
{"id":"babygpt-yhmu","title":"KenLMDemo: fix squished memory grid / inspector ratio on desktop","description":"Context: After babygpt-q5cb redesign, desktop layout can allocate too much width to Slot Inspector, causing the 4×4 memory grid to become very narrow/tall (user screenshot 2025-12-23 01:25:57 shows middle block squished).\n\nGoal: Adjust CSS grid ratios/breakpoints so the memory grid remains readable on typical desktop widths.\n\nAcceptance criteria:\n- On ~1100–1400px wide viewports, the memory grid should have enough width for 4 columns without cells becoming skinny.\n- Inspector may stack below the grid when space is limited.\n- Keep the overall window motif and semantic color legend unchanged.\n\nFiles: src/components/KenLMDemo.module.css (and KenLMDemo.tsx only if layout structure must change).","status":"closed","priority":1,"issue_type":"bug","created_at":"2025-12-23T01:28:09.636065-05:00","updated_at":"2025-12-23T01:30:53.353941-05:00","closed_at":"2025-12-23T01:30:53.353941-05:00","close_reason":"Adjusted rightMain grid min widths + breakpoint so memory grid stays readable; build passes; will sync/push."}
{"id":"babygpt-z4x","title":"3.2 Audit 1.1.1 What Is Probability dependencies","description":"AUDIT: Read section 1.1.1, verify surprise=-log2(p) is defined BEFORE any cross-entropy reference. IF cross-entropy appears first: create ticket to reorder or add surprise definition earlier.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:31.587934-05:00","updated_at":"2025-12-20T19:42:00.562455-05:00","closed_at":"2025-12-20T19:42:00.562455-05:00","close_reason":"Closed"}
{"id":"babygpt-zd1","title":"3.11 Audit 1.2 Tokenization dependencies","description":"AUDIT: Read section 1.2, verify proper transition from theory to implementation. Check if chars-vs-words choice is motivated by sparsity discussion. IF not connected: add SectionLink to 1.1.7.2.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:54.65702-05:00","updated_at":"2025-12-20T19:42:28.510479-05:00","closed_at":"2025-12-20T19:42:28.510479-05:00","close_reason":"Closed"}
{"id":"babygpt-zdb","title":"6.4 Terminology audit: loss vs surprise vs cross-entropy","description":"AUDIT: trace where each term first appears and if relationship is explained. IF gap: add callout establishing 'surprise = -log(p), loss = average surprise, cross-entropy = expected surprise'. Create remediation ticket.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.642076-05:00","updated_at":"2025-12-20T20:29:39.652984-05:00","closed_at":"2025-12-20T20:29:39.652984-05:00","close_reason":"PASS: Relationship established in Ch2: surprise=-log(p), loss=cross-entropy=average surprise, defined at line 1037 and 2.10 sections."}
{"id":"babygpt-zmt","title":"CSS: Replace 100 hardcoded hex colors with CSS variables (19 files)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:08.49681-05:00","updated_at":"2025-12-20T23:25:25.33739-05:00","closed_at":"2025-12-20T23:25:25.33739-05:00","close_reason":"Closed"}
{"id":"babygpt-zww","title":"2.1 Validate Ch1 invariant: P(next|context) via chain rule","description":"AUDIT: Read Invariants component in Ch1, verify claim 'P(next|context) via chain rule'. Trace sections 1.1.5 + earlier to confirm derivation exists. IF missing: create ticket to add derivation. IF present but unclear: create ticket to clarify.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:59.990794-05:00","updated_at":"2025-12-20T19:32:32.044197-05:00","closed_at":"2025-12-20T19:32:32.044197-05:00","close_reason":"Closed"}
