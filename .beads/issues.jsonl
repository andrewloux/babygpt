{"id":"babygpt-04s","title":"1.1.5 Gap: Why telescoping cancellation works conceptually","description":"Line ~463-465: Cancellation is SHOWN but not EXPLAINED. Is it luck? Feature of decomposition? FIX: Add explanation that telescoping works because we're decomposing a joint probability into a product of conditionals. Each P(x_i | x_\u003ci) appears once in numerator (for position i) and once in denominator (as part of x_\u003ci+1 context). The structure guarantees cancellation — it's not coincidence.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:08.603807-05:00","updated_at":"2025-12-20T21:05:37.442196-05:00","closed_at":"2025-12-20T21:05:37.442196-05:00","close_reason":"FIXED: Points at actual numbers in example - 'Look at the 5: denominator of 5/6 and numerator of 4/5. Both are C(the).' Shows WHY they cancel, not just that they do."}
{"id":"babygpt-0gn","title":"2.3 Gap: Why THIS definition of similarity","description":"Line ~364-367: 'Close if they predict similar next characters' presented as obvious. WHY this definition? Why not semantic similarity? FIX: Add justification: we're building a LANGUAGE MODEL, not a thesaurus. The model's job is to predict next tokens. Two tokens are 'the same' for our purposes if they make the same predictions. Semantic similarity is irrelevant — predictive role is everything.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:27.539968-05:00","updated_at":"2025-12-20T21:29:45.991283-05:00","closed_at":"2025-12-20T21:29:45.991283-05:00","close_reason":"FIXED: Added justification in Section 2.3 explaining why similarity is defined by predictive role (language model purpose) rather than semantic similarity. Brief, first-principles explanation pointing at the model's job."}
{"id":"babygpt-0jb","title":"2.4 Gap: HOW reusability solves capacity","description":"Line ~546-548: Claims 27^8 contexts can't be stored but embeddings work because 'reusable'. HOW? Math not shown. FIX: Add explicit math. Lookup table: 27^8 × 27 = 282 trillion params. Embedding approach: 27 × D embeddings + D × 27 output weights. For D=64: 27×64 + 64×27 = 3,456 params. That's 80 billion times smaller. The compression comes from SHARING embeddings across contexts.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:00.813514-05:00","updated_at":"2025-12-20T21:39:52.578793-05:00","closed_at":"2025-12-20T21:39:52.578793-05:00","close_reason":"Fixed: Added explicit math - 282 trillion vs 3,456 params"}
{"id":"babygpt-0ky","title":"2.13 Validate Ch2 invariant: Softmax converts logits to probs","description":"AUDIT: Read section 2.7, verify softmax formula and role (converts logits to probs that sum to 1). IF missing: add formula. IF role unclear: add explicit 'why softmax?' callout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:20.801913-05:00","updated_at":"2025-12-20T19:47:18.814628-05:00","closed_at":"2025-12-20T19:47:18.814628-05:00","close_reason":"Closed"}
{"id":"babygpt-0la","title":"1.4 Validate Ch1 map waypoint 1.3.2 Free Lunch","description":"AUDIT: Navigate to 1.3.2, verify 'One block of text becomes thousands of training targets' is accurate. IF mismatch: update description. Check if the 'free lunch' concept is actually explained.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:23.742655-05:00","updated_at":"2025-12-20T19:31:19.283738-05:00","closed_at":"2025-12-20T19:31:19.283738-05:00","close_reason":"Closed"}
{"id":"babygpt-0ok","title":"PHASE 2: Invariant Validation","description":"Validate all chapter invariants are properly established in content (14 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:58.324938-05:00","updated_at":"2025-12-20T20:30:20.161825-05:00","closed_at":"2025-12-20T20:30:20.161825-05:00","close_reason":"All invariant validation issues resolved. Invariants properly established and consistently referenced across both chapters."}
{"id":"babygpt-0q9","title":"8.5 Gap audit: perplexity","description":"AUDIT: Perplexity appears in 1.1.7.2 and 1.3.1. Verify defined before first use with formula. IF gap: create ticket to add perplexity definition section.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:47.075429-05:00","updated_at":"2025-12-20T19:56:22.071385-05:00","closed_at":"2025-12-20T19:56:22.071385-05:00","close_reason":"Closed","comments":[{"id":11,"issue_id":"babygpt-0q9","author":"andrewlouis","text":"NEEDS_FIX: Perplexity first used at line 757 (section 1.1.7.1) without definition. Formula appears later at line 903 in Exercise 1.1. Need to add perplexity definition (perplexity = 2^H where H is cross-entropy) before line 757, or add a forward reference callout.","created_at":"2025-12-21T00:48:19Z"}]}
{"id":"babygpt-0w0","title":"CSS: Consolidate border-radius to design tokens","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:16.703776-05:00","updated_at":"2025-12-20T23:32:31.882141-05:00","closed_at":"2025-12-20T23:32:31.882141-05:00","close_reason":"Closed"}
{"id":"babygpt-167","title":"NeuralTrainingDemo: Polish sweep","description":"Audit NeuralTrainingDemo for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (training animation, weight updates)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:07.042608-05:00","updated_at":"2025-12-20T22:54:58.05393-05:00","closed_at":"2025-12-20T22:54:58.05393-05:00","close_reason":"Wrong - this IS a gold standard (Dec 19), not needing sweep"}
{"id":"babygpt-1l3","title":"Gradient Tugs: Add Karpathy-style code walkthrough","description":"GradientTraceDemo: Two-panel (code left, output right). Tiny example: vocab=[q,u,a], dim=4, context=q, target=u. 8 steps: lookup → scores → softmax → loss → centroid → gradient → update → verify. Show actual numbers. Controls: Next/Prev/Reset. Key: watch E[q] move toward E[u] with real coordinates.","status":"closed","issue_type":"feature","created_at":"2025-12-20T22:13:13.851354-05:00","updated_at":"2025-12-20T22:28:05.909701-05:00","closed_at":"2025-12-20T22:28:05.909701-05:00","close_reason":"Implemented GradientTraceDemo: 8-step Karpathy-style code walkthrough showing actual numbers (q-u-a vocab, 4D embeddings). Each step shows code and computed output. Demonstrates lookup, dot product scores, softmax, loss, centroid, gradient, update, and verification that P(u) increased."}
{"id":"babygpt-1qr","title":"2.3 Gap: Why dot product, not Euclidean or KL","description":"Line ~380-382: Dot product presented as THE similarity metric. Why not Euclidean distance or KL divergence? FIX: Add Callout explaining the choice. Dot product: (1) differentiable, (2) distributes over addition (crucial for gradients), (3) computation is just multiply-accumulate (fast on GPUs). Euclidean works but gradients are messier. KL requires logs. Dot product is the engineering sweet spot.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:34.701876-05:00","updated_at":"2025-12-20T21:30:53.858216-05:00","closed_at":"2025-12-20T21:30:53.858216-05:00","close_reason":"FIXED: Added engineering justification for dot product in Section 2.6. Explains three constraints: (1) differentiability (clean gradients), (2) distributes over addition (crucial for gradient flow/attention), (3) GPU-efficient (multiply-accumulate primitive). Addresses why not Euclidean (messier gradients, doesn't compose) or KL divergence (requires logs)."}
{"id":"babygpt-20f","title":"5.4 Validate SectionLinks in Ch2 exercises","description":"AUDIT: Lines 1804, 1825, 1836, 1876 in Ch2 exercises. Verify each SectionLink to 2.2, 2.3, 2.6 points to correct section. IF any broken: fix. IF any inaccurate: update link text.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:40.221003-05:00","updated_at":"2025-12-20T20:26:48.405294-05:00","closed_at":"2025-12-20T20:26:48.405294-05:00","close_reason":"FIXED: Corrected 2 SectionLinks in Ch2 exercises (lines 1845, 1856) from 2.2 to 2.3 - the CharacterClusterViz (similarity panel, corpus editor) is in Section 2.3","comments":[{"id":13,"issue_id":"babygpt-20f","author":"andrewlouis","text":"NEEDS_FIX: Line 1825 has incorrect SectionLink - says 'cosine similarity (in the Section 2.2 panel)' but CharacterClusterViz (which shows cosine similarity) is in Section 2.3, not 2.2. Should be: 'cosine similarity (in the \u003cSectionLink to=\"2.3\"\u003eSection 2.3\u003c/SectionLink\u003e panel)'. Other SectionLinks at lines 1804, 1825 (second one to 2.6), and 1876 are correct.","created_at":"2025-12-21T01:03:52Z"}]}
{"id":"babygpt-2a3","title":"PHASE 11: Explanatory Gap Remediation","status":"open","priority":1,"issue_type":"epic","created_at":"2025-12-20T20:47:16.994855-05:00","updated_at":"2025-12-20T20:47:16.994855-05:00"}
{"id":"babygpt-2a3.1","title":"Ch2: Add visible recap + stronger section-to-section bridges (no hidden arc list)","description":"User feedback: Chapter 2 needs a clearer recap of what we've learned so far *and* how each section built on the previous one. Right now the recap list is mostly hidden behind the collapsible \u003cdetails\u003e in Section 2.9 (\"Optional: the Chapter 2 arc (one list)\").\n\nTarget file: src/chapters/Chapter2.tsx\nPrimary location: \u003cSection number=\"2.9\" title=\"Synthesis: From Counts to Coordinates\"\u003e\n\nWhat to do:\n- Replace the collapsed/optional recap with a short, always-visible recap (e.g. an \u003col\u003e or \u003cul\u003e) covering 2.1 → 2.8 in 6–8 bullets.\n- Each bullet must explicitly state:\n  1) what that section contributed, and\n  2) why it needed the prior step (dependency language like “We needed X before Y because…”).\n- Keep the voice humble + plain. Avoid mic-drop phrasing and avoid the rhetorical pattern “not X, but Y.”\n- Keep it tight: 1–2 sentences per bullet.\n- Keep the existing Invariants block, but ensure the reader gets orientation even if they don’t click anything.\n\nNon-goals:\n- Don’t rewrite the whole chapter. This is about orientation/bridging and removing the feeling of “teleporting” between concepts.\n","acceptance_criteria":"Section 2.9 contains a visible recap (not behind \u003cdetails\u003e) that links 2.1→2.8 with clear dependencies; reader can skim in ~30s and stay oriented.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:16:36.306078-05:00","updated_at":"2025-12-21T15:36:26.283754-05:00","closed_at":"2025-12-21T15:36:26.283754-05:00","close_reason":"Made Ch2 Section 2.9 recap visible (removed collapsible arc list) and rewrote bullets to explicitly state dependencies between 2.1–2.8; build passes.","labels":["chapter2","copy","structure"],"dependencies":[{"issue_id":"babygpt-2a3.1","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:16:36.307689-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.2","title":"Ch2: Add strong exercise motivation + 'what code you will write' preface","description":"User feedback: Chapter 2 exercises currently start abruptly. Add a proper motivational ramp that explains (1) why these exercises exist, (2) what skill they’re testing, and (3) what code the reader should be able to write after finishing them.\n\nTarget file: src/chapters/Chapter2.tsx\nLocation: \u003cSection number=\"2.11\" title=\"Exercises\"\u003e — insert content BEFORE Exercise 2.1.\n\nWhat to add:\n- 2–4 short Paragraphs that connect Chapter 2 content to the exercises.\n- A concrete “What you’ll be able to implement after this” list (bullets are fine). Suggested items:\n  - embedding lookup (row selection / E[ix])\n  - dot product as similarity score\n  - softmax to turn scores into probabilities\n  - cross-entropy / negative log-prob as loss\n  - one training step nudging embeddings based on error (high-level; no calculus)\n- A sentence explaining why this matters for the next chapter (context window widening / combining multiple token vectors).\n\nTone constraints:\n- Humble, precise, non-preachy.\n- Avoid minimizing language like “this is easy / just memorize / no magic.”\n\nNon-goals:\n- Don’t change the exercises themselves unless needed for consistency with the new preface.\n","acceptance_criteria":"Section 2.11 starts with a short motivation + explicit 'by the end you can implement X' list; exercises feel purposeful and tied to upcoming code.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:17:00.186047-05:00","updated_at":"2025-12-21T15:36:58.069514-05:00","closed_at":"2025-12-21T15:36:58.069514-05:00","close_reason":"Added Exercises preface in Ch2 (purpose + pipeline + concrete 'what you can implement' list + forward pointer); build passes.","labels":["chapter2","copy","exercises"],"dependencies":[{"issue_id":"babygpt-2a3.2","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:17:00.191387-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.3","title":"Ch1: Rewrite the post-CorridorDemo 'multiplication keeps track' sentence for precision","description":"User feedback: This sentence is currently vague / feels like hand-waving:\n  \"It can look like a formula at first. It's just the bookkeeping for 'of those, of those, of those.' Once you see the corridor narrowing, multiplication is what keeps track.\"\n\nTarget file: src/chapters/Chapter1.tsx\nLocation: around the CorridorDemo in Section 1.1 (search for \"Once you see\\n          the corridor narrowing\"). Current line numbers ~460–461.\n\nWhat to do:\n- Replace that paragraph with a clearer, first-principles statement that explicitly ties the corridor metaphor to the chain rule product.\n- Use concrete language: each factor is “given we survived the first k tokens, what fraction survives one more step?”; multiplying factors yields the final fraction of worlds/texts.\n- Avoid vague meta-language (“bookkeeping”) as the main message.\n- Keep it short (1–2 Paragraphs). The longer “filter worlds” explanation that follows can stay, but ensure there is no redundancy.\n\nOptional improvement:\n- If redundancy is high, merge/trim adjacent paragraphs so the reader gets one clean explanation, not three near-duplicates.\n\nTone constraints:\n- Humble and precise; no mic-drop phrasing.\n","acceptance_criteria":"The paragraph after \u003cCorridorDemo /\u003e states precisely what multiplication is tracking (conditional survival fractions / chain rule) without vague 'bookkeeping' language.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:17:23.705458-05:00","updated_at":"2025-12-21T15:36:10.434906-05:00","closed_at":"2025-12-21T15:36:10.434906-05:00","close_reason":"Replaced vague 'multiplication keeps track' line after CorridorDemo with explicit survival-fraction/chain-rule wording; smoothed into worlds-filtering example; build passes.","labels":["chapter1","copy","probability"],"dependencies":[{"issue_id":"babygpt-2a3.3","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:17:23.710391-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.4","title":"Ch1: Strengthen first-principles explanation of encode/decode (beyond 'agreed mapping')","description":"User feedback: The current encode/decode intro is too weak first-principles:\n  \"The word code just means “an agreed mapping.” In this chapter, our codebook is two tiny dictionaries: stoi and itos.\"\n\nTarget file: src/chapters/Chapter1.tsx\nLocation: \u003cSection number=\"1.2.2\" title=\"Encoding and Decoding\"\u003e (search \"The word code\"). Current line ~1357.\n\nWhat to do:\n- Rewrite the intro to explain *why* we encode at all:\n  - GPUs/arrays can’t index into matrices with letters/strings; they need integer row IDs.\n  - Encoding turns symbols into stable addresses (0..V-1) so we can store vectors/logits in arrays.\n  - Decoding is the inverse mapping back to readable text.\n- Keep “codebook” language only if it’s made concrete (a bijection between tokens and integer IDs). Avoid airy phrasing.\n- Include one tiny concrete example (2–3 lines of prose): e.g. vocab { 'h':3, 'e':2, 'l':4, 'o':5 } so \"hello\" → [3,2,4,4,5] and back.\n- Keep the existing note that IDs are labels (permutation doesn’t matter), but tighten to avoid repetition.\n\nTone constraints:\n- Humble, plain, non-minimizing.\n","acceptance_criteria":"Section 1.2.2 explains encode/decode as 'addressing rows in arrays' with a tiny concrete example; removes/rewrites the weak 'agreed mapping' line.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T15:17:46.683694-05:00","updated_at":"2025-12-21T15:35:52.027174-05:00","closed_at":"2025-12-21T15:35:52.027174-05:00","close_reason":"Rewrote 1.2.2 encode/decode as first-principles 'IDs are array addresses' explanation; kept reversible stoi/itos example; build passes.","labels":["chapter1","copy","tokenization"],"dependencies":[{"issue_id":"babygpt-2a3.4","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T15:17:46.687282-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.5","title":"ExplosionDemo ('Generalization Wall'): redesign to be more compact and information-dense","description":"User feedback (Screenshot 2025-12-21 at 15.50.01): The 'Generalization Wall' viz feels wasteful in vertical space for the amount of information conveyed. We want a tighter, more expressive layout that communicates the combinatorics explosion quickly.\n\nWhere this appears\n- Chapter 1, Section 1.1.3 “Why This Happens”\n- Used via: \u003cExplosionDemo /\u003e in src/chapters/Chapter1.tsx\n\nTarget files\n- src/components/ExplosionDemo.tsx\n- src/components/ExplosionDemo.module.css\n\nCurrent structure (summary)\n- Header (title/subtitle)\n- Slider for context length T (1–10)\n- Equation block showing 27^T expanded as repeated multiplication + 'Your data' fixed to 1,000,000\n- Log-scale bar (oversaturated → sparse) + crossover line\n- Coverage result block + optional 'insight' paragraph (only for T\u003e=6)\n\nWhat to change\n1) Reduce vertical footprint while keeping clarity:\n   - Convert the viz into a 2-column grid on desktop:\n     - Left: slider + key numbers (T, possibilities, data)\n     - Right: the log-scale 'crossover' bar + coverage/result state\n   - On mobile: stack cleanly (no horizontal overflow).\n\n2) Make the math less bulky:\n   - Remove or hide the full exponent chain “27 × 27 × …” by default.\n   - Keep a compact representation like “27^T ≈ 205.9×10^12” and optionally reveal the expanded chain behind a small “Show expansion” toggle or \u003cdetails\u003e.\n\n3) Tighten spacing + hierarchy:\n   - Reduce padding/margins between blocks.\n   - Ensure the most important output (coverage + state label) stays above the fold.\n\n4) Visual polish:\n   - Bring it in line with other premium vizzes (dark glass, cyan/magenta accents, crisp mono labels).\n   - Consider wrapping in VizCard for consistent header/padding (optional but preferred).\n\nNotes\n- Keep existing logic/behavior unless it blocks the redesign.\n- Preserve accessibility: slider label, readable text contrast.\n\nDefinition of done\n- Screenshot comparison shows the viz occupies significantly less height while remaining clearer.\n- npm run build passes.\n","acceptance_criteria":"- ExplosionDemo conveys the same message with noticeably less vertical space (target: ~30–40% shorter at desktop widths).\\n- Layout remains readable and premium (matches BabyGPT viz style).\\n- Mobile layout remains usable.\\n- npm run build passes.","notes":"Additional user screenshot (2025-12-21 16:28): current layout wraps awkwardly: the 'T = 10' readout splits across lines and the long possibilities expansion wraps. Redesign must keep the primary numeric line unbroken (no wrap) and move any long expansion behind a compact toggle or horizontal scroll.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T16:27:05.18175-05:00","updated_at":"2025-12-21T16:38:43.200476-05:00","closed_at":"2025-12-21T16:38:43.200476-05:00","close_reason":"Redesigned ExplosionDemo into compact 2-column VizCard layout; moved long 27×... expansion into a small details toggle w/ horizontal scroll; fixed T readout wrapping; build passes.","labels":["chapter1","design","viz"],"dependencies":[{"issue_id":"babygpt-2a3.5","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T16:27:05.194754-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.6","title":"Ch1 intro: remove premature entropy jargon; reposition Shannon 1.3 bits claim after surprise definition","description":"User feedback: Current Chapter 1 opening paragraphs introduce domain-specific vocabulary too early (entropy, bits per character, 'surprise') and feel like forward-referencing before the concepts are explained.\n\nTarget file\n- src/chapters/Chapter1.tsx\n\nCurrent location\n- Section 1.1 \"The Physics of the Problem\" paragraphs around the Shannon intro:\n  - \"Shannon estimated that English has an entropy around 1.3 bits per character...\"\n  - \"Entropy is a number: the fundamental limit...\"\n\nWhat to do\n- Rewrite the Section 1.1 Shannon intro so it stays intuitive and avoids jargon that hasn’t been defined yet.\n- Move the numeric Shannon estimate (~1.3 bits/character) to Section 1.1.1 AFTER the MathBlock defining surprise = -log2(p).\n- Keep citations block as-is (or keep equivalent references) but ensure the narrative doesn’t require knowing what entropy/bits are before we define them.\n\nTone constraints\n- Humble, clear, quietly playful. Avoid mic-drop or “trust me” language.\n","acceptance_criteria":"- Section 1.1 no longer uses 'entropy'/'bits per character' before surprise/log definition.\\n- Shannon benchmark (1.3 bits/char) appears only after surprise is defined (Section 1.1.1).\\n- Paragraphs still flow cleanly into Section 1.1.1.\\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T16:44:22.010137-05:00","updated_at":"2025-12-21T16:46:16.284741-05:00","closed_at":"2025-12-21T16:46:16.284741-05:00","close_reason":"Removed entropy/bits jargon from Section 1.1 opener; moved Shannon ~1.3 bits/char benchmark to Section 1.1.1 after surprise is defined; build passes.","labels":["chapter1","copy","structure"],"dependencies":[{"issue_id":"babygpt-2a3.6","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T16:44:22.014841-05:00","created_by":"daemon"}]}
{"id":"babygpt-2a3.7","title":"Ch1: remove/move 'Why can neural networks handle unseen contexts?' collapsible (currently out of place)","description":"User feedback (Screenshot 2025-12-21 16:40): The collapsible “Why can neural networks handle unseen contexts?” feels out of place here, because neural networks are treated as a first-class topic elsewhere. The insertion currently breaks the flow of the FormalRigor block.\n\nCurrent location\n- src/chapters/Chapter1.tsx inside \u003cFormalRigor title=\"Chain Rule: Formal Rigor\"\u003e, near the end of the block.\n- Search for: \u003csummary\u003eWhy can neural networks handle unseen contexts?\u003c/summary\u003e\n\nWhat to do\nOption A (preferred): remove from FormalRigor entirely and move the concept to a better location:\n- Either Section 1.7 “What’s Next” (bridge to Chapter 2), or early Chapter 2, where neural nets are the topic.\n- Keep it short (1–2 Paragraphs). Focus on the conceptual difference: counting divides by counts and hits undefined contexts; neural nets compute a smooth function that always outputs a distribution.\n- Add a forward pointer like “We’ll cash this in in Chapter 2.”\n\nOption B: delete entirely if it’s redundant with nearby content and the doc already explains the transition sufficiently.\n\nConstraints\n- Avoid heavy jargon. Don’t introduce new math.\n- Ensure the narrative flow in the FormalRigor block improves (no jarring pivot).\n\nDefinition of done\n- The FormalRigor block reads cleanly from end to citations.\n- If moved, new placement feels natural and helps the Chapter 1→2 transition.\n- npm run build passes.\n","acceptance_criteria":"- The collapsible 'Why can neural networks handle unseen contexts?' is no longer inside FormalRigor in Section 1.1.5.\\n- If we still want the idea, it appears in a more appropriate location (likely Section 1.7 or Chapter 2) with a short cross-link.\\n- Chapter 1 flow feels less jarring in the chain-rule formal block.\\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T16:49:30.981936-05:00","updated_at":"2025-12-21T16:51:59.865037-05:00","closed_at":"2025-12-21T16:51:59.865037-05:00","close_reason":"Removed the neural-network unseen-contexts collapsible from the Chain Rule FormalRigor block (it was jarring/redundant); flow now goes straight into citations; build passes.","labels":["chapter1","copy","structure"],"dependencies":[{"issue_id":"babygpt-2a3.7","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T16:49:30.98633-05:00","created_by":"daemon"}]}
{"id":"babygpt-2ez","title":"1.6 Validate Ch1 map waypoint 1.7 What's Next","description":"AUDIT: Navigate to 1.7, verify 'map not phone book' description. IF mismatch: update. Check if section properly foreshadows Ch2 without premature detail.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:24.327936-05:00","updated_at":"2025-12-20T19:31:19.841105-05:00","closed_at":"2025-12-20T19:31:19.841105-05:00","close_reason":"Closed"}
{"id":"babygpt-2ml","title":"PHASE 8: Gap Detection","description":"Find concepts used before defined (7 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:11:00.094835-05:00","updated_at":"2025-12-20T20:30:36.701205-05:00","closed_at":"2025-12-20T20:30:36.701205-05:00","close_reason":"Gap detection audit complete. Fixed: perplexity definition. Other concepts properly introduced before use."}
{"id":"babygpt-2ox","title":"SoftmaxSimplexViz: Polish sweep","description":"Sweep SoftmaxSimplexViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states, ambient glow?)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Trail animation smoothness\n- Compare against GradientDescentViz/CrossEntropyViz as gold standard","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:51:31.167963-05:00","updated_at":"2025-12-20T22:52:16.12619-05:00","closed_at":"2025-12-20T22:52:16.12619-05:00","close_reason":"Recreating with expanded gold standards"}
{"id":"babygpt-2tf","title":"3.10 KNOWN: Audit 1.1.8 Applying Chain Rule placement","description":"KNOWN ISSUE: Appears after KenLM/Sparsity tangents, may break narrative flow","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:13:54.365264-05:00","updated_at":"2025-12-20T19:41:38.060262-05:00","closed_at":"2025-12-20T19:41:38.060262-05:00","close_reason":"Closed"}
{"id":"babygpt-30e","title":"1.14 Audit Ch2 ChapterMap missing sections","description":"AUDIT: List all 13 Ch2 sections, compare to 6 ChapterMap waypoints. IDENTIFY: critical sections missing (softmax 2.7? synthesis 2.9?). CREATE TICKET to add missing waypoints if navigation UX suffers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:39.002014-05:00","updated_at":"2025-12-20T20:24:36.059618-05:00","closed_at":"2025-12-20T20:24:36.059618-05:00","close_reason":"FIXED: Added missing ChapterMap waypoints for sections 2.5, 2.7, 2.8, 2.9","comments":[{"id":10,"issue_id":"babygpt-30e","author":"andrewlouis","text":"NEEDS_FIX: ChapterMap has 6 waypoints but Chapter 2 has 14 sections (including subsections). Critical missing waypoints: 2.5 (Embedding Lookup - important mechanical detail), 2.7 (Softmax - critical for probabilities), 2.8 (Tensors - batching), 2.9 (Synthesis - ties counts to coordinates). Recommendation: Add waypoints for 2.5, 2.7, and 2.9 at minimum. 2.8 optional but helpful for navigation.","created_at":"2025-12-21T00:39:13Z"}]}
{"id":"babygpt-3538","title":"Add ConditioningShiftViz: visualize how conditioning shifts a probability distribution (3D-ish)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.33.28.png. They want a beautiful visualization showing “probability distribution shift” when you condition (P(X) vs P(X|context)). They suggested a 3D graph vibe like a PMF plot from school.\n\nWhere to place\n- Chapter 1, Section 1.1.4 “Conditional Probability” (near the bullet list with P(e), P(e|th), P(q|th)).\n- File: src/chapters/Chapter1.tsx\n\nImplementation\n1) Create a new component: src/components/ConditioningShiftViz.tsx + src/components/ConditioningShiftViz.module.css\n2) Export it from src/components/index.ts (barrel file).\n3) The viz should show at least two distributions over the same support:\n   - Unconditional distribution (baseline)\n   - Conditional distribution given a context (shifted)\n4) “3D” is optional but the output must feel premium: use perspective/isometric bars or a layered depth effect via SVG/CSS. Avoid adding heavy new npm dependencies unless absolutely necessary.\n5) Include clear labels and a simple toggle (e.g., buttons) to switch between distributions, or show both at once.\n\nAcceptance\n- Viz clearly communicates “conditioning changes the whole distribution”.\n- Visual style matches existing BabyGPT viz aesthetics (dark glass, cyan/magenta accents).\n- Responsive on mobile.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:08.400275-05:00","updated_at":"2025-12-21T16:03:27.845155-05:00","closed_at":"2025-12-21T16:03:27.845155-05:00","close_reason":"Added ConditioningShiftViz (3D-ish layered bar chart) and inserted into Ch1 Section 1.1.4; exported via barrel; npm run build passes.","dependencies":[{"issue_id":"babygpt-3538","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.141152-05:00","created_by":"daemon"}]}
{"id":"babygpt-3ch","title":"Dot Product: Add 3B1B-quality geometric animations","description":"GeometricDotProductViz: SVG arrows with projection/shadow animation. Vectors as arrows from origin (cyan A, magenta B). Shadow animation sequence: perpendicular drop, landing point, multiply by |B|. Interactive drag to change angle. Insert BEFORE existing DotProductViz. Pattern: follow GradientDescentViz for SVG+CSS transitions.","status":"closed","issue_type":"feature","created_at":"2025-12-20T22:13:12.208942-05:00","updated_at":"2025-12-20T22:30:35.703355-05:00","closed_at":"2025-12-20T22:30:35.703355-05:00","close_reason":"Implemented GeometricDotProductViz with 3B1B-style visualization: draggable vector arrows, projection/shadow animation, angle arc (green for positive, red for negative), real-time dot product calculation, magnitude breakdown. Added bridge text connecting geometric 'shadow' view to probability 'overlap' view."}
{"id":"babygpt-3jd","title":"task","description":"Perplexity is first used at line 757 (section 1.1.7.1 'One Billion Word Benchmark' callout) without definition. The formula (perplexity = 2^H) appears later at line 903 in Exercise 1.1. REMEDIATION: Add perplexity definition (perplexity = 2^H where H is cross-entropy) either: (1) before line 757, OR (2) add forward reference callout at line 757 like 'perplexity (a metric we'll define shortly)' and ensure clear definition by line 903.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:56:01.399602-05:00","updated_at":"2025-12-20T20:27:41.047137-05:00","closed_at":"2025-12-20T20:27:41.047137-05:00","close_reason":"FIXED: Added inline definition of perplexity at first use (line 757): explains it measures model confusion as effective number of choices, lower is better, with example"}
{"id":"babygpt-3rk","title":"Code blocks: prevent inline-code pill styling inside \u003cpre\u003e","description":"Global inline \u003ccode\u003e styling was bleeding into Shiki-rendered code blocks, making each line look like a pill. Add a global pre code reset so code blocks render cleanly.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:45:18.997273-05:00","updated_at":"2025-12-21T10:45:26.195125-05:00","closed_at":"2025-12-21T10:45:26.195125-05:00","close_reason":"Fixed by resetting pre code styles in global.css"}
{"id":"babygpt-3u3","title":"2.10 Gap: Why this gradient is the right direction","description":"Line ~1088-1090: Gradient formula given as 'predicted centroid minus actual embedding'. Shows WHAT, not WHY this is the right direction. FIX: Add intuition: gradient descent moves embeddings to reduce loss. The gradient points toward where the embedding SHOULD have been (the actual) and away from where it wrongly pointed (other candidates). It's literally 'move toward truth, away from mistakes'.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:32.099972-05:00","updated_at":"2025-12-20T21:52:27.317001-05:00","closed_at":"2025-12-20T21:52:27.317001-05:00","close_reason":"Fixed: Added WHY this direction - score = context·candidate, moving closer to E[actual] increases dot product with right answer → higher probability → lower loss"}
{"id":"babygpt-3w3","title":"Restore missing Chapter 1 callouts","description":"Add: This Becomes the Training Objective, Shannon's Information Theory, Meet the engineers, The Engineering Workhorse: KenLM","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T18:34:04.240474-05:00","updated_at":"2025-12-20T18:47:03.787481-05:00","closed_at":"2025-12-20T18:47:03.787481-05:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Added: This Becomes the Training Objective, Shannon's Information Theory (Citations), When Was This the Dominant Approach, From Surprise to Loss","created_at":"2025-12-20T23:38:34Z"},{"id":4,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Added: Sampling: Let It Talk (intro paragraphs before A concrete benchmark)","created_at":"2025-12-20T23:42:00Z"},{"id":5,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Remaining candidates from backup not in current: Build a toy model, Build the pipeline, Define the goal, Dot Product, Grassmann's Vision, Hit the limit, Meet the engineers, Overlap Isn't Meaning, Pick the biggest term, Retrain on corpus, Shannon's Question, Solution 1: Decomposition, Sort by score/coordinate, The Atoms, The Embedding Table, The Engineering Workhorse: KenLM, The Ground Truth, The Handoff, The Job Description, The Math: Chain Rule","created_at":"2025-12-20T23:43:44Z"},{"id":6,"issue_id":"babygpt-3w3","author":"andrewlouis","text":"Analysis complete: Most 'missing' titles are: (1) ChapterMap navigation entries (Build a toy model, Define the goal, Hit the limit, Meet the engineers, The Ground Truth, The Handoff - not actual content sections), (2) Interactive component button labels (Pick the biggest term, Sort characters by score, Retrain on corpus), (3) Content exists under different names (Overlap Isn't Meaning → Overlap vs. Understanding, Solution 1: Decomposition → content at line 337, Shannon's Question → content at line 105). All actual content sections have been restored.","created_at":"2025-12-20T23:45:49Z"}]}
{"id":"babygpt-431","title":"1.1.3 Gap: Why expect transfer between 'the cat' and 'the dog'","description":"Line ~400: P=0 for unseen context stated as problem, but WHY should we expect 'the cat' knowledge to help with 'the dog'? What principle? FIX: Add explicit statement: humans generalize because 'cat' and 'dog' share syntactic role (noun after article). Counting models have no notion of 'role' — only exact string match. This is the fundamental limit we're building toward.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:52.673357-05:00","updated_at":"2025-12-20T20:56:09.459048-05:00","closed_at":"2025-12-20T20:56:09.459048-05:00","close_reason":"FIXED: Added explicit explanation of syntactic role as basis for generalization - humans expect 'cat' and 'dog' to be related because they share grammatical function (nouns after article), while counting models lack this notion of role"}
{"id":"babygpt-43p","title":"9.3 Redundancy audit: why embeddings motivation","description":"AUDIT: Compare embedding motivation in 1.6, 1.7, 2.1, 2.2. IF excessive: create ticket to reduce OR add 'building on the motivation from...' to make spiral pedagogy explicit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:08.898664-05:00","updated_at":"2025-12-20T20:29:54.992536-05:00","closed_at":"2025-12-20T20:29:54.992536-05:00","close_reason":"PASS: 1.6/1.7 set up the problem (sparsity), 2.1/2.2 provide the solution (embeddings). Progressive build-up, not redundancy."}
{"id":"babygpt-4bi","title":"3.8 KNOWN: Audit 1.1.7.1 KenLM placement","description":"KNOWN ISSUE: Engineering (hashing, linear probing) before motivation. Check if decoder/beam search concepts introduced without context","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:13:33.283801-05:00","updated_at":"2025-12-20T19:40:32.292831-05:00","closed_at":"2025-12-20T19:40:32.292831-05:00","close_reason":"Closed"}
{"id":"babygpt-4wn","title":"CSS: Extract card/panel background patterns to utilities","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:11.182352-05:00","updated_at":"2025-12-20T23:30:02.384798-05:00","closed_at":"2025-12-20T23:30:02.384798-05:00","close_reason":"Closed"}
{"id":"babygpt-5dh","title":"GradientDescentViz: Polish sweep","description":"Audit GradientDescentViz for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (gradients, transitions, hover states)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:04.481612-05:00","updated_at":"2025-12-20T22:54:57.310352-05:00","closed_at":"2025-12-20T22:54:57.310352-05:00","close_reason":"Wrong - this IS a gold standard (Dec 15), not needing sweep"}
{"id":"babygpt-5k0","title":"4.11 Audit 2.10 gradient formula dependencies","description":"AUDIT: Read section 2.10 gradient formula, verify all prereqs present: dot product (2.6), softmax (2.7), cross-entropy. IF any missing: add SectionLink. Check if formula has intuitive explanation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:23.421747-05:00","updated_at":"2025-12-20T19:56:29.380412-05:00","closed_at":"2025-12-20T19:56:29.380412-05:00","close_reason":"Closed"}
{"id":"babygpt-5ux","title":"1.3 Gap: Causal mask never explained","description":"Line ~1418-1422: 'Causal Mask' mentioned but never explained. What does 'blocking vision' mean mathematically? FIX: Add Callout explaining causal masking. In attention, each position can 'see' other positions. Causal mask sets attention weights to 0 for future positions (i \u003e j). Mathematically: mask[i,j] = 0 if j \u003e i, else 1. Multiply attention scores by mask before softmax.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:55.60735-05:00","updated_at":"2025-12-20T21:21:05.10468-05:00","closed_at":"2025-12-20T21:21:05.10468-05:00","close_reason":"FIXED: First principles - 'When predicting char 5, only look at 0-4. If you could see 5, you'd copy it.' No jargon (attention/autoregressive/time arrow)."}
{"id":"babygpt-665","title":"9.1 Redundancy audit: sparsity explanations","description":"AUDIT: Compare sparsity discussion in 1.1.2, 1.1.3, 1.1.7.2, 1.6. Document what each adds. IF excessive overlap: create ticket to consolidate OR add 'As we saw in section X...' callbacks to make repetition intentional.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:08.324132-05:00","updated_at":"2025-12-20T19:51:53.838302-05:00","closed_at":"2025-12-20T19:51:53.838302-05:00","close_reason":"Closed"}
{"id":"babygpt-66l","title":"Inline code styling polish sweep - fix background and visual consistency across all chapters","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:10:53.971804-05:00","updated_at":"2025-12-20T23:14:34.396536-05:00","closed_at":"2025-12-20T23:14:34.396536-05:00","close_reason":"Closed"}
{"id":"babygpt-6ar","title":"10.1 Transition audit: 1.1.6-\u003e1.1.7.1","description":"AUDIT: Check bridge from 'building probabilities from corpus' to 'KenLM engineering'. IF abrupt: create ticket to add transition paragraph like 'Now that we can build probabilities, how do we make this fast?'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.1061-05:00","updated_at":"2025-12-20T19:53:45.333726-05:00","closed_at":"2025-12-20T19:53:45.333726-05:00","close_reason":"Closed"}
{"id":"babygpt-6bq","title":"GradientTraceDemo: Polish sweep","description":"Sweep GradientTraceDemo for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (code highlighting, step transitions, output formatting)\n- Accessibility (aria labels, keyboard nav for step buttons)\n- Mobile responsiveness (two-panel layout on small screens)\n- Compare against CodeWalkthrough/NeuralTrainingDemo as gold standard","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:51:32.019662-05:00","updated_at":"2025-12-20T22:52:16.390006-05:00","closed_at":"2025-12-20T22:52:16.390006-05:00","close_reason":"Recreating with expanded gold standards"}
{"id":"babygpt-6fl","title":"3.15 Audit 1.7 What's Next dependencies","description":"AUDIT: Read section 1.7, verify embeddings are foreshadowed but NOT explained. IF too much detail: create ticket to trim. IF not enough foreshadowing: add 'we need a way to share knowledge' hook.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:55.799332-05:00","updated_at":"2025-12-20T19:42:29.661732-05:00","closed_at":"2025-12-20T19:42:29.661732-05:00","close_reason":"Closed"}
{"id":"babygpt-6ol0","title":"CausalMaskViz: redesign for expressiveness + premium layout (match other vizzes)","description":"Context\n- User feedback + screenshot: Screenshot 2025-12-21 at 10.48.20.png. The current causal mask grid is clean but feels weak compared to other BabyGPT visualizations. Goal: a reader should almost grasp the causal-mask idea immediately from the viz layout/interaction.\n\nWhere\n- Component: src/components/CausalMaskViz.tsx (search for: \"Hover over the grid to check visibility\")\n- Styles: src/components/CausalMaskViz.module.css\n- Used in chapter content via barrel import (src/components/index.ts already exports CausalMaskViz).\n\nWhat\n- Current viz: a static-ish 0/1 lower-triangular grid with hover. It communicates the rule, but doesn't feel “alive” or conceptually rich.\n\nTask\n1) Audit the current UX (run npm run dev, navigate to the CausalMaskViz section) and note what's missing compared to other premium vizzes (VizCard layout, clear legend, immediate story).\n2) Redesign the component to feel like other vizzes:\n   - Wrap in \u003cVizCard\u003e if consistent with other viz components (title/subtitle/figNum optional).\n   - Add a legend that clearly states: “row token can attend to column tokens”.\n   - Make hover interaction more informative: on hover over a row token T_i, highlight the visible set (0..i) and dim the rest; optionally show a short sentence like “T4 can see T1–T4; cannot see T5”.\n   - Consider adding a small “decoder lens” metaphor (corridor / visibility cone) or animation showing information flow left-to-right.\n   - Keep it lightweight (SVG/CSS/DOM). No new heavy dependencies.\n3) Match visual language of other vizzes (glass card, ambient glow, cyan/magenta accents, clean typography).\n4) Ensure accessibility:\n   - Keyboard focusable cells or row selectors (at least rows).\n   - ARIA labels for what is visible when selected/hovered.\n5) Verify responsiveness (mobile layout) and that it's still clear when scaled down.\n\nAcceptance\n- The viz looks and feels consistent with the best vizzes elsewhere (premium layout, clear labeling).\n- A first-time reader can understand “no peeking right” quickly, without reading paragraphs.\n- Hover/selection produces an obvious, explanatory effect (not subtle).\n- No regressions to build: npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T11:01:43.690454-05:00","updated_at":"2025-12-21T14:38:36.912101-05:00","closed_at":"2025-12-21T14:38:36.912101-05:00","close_reason":"Redesigned CausalMaskViz with VizCard + legend + row selection + strong highlight/dim interaction and accessible live readout; matches premium viz styling; build passes.","dependencies":[{"issue_id":"babygpt-6ol0","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T11:02:03.622402-05:00","created_by":"daemon"}]}
{"id":"babygpt-6r7","title":"PHASE 6: Terminology Consistency","description":"Audit term usage consistency across chapters (7 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:59.510839-05:00","updated_at":"2025-12-20T20:30:34.498443-05:00","closed_at":"2025-12-20T20:30:34.498443-05:00","close_reason":"Terminology consistency audit complete. All 7 issues passed: context/history, token/character, embedding/vector, loss/surprise/cross-entropy, logits/scores, context_length/block_size/T, notation consistent."}
{"id":"babygpt-6sy","title":"2.4 Validate Ch1 invariant: Training pairs from sliding window","description":"AUDIT: Read sections 1.3.x, verify (context, target) pair generation is clearly shown. IF missing: create ticket to add SlidingWindowDemo or visual. IF present but unclear: create ticket to add step-by-step walkthrough.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:00.861869-05:00","updated_at":"2025-12-20T19:32:32.920995-05:00","closed_at":"2025-12-20T19:32:32.920995-05:00","close_reason":"Closed"}
{"id":"babygpt-6u9","title":"3.4 Audit 1.1.3 Why This Happens dependencies","description":"AUDIT: Read section 1.1.3, verify 27^N explosion formula comes AFTER joint probability is motivated. Check ExplosionDemo has adequate textual setup. IF gap: add setup paragraph.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:32.143694-05:00","updated_at":"2025-12-20T19:42:01.188407-05:00","closed_at":"2025-12-20T19:42:01.188407-05:00","close_reason":"Closed"}
{"id":"babygpt-71u","title":"2.5 Validate Ch1 invariant: X[i] shifted by 1 = Y[i]","description":"AUDIT: Read sections 1.3.2/1.4, verify 'targets are inputs shifted by 1' is explicitly shown. IF missing: create ticket to add visual or code showing X/Y alignment. IF unclear: add explicit explanation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:01.151008-05:00","updated_at":"2025-12-20T19:32:33.208679-05:00","closed_at":"2025-12-20T19:32:33.208679-05:00","close_reason":"Closed"}
{"id":"babygpt-7eh","title":"2.6 Gap: Why similarity must distribute over addition","description":"Line ~704-710: The constraint (αa + βb)·c = α(a·c) + β(b·c) is given but not motivated. WHY require this? What breaks without it? FIX: Add explanation: if we blend embeddings (weighted average), we want the similarity of the blend to be the weighted average of similarities. This is ESSENTIAL for attention, where we compute weighted sums of values. Linearity = predictable blending behavior.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:17.439573-05:00","updated_at":"2025-12-20T21:46:40.899782-05:00","closed_at":"2025-12-20T21:46:40.899782-05:00","close_reason":"Fixed: Concrete numbers - 'a' scores 0.7, 'e' scores 0.5, average scores 0.6. You already know the answer."}
{"id":"babygpt-7f6o","title":"Ch1 copy: add precise meaning + origin of 'encode'/'decode' in tokenization section","description":"Context\n- User request: give the reader a precise, grounded definition of “encode” and “decode” (and a bit of etymology) when we introduce stoi/itos.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for the existing lines:\n  - “Encoding means: walk through the text…”\n  - “Decoding means: take IDs…”\n\nTask\n1) Expand that area slightly with a 2–4 sentence explanation:\n   - “code” as an agreed mapping between symbols\n   - encoding = symbols → IDs, decoding = IDs → symbols\n   - in this chapter, the “code” is just stoi/itos dictionaries\n2) Keep it short and concrete; avoid grandiose phrasing.\n\nAcceptance\n- Reader understands why the word “encode” is used here, not just what the steps are.\n- No extra jargon added.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:09.464484-05:00","updated_at":"2025-12-21T14:54:43.583289-05:00","closed_at":"2025-12-21T14:54:43.583289-05:00","close_reason":"Added short, concrete 'code as mapping' explanation before encode/decode definitions (stoi/itos as the codebook); build passes.","dependencies":[{"issue_id":"babygpt-7f6o","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.178805-05:00","created_by":"daemon"}]}
{"id":"babygpt-7g8","title":"7.5 Difficulty audit: Ch2 internal flow","description":"AUDIT: Map difficulty curve through Ch2 (philosophical-\u003econceptual-\u003emechanical-\u003emath). IF abrupt jumps: identify specific section boundaries needing transition text.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:29.491128-05:00","updated_at":"2025-12-20T20:07:49.488919-05:00","closed_at":"2025-12-20T20:07:49.488919-05:00","close_reason":"Closed"}
{"id":"babygpt-7zm","title":"SoftmaxSimplexViz: Polish sweep","description":"Sweep SoftmaxSimplexViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states, ambient glow?)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Trail animation smoothness\n\nGold standards to reference:\n- GradientDescentViz (ambient glow, formula display, step interaction)\n- CrossEntropyViz (curve rendering, slider interaction, dynamic labels)\n- CodeWalkthrough (panel layouts)\n- NeuralTrainingDemo (interactive training viz)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:52:30.870818-05:00","updated_at":"2025-12-20T23:11:20.692197-05:00","closed_at":"2025-12-20T23:11:20.692197-05:00","close_reason":"Closed"}
{"id":"babygpt-80n","title":"4.4 Audit 2.4 Vectors Are Storage dependencies","description":"AUDIT: Read section 2.4, verify transition from 'what to store' (2.3) to 'how to store it'. Check one-hot encoding connects to later matrix mult. IF gap: add bridging sentence.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.389554-05:00","updated_at":"2025-12-20T19:52:11.048675-05:00","closed_at":"2025-12-20T19:52:11.048675-05:00","close_reason":"Closed"}
{"id":"babygpt-882","title":"6.1 Terminology audit: context vs history","description":"AUDIT: grep both terms in Ch1/Ch2, document usage counts and locations. IF inconsistent: create ticket to standardize on one term with search-replace. IF interchangeable: add callout explaining both terms.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:10.802391-05:00","updated_at":"2025-12-20T20:29:36.974728-05:00","closed_at":"2025-12-20T20:29:36.974728-05:00","close_reason":"PASS: 'context' is used consistently (60 uses in Ch1, 39 in Ch2). 'history' is rarely used. Terminology is clear."}
{"id":"babygpt-8il","title":"CSS: Standardize transition timing to design tokens","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:12.085686-05:00","updated_at":"2025-12-20T23:30:48.107098-05:00","closed_at":"2025-12-20T23:30:48.107098-05:00","close_reason":"Closed"}
{"id":"babygpt-8jy","title":"PHASE 5: Cross-Reference Validation","description":"Validate all SectionLinks point correctly (5 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:59.192094-05:00","updated_at":"2025-12-20T20:30:33.445132-05:00","closed_at":"2025-12-20T20:30:33.445132-05:00","close_reason":"Cross-reference validation complete. Fixed: 2 incorrect SectionLinks in Ch2 exercises (2.2→2.3)."}
{"id":"babygpt-8lk","title":"4.7 Audit 2.7 Softmax dependencies","description":"AUDIT: Read section 2.7, verify it connects to Ch1 'sum to 1' invariant. Check logits are introduced before softmax applied. IF no Ch1 connection: add 'Recall from Ch1 that...' callout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:22.260705-05:00","updated_at":"2025-12-20T19:54:25.361206-05:00","closed_at":"2025-12-20T19:54:25.361206-05:00","close_reason":"Closed"}
{"id":"babygpt-8ne","title":"10.3 Transition audit: 1.5-\u003e1.6","description":"AUDIT: Check bridge from 'what we built' summary to 'the limit'. IF abrupt: create ticket to add transition like 'Everything works... until it doesn't.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.686574-05:00","updated_at":"2025-12-20T19:53:45.925594-05:00","closed_at":"2025-12-20T19:53:45.925594-05:00","close_reason":"Closed"}
{"id":"babygpt-8sj","title":"8.3 Gap audit: gradient","description":"AUDIT: Find first 'gradient' usage, verify 'slope' intuition is adequate prep. IF gap: create ticket to add intuitive gradient intro in 2.10 before formula.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:46.520425-05:00","updated_at":"2025-12-20T20:10:58.420491-05:00","closed_at":"2025-12-20T20:10:58.420491-05:00","close_reason":"Closed"}
{"id":"babygpt-8us","title":"1.12 Validate Ch2 map waypoint 2.6 Dot Product","description":"AUDIT: Navigate to 2.6, verify 'similarity score you can compute' description. IF mismatch: update. Check if dot product as similarity is clearly established.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:38.416958-05:00","updated_at":"2025-12-20T19:36:27.88625-05:00","closed_at":"2025-12-20T19:36:27.88625-05:00","close_reason":"Closed"}
{"id":"babygpt-8yn","title":"3.12 Audit 1.3 Sliding Window dependencies","description":"AUDIT: Read section 1.3, verify it requires tokenization (1.2) and refs decomposition from 1.1.5. IF missing refs: add SectionLinks. Check if sliding window concept has visual support.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:54.936707-05:00","updated_at":"2025-12-20T19:42:28.81226-05:00","closed_at":"2025-12-20T19:42:28.81226-05:00","close_reason":"Closed"}
{"id":"babygpt-9j5","title":"2.8 Validate Ch2 invariant: Embeddings give learnable geometry","description":"AUDIT: Read sections 2.1/2.4, verify 'embeddings provide learnable geometry' is established. IF missing: create ticket to add visual showing geometry emerging from training. IF unclear: add Grassmann connection.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.348171-05:00","updated_at":"2025-12-20T19:42:48.676355-05:00","closed_at":"2025-12-20T19:42:48.676355-05:00","close_reason":"Closed"}
{"id":"babygpt-9qo","title":"2.3 Validate Ch1 invariant: Tokenization via stoi/itos","description":"AUDIT: Read sections 1.2.x, verify stoi/itos pattern is established with code examples. IF missing code: create ticket to add TokenizerDemo or code block. IF pattern unclear: create ticket to add explicit naming.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:00.570455-05:00","updated_at":"2025-12-20T19:32:32.628999-05:00","closed_at":"2025-12-20T19:32:32.628999-05:00","close_reason":"Closed"}
{"id":"babygpt-9vf","title":"2.10 Validate Ch2 invariant: Embedding lookup is E[ix]","description":"AUDIT: Read sections 2.5.x, verify embedding lookup as row selection E[ix] is shown with code. IF missing code: create ticket to add NumPy example. IF unclear: add visual showing row extraction.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.932113-05:00","updated_at":"2025-12-20T19:44:30.92558-05:00","closed_at":"2025-12-20T19:44:30.92558-05:00","close_reason":"Closed"}
{"id":"babygpt-a2s","title":"6.7 KNOWN: Audit notation shift Ch1-\u003eCh2","description":"KNOWN ISSUE: Document ALL notation changes (probability notation, variable names, tensor shapes). Create ticket to add 'Notation Bridge' section at Ch2 start if significant differences found.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:16:12.505195-05:00","updated_at":"2025-12-20T20:30:03.290638-05:00","closed_at":"2025-12-20T20:30:03.290638-05:00","close_reason":"PASS: Notation is consistent across chapters. Variables introduced with same meaning: P(next|c), E[ix], D, T, B. Ch2 builds on Ch1 notation without conflict."}
{"id":"babygpt-a61","title":"4.1 Audit 2.1 Grassmann dependencies","description":"AUDIT: Read section 2.1, verify it refs Ch1 lookup table limitation. Check 'memorization vs generalization' connects to Ch1 sparsity. IF no Ch1 ref: add SectionLink to 1.6.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:20.532018-05:00","updated_at":"2025-12-20T19:49:30.927063-05:00","closed_at":"2025-12-20T19:49:30.927063-05:00","close_reason":"Closed"}
{"id":"babygpt-a7p","title":"10.6 Transition audit: 2.9-\u003e2.10","description":"AUDIT: Check bridge from 'synthesis: we have a map' to 'the nudge: let's make it move'. IF abrupt: create ticket to add motivating question like 'But how do we make the map accurate?'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:28.568598-05:00","updated_at":"2025-12-20T20:19:47.08151-05:00","closed_at":"2025-12-20T20:19:47.08151-05:00","close_reason":"Closed"}
{"id":"babygpt-ae7","title":"2.12 Validate Ch2 invariant: Dot product is similarity metric","description":"AUDIT: Read section 2.6, verify dot product established as primary similarity metric with formula and example. IF missing formula: add. IF missing example: add WorkedExample.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:20.503823-05:00","updated_at":"2025-12-20T19:46:16.79037-05:00","closed_at":"2025-12-20T19:46:16.79037-05:00","close_reason":"Closed"}
{"id":"babygpt-ahx","title":"9.4 Redundancy audit: sum-to-1 rule","description":"AUDIT: Compare 'probabilities sum to 1' in 1.1.1 vs 2.7. IF Ch2 ref is unnecessary: create ticket to add proper callback 'Recall from Ch1 that...' instead of re-explaining.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:09.182071-05:00","updated_at":"2025-12-20T20:29:55.921523-05:00","closed_at":"2025-12-20T20:29:55.921523-05:00","close_reason":"PASS: Ch1 establishes sum-to-1 as probability axiom (1.1.1), Ch2 references it as motivation for softmax (2.7 line 804). Callback reinforces learning."}
{"id":"babygpt-ar2","title":"1.2 Gap: Why counting models can't share information","description":"Line ~574-575: 'Counting model can't share information across contexts unless literal overlap' — but WHY? What prevents generalization at data structure level? FIX: Add explanation: a hash table maps exact keys to values. There's no notion of 'similar keys'. Hash('cat sat') and Hash('dog sat') are unrelated integers. The data structure has no geometry — keys are just addresses, not locations in a space.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:32.990534-05:00","updated_at":"2025-12-20T21:09:03.018087-05:00","closed_at":"2025-12-20T21:09:03.018087-05:00","close_reason":"FIXED: Added data structure explanation - hash tables map exact keys to unrelated integers with no geometry for similarity"}
{"id":"babygpt-ars","title":"CrossEntropyViz: Polish sweep","description":"Audit CrossEntropyViz for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (curve rendering, guide lines, axis labels)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:05.323407-05:00","updated_at":"2025-12-20T22:54:57.553713-05:00","closed_at":"2025-12-20T22:54:57.553713-05:00","close_reason":"Wrong - this IS a gold standard (Dec 19), not needing sweep"}
{"id":"babygpt-b36k","title":"Ch1 tone: replace 'This isn't a trick or a formula to memorize' with more humble wording","description":"Context\n- User request: avoid language that minimizes the reader's effort. “This isn’t a trick…” can read as dismissive if the concept is hard.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: \"This isn't a trick or a formula to memorize\"\n\nTask\n1) Replace that sentence with wording that is humble and acknowledges this may take a moment to digest.\n2) Keep the meaning: multiplication is just bookkeeping for repeated “of those” filtering; do not add preachy/mic-drop phrasing.\n\nAcceptance\n- Sentence is rewritten with kinder tone, same technical meaning.\n- No other copy changes.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:08.121241-05:00","updated_at":"2025-12-21T14:50:57.206183-05:00","closed_at":"2025-12-21T14:50:57.206183-05:00","close_reason":"Replaced minimizing phrasing with gentler 'looks like a formula at first' wording while keeping the bookkeeping point; build passes.","dependencies":[{"issue_id":"babygpt-b36k","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.899537-05:00","created_by":"daemon"}]}
{"id":"babygpt-bgc","title":"8.6 Gap audit: one-hot encoding","description":"AUDIT: Verify one-hot defined before matrix multiplication explanation in 2.5. IF gap: create ticket to add one-hot intro earlier.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:47.356896-05:00","updated_at":"2025-12-20T20:13:53.369802-05:00","closed_at":"2025-12-20T20:13:53.369802-05:00","close_reason":"Closed"}
{"id":"babygpt-bpm","title":"1.1.7.1 Gap: Why pointer overhead is bad (cache misses)","description":"Line ~1030-1038: Memory calculation concrete but doesn't explain WHY pointer overhead hurts. FIX: Add 1-2 sentences on CPU cache hierarchy. Following a pointer = random memory access = cache miss = ~100 cycles. Array traversal = sequential access = cache hit = ~1 cycle. Pointer-chasing is 100x slower per access. At millions of lookups/second, this dominates runtime.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:40.398536-05:00","updated_at":"2025-12-20T21:10:02.264407-05:00","closed_at":"2025-12-20T21:10:02.264407-05:00","close_reason":"FIXED: Added hardware mechanism explanation for pointer overhead in KenLM section. Cache miss (~100 cycles) vs L1 cache hit (~1 cycle) makes 100× difference at millions of lookups/second."}
{"id":"babygpt-bs5","title":"6.2 Terminology audit: token vs character","description":"AUDIT: find all 'token'/'character' occurrences. IF confusing transitions: add clarifying phrases like 'in our character-level model, each token is a single character'. Document where explicit clarification needed.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.081601-05:00","updated_at":"2025-12-20T20:29:37.892023-05:00","closed_at":"2025-12-20T20:29:37.892023-05:00","close_reason":"PASS: 'character' used for tutorial's char-level focus (91 uses). 'token' used when discussing general concepts (44 uses). Clear distinction maintained."}
{"id":"babygpt-bzz","title":"1.2 Validate Ch1 map waypoint 1.1.5 Chain Rule","description":"AUDIT: Navigate to 1.1.5, verify description 'Turn P(text) into a product of P(next|context)' matches section. IF mismatch: update description to reflect actual section focus. Document what section actually covers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:23.150647-05:00","updated_at":"2025-12-20T19:31:18.711913-05:00","closed_at":"2025-12-20T19:31:18.711913-05:00","close_reason":"Closed"}
{"id":"babygpt-c0k","title":"2.9 Validate Ch2 invariant: Similar = similar predictive role","description":"AUDIT: Read section 2.3, verify 'similar = similar next-char distributions' is explicit. IF missing: create ticket to add CharacterClusterViz or fingerprint comparison. IF implicit: add explicit definition.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.635791-05:00","updated_at":"2025-12-20T19:43:30.661081-05:00","closed_at":"2025-12-20T19:43:30.661081-05:00","close_reason":"Closed"}
{"id":"babygpt-c6g","title":"9.2 Redundancy audit: chain rule explanations","description":"AUDIT: Compare 1.1.5 and 1.1.8 chain rule content. IF 1.1.8 doesn't add value: create ticket to merge into 1.1.5 OR convert 1.1.8 to pure application/example.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:08.60991-05:00","updated_at":"2025-12-20T19:50:14.742334-05:00","closed_at":"2025-12-20T19:50:14.742334-05:00","close_reason":"Closed"}
{"id":"babygpt-c8j","title":"1.3 Validate Ch1 map waypoint 1.1.7.2 Sparsity Trap","description":"AUDIT: Navigate to 1.1.7.2, verify 'counting hits zeros' description. IF mismatch: update to match actual section content. Note if section covers more than just zero-probability problem.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:23.455896-05:00","updated_at":"2025-12-20T19:31:18.997429-05:00","closed_at":"2025-12-20T19:31:18.997429-05:00","close_reason":"Closed"}
{"id":"babygpt-cax","title":"2.2 Validate Ch1 invariant: Probabilities sum to 1","description":"AUDIT: Read section 1.1.1, verify 'probabilities sum to 1' is explicitly stated and demonstrated. IF missing: create ticket to add axiom statement. IF implicit only: create ticket to make explicit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:00.276732-05:00","updated_at":"2025-12-20T19:32:32.328595-05:00","closed_at":"2025-12-20T19:32:32.328595-05:00","close_reason":"Closed"}
{"id":"babygpt-cbs","title":"2.11 Validate Ch2 invariant: Shape [B,T] -\u003e [B,T,D]","description":"AUDIT: Read section 2.8, verify shape transformation [B,T]-\u003e[B,T,D] is explicitly shown. IF missing: create ticket to add TensorShapeBuilder visual. IF present but unclear: add step-by-step shape trace.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:20.216665-05:00","updated_at":"2025-12-20T19:45:33.564581-05:00","closed_at":"2025-12-20T19:45:33.564581-05:00","close_reason":"Closed"}
{"id":"babygpt-cxl","title":"2.1 Gap: Abrupt jump from colors to language","description":"Line ~113-119: MASSIVE jump from 'abstract relationships can be coordinates' to language. What makes language LIKE colors? Answer comes 120 lines later. FIX: Add bridge sentence immediately: 'Colors have measurable relationships (wavelength ratios). Language has measurable relationships too (co-occurrence statistics). If we can measure it, we can coordinatize it. That's the move.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:04.826141-05:00","updated_at":"2025-12-20T21:26:25.553877-05:00","closed_at":"2025-12-20T21:26:25.553877-05:00","close_reason":"FIXED: Replaced vague 'measurable relationships' with specific procedures — Grassmann's knob-matching experiment, our counting. Both produce numbers; previous paragraph establishes numbers=coordinates."}
{"id":"babygpt-def","title":"Extract Slider reusable component (styled range input with label, used in 19 files)","description":"Create a reusable Slider component (styled \u003cinput type=range\u003e) with label, optional value display, min/max/step, and onChange; centralize the CSS used across ~19 files. Export from src/components/index.ts and migrate call sites.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:22.889167-05:00","updated_at":"2025-12-21T00:24:44.480982-05:00","closed_at":"2025-12-21T00:24:44.480982-05:00","close_reason":"Added Slider component and migrated all range inputs to it"}
{"id":"babygpt-dfs","title":"CSS: Extract ambientGlow pattern to shared utility (13 files)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:10.282355-05:00","updated_at":"2025-12-20T23:28:50.945953-05:00","closed_at":"2025-12-20T23:28:50.945953-05:00","close_reason":"Closed"}
{"id":"babygpt-diik","title":"Ch1 restructure: fold Benchmark + Train/Test callout content into core narrative","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.39.25.png: Several important ideas are split across multiple callouts. User wants these to be core content, not fragmented into boxes.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for callout titles around the KenLM/benchmark section (e.g., “Concrete Numbers: The One Billion Word Benchmark”, “Train vs. Test”).\n\nTask\n1) Decide which facts must be in the main narrative (not optional sidebars).\n2) Convert those callouts into normal \u003cParagraph\u003e flow (and possibly a single compact callout for citations only).\n3) Keep the section structure readable: clear subheadings or short paragraphs; avoid wall-of-text.\n\nAcceptance\n- Benchmark + train/test points read as core story, not side quests.\n- The chapter feels more linear and less “boxy”.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:08.94939-05:00","updated_at":"2025-12-21T13:33:24.666338-05:00","closed_at":"2025-12-21T13:33:24.666338-05:00","close_reason":"Folded benchmark/train-test/timeline callouts into main narrative (less boxy), keeping flow linear; build passes.","dependencies":[{"issue_id":"babygpt-diik","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.613028-05:00","created_by":"daemon"}]}
{"id":"babygpt-dsb3","title":"Ch1 structure: establish surprise + perplexity earlier (before KenLM benchmark mention)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.37.05.png: Perplexity shows up later as if the reader already understands it. Reader needs surprise/perplexity established earlier and more organically.\n\nCurrent state\n- Chapter 1 mentions “perplexity 67.6” in the KenLM/benchmark callout before a full, intuitive foundation is laid.\n\nTask\n1) Identify the first time perplexity is mentioned in src/chapters/Chapter1.tsx (search: \"perplexity\").\n2) Add an earlier, plain-language introduction that connects:\n   - probability → surprise (−log₂ p) → average surprise (entropy) → perplexity = 2^{average surprise}\n3) Add a tiny numeric example (2–3 steps) so the reader can compute a perplexity-like number by hand.\n4) Ensure the later benchmark section can reference perplexity without re-teaching basics.\n\nAcceptance\n- Reader encounters surprise + perplexity foundations before any benchmark name-drops.\n- Later perplexity references feel earned, not abrupt.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:08.689512-05:00","updated_at":"2025-12-21T13:32:35.79162-05:00","closed_at":"2025-12-21T13:32:35.79162-05:00","close_reason":"Introduced surprise→cross-entropy→perplexity (with tiny numeric example) before first benchmark mention; build passes.","dependencies":[{"issue_id":"babygpt-dsb3","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.378231-05:00","created_by":"daemon"}]}
{"id":"babygpt-dut","title":"1.7 Audit Ch1 ChapterMap missing sections","description":"AUDIT: List all 18 Ch1 sections, compare to 6 ChapterMap waypoints. IDENTIFY: critical sections missing from map (tokenization 1.2? context length 1.3.1?). CREATE TICKET to add missing waypoints if navigation UX suffers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:24.620052-05:00","updated_at":"2025-12-20T19:31:20.132315-05:00","closed_at":"2025-12-20T19:31:20.132315-05:00","close_reason":"Closed"}
{"id":"babygpt-dx8","title":"PHASE 9: Redundancy Detection","description":"Find excessive repetition vs useful reinforcement (4 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:11:00.385827-05:00","updated_at":"2025-12-20T20:30:44.386463-05:00","closed_at":"2025-12-20T20:30:44.386463-05:00","close_reason":"Redundancy detection audit complete. All 4 issues passed: sparsity builds progressively, chain rule sections complement, embeddings motivation scaffolds, sum-to-1 callback reinforces learning."}
{"id":"babygpt-dzx","title":"2.7 Gap: Why exponentiation amplification is useful","description":"Line ~818-819: 'Exponentiation amplifies differences' — but WHY is this useful? What would happen with a different function? FIX: Add explanation: we want the model to be CONFIDENT — put most probability on one answer. Linear scaling preserves relative gaps. Exponential WIDENS gaps, making the winner dominate. This is 'soft argmax' — we approximate max() but keep it differentiable.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:24.587271-05:00","updated_at":"2025-12-20T21:48:11.854582-05:00","closed_at":"2025-12-20T21:48:11.854582-05:00","close_reason":"Fixed: Concrete numbers [2.0,1.5]→[7.4,4.5]→[0.62,0.38]. WHY: hedging wastes probability, amplification rewards commitment."}
{"id":"babygpt-e0e","title":"Restore missing Chapter 2 sections","description":"Add: Grassmann's Vision Finally Built, Sort characters by score, Sort characters by this coordinate, Solution 1: Decomposition, Overlap Isn't Meaning","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T18:34:15.036781-05:00","updated_at":"2025-12-20T18:48:42.893995-05:00","closed_at":"2025-12-20T18:48:42.893995-05:00","close_reason":"Closed","comments":[{"id":7,"issue_id":"babygpt-e0e","author":"andrewlouis","text":"Verified: Grassmann's Vision content exists at lines 1647-1665 (within Section 2.10 The Nudge). Sort characters buttons are component labels, not sections. Overlap section exists as 2.6. All content present.","created_at":"2025-12-20T23:48:39Z"}]}
{"id":"babygpt-e1gk","title":"Ch1 ending: add stronger motivation for exercises; visualize generalization gap (avoid anti-climax)","description":"Context\n- User feedback: The end of the Chapter 1 “theory” arc feels anti-climactic and does not motivate why the exercises matter or what the reader will implement. They also want a visual demonstration of “intent vs gap” for generalization, ideally using our probability distribution concepts.\n\nWhere\n- Primary file: src/chapters/Chapter1.tsx\n- Look for the Chapter 1 exercises section (search for: title=\"Exercises\" or \u003cExercise).\n- Also look at transitions into exercises and the last paragraphs before the exercises start.\n\nTask\n1) Add a short pre-exercises “Why these exercises exist” block immediately before the exercises section. It must answer explicitly:\n   - What the reader is about to implement (tokenization, sliding window training pairs, etc.).\n   - Why implementation matters: it operationalizes the chain rule + conditioning + surprise into real data a model can train on.\n   - Why we care: the step from memorization (exact match) to generalization requires representing shared structure; this chapter builds the pipeline that makes later learning possible.\n2) Strengthen motivation with a small visual (preferred):\n   - If ConditioningShiftViz (issue babygpt-3538) exists, reuse it or extend it.\n   - Otherwise create a minimal lightweight viz inside Chapter 1 that contrasts:\n       (a) “counting/lookup” behavior: sharp, brittle distribution with zeros\n       vs\n       (b) “generalizing model” intent: smoother distribution that assigns non-zero mass to plausible unseen continuations.\n   - The viz does NOT need to be mathematically perfect; it must be conceptually correct and help intuition.\n3) Avoid hype; use humble, supportive tone. Do not imply the reader “should already get it”.\n\nAcceptance\n- The chapter no longer ends soft: reader feels a clear reason to do exercises and a concrete expectation of what they'll build.\n- If a viz is added, it clearly communicates the generalization gap in one glance.\n- The exercises feel like the natural next move, not a disconnected appendix.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T11:01:43.960899-05:00","updated_at":"2025-12-21T14:47:41.929622-05:00","closed_at":"2025-12-21T14:47:41.929622-05:00","close_reason":"Added pre-exercises motivation block + new GeneralizationGapViz (interactive smoothing slider) to show why P=0 is disastrous and why exercises matter; build passes.","dependencies":[{"issue_id":"babygpt-e1gk","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T11:02:03.868115-05:00","created_by":"daemon"}]}
{"id":"babygpt-eeb","title":"3.6 Audit 1.1.5 Chain Rule dependencies","description":"AUDIT: Read section 1.1.5, verify all prereqs present: joint prob from 1.1.2, conditional prob from 1.1.4. IF any missing: add SectionLink backward reference.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:32.718033-05:00","updated_at":"2025-12-20T19:42:01.781533-05:00","closed_at":"2025-12-20T19:42:01.781533-05:00","close_reason":"Closed"}
{"id":"babygpt-egk","title":"4.3 Audit 2.3 What Can We Measure dependencies","description":"AUDIT: Read section 2.3, verify fingerprint/similarity concept is established BEFORE dot product in 2.6. Check P(next|c) connects to Ch1. IF no Ch1 connection: add SectionLink.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.099299-05:00","updated_at":"2025-12-20T20:25:32.224877-05:00","closed_at":"2025-12-20T20:25:32.224877-05:00","close_reason":"FIXED: Added SectionLink to Chapter 1.1.5 (Chain Rule) in Section 2.3 when introducing P(next|c)","comments":[{"id":12,"issue_id":"babygpt-egk","author":"andrewlouis","text":"NEEDS_FIX: Section 2.3 establishes P(next|c) fingerprint concept but lacks explicit reference to Chapter 1 counting/probability foundations. Recommendation: Add SectionLink to Chapter 1 when introducing P(next|c) to show where this counting concept comes from.","created_at":"2025-12-21T00:51:28Z"}]}
{"id":"babygpt-esr","title":"3.1 Audit 1.1 Physics dependencies","description":"AUDIT: Read section 1.1, check for forward references to later concepts. Verify Shannon 1.3 bits/char target is introduced. IF forward refs found: create ticket to remove or add 'we'll see later' framing. IF Shannon missing: add.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:31.302577-05:00","updated_at":"2025-12-20T19:42:00.260142-05:00","closed_at":"2025-12-20T19:42:00.260142-05:00","close_reason":"Closed"}
{"id":"babygpt-f94","title":"3.3 Audit 1.1.2 Problem With Sequences dependencies","description":"AUDIT: Read section 1.1.2, verify joint probability P(x1,x2,...) is introduced AFTER single-token probability in 1.1.1. IF out of order: create ticket to add bridging sentence or reorder.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:31.86495-05:00","updated_at":"2025-12-20T19:42:00.870741-05:00","closed_at":"2025-12-20T19:42:00.870741-05:00","close_reason":"Closed"}
{"id":"babygpt-fa1","title":"5.1 Validate SectionLink in 1.2 to 1.1","description":"AUDIT: Line 1058 in Ch1, find SectionLink to 1.1. Verify 'sparsity problem' text accurately reflects 1.1 content. IF inaccurate: update link text to match what 1.1 actually covers.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:39.346986-05:00","updated_at":"2025-12-20T19:43:40.154249-05:00","closed_at":"2025-12-20T19:43:40.154249-05:00","close_reason":"Closed"}
{"id":"babygpt-fgj","title":"2.5 Gap: Why one-hot leads to gradients","description":"Line ~610-611: 'One-hot decides which row gets credit' — but WHY does this lead to gradients? Gradient flow unexplained. FIX: Add explanation of gradient routing. One-hot selects row i. Loss depends on row i. Gradient ∂L/∂E[i] is non-zero ONLY for row i. One-hot acts as a 'router' — it tells backprop which embedding to update. All other rows have zero gradient for this example.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:09.295596-05:00","updated_at":"2025-12-20T21:42:32.33788-05:00","closed_at":"2025-12-20T21:42:32.33788-05:00","close_reason":"Fixed: Explained gradient routing from first principles - if row 3 is used and prediction is wrong, only row 3 can be adjusted. No backprop jargon."}
{"id":"babygpt-fp5","title":"1.9 Validate Ch2 map waypoint 2.2 Reuse Question","description":"AUDIT: Navigate to 2.2, verify 'context explosion' description. IF mismatch: update. Check if section clearly establishes the combinatorial explosion problem.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:37.51601-05:00","updated_at":"2025-12-20T19:31:56.502713-05:00","closed_at":"2025-12-20T19:31:56.502713-05:00","close_reason":"Closed"}
{"id":"babygpt-g31","title":"Extract DataRow reusable component (key-value display pattern)","description":"Extract the repeated key/value row pattern (label + value + optional monospace styling) into a DataRow component and migrate call sites.","status":"blocked","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:23.72084-05:00","updated_at":"2025-12-21T14:58:27.955576-05:00"}
{"id":"babygpt-ghl","title":"1.1.1 Gap: Why rare events NEED high surprise","description":"Line ~214-219: Log formula shown but the NECESSITY isn't derived. What breaks with a different function? FIX: Add a Callout showing that log is the UNIQUE function satisfying: (1) additive for independent events, (2) monotonic in probability, (3) continuous. Cite Shannon's uniqueness theorem or derive from first principles.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:35.057347-05:00","updated_at":"2025-12-20T20:53:26.119947-05:00","closed_at":"2025-12-20T20:53:26.119947-05:00","close_reason":"FIXED: Added explanation of Shannon's uniqueness theorem showing log is the unique function satisfying additivity, monotonicity, and continuity requirements for surprise"}
{"id":"babygpt-glfs","title":"Ch1 rendering: audit and remove stray spacing artifacts from {' '} and similar patterns","description":"Context\n- User request: there are visible weird spacing artifacts in rendered prose. They pointed to patterns like: sat\"\u003c/code\u003e{' '} and said there are multiple spots like this.\n\nWhere\n- Primary file: src/chapters/Chapter1.tsx\n- Search for: {' '} (literal {' '} in JSX)\n\nTask\n1) Audit each {' '} insertion in Chapter1 and verify whether it produces visible double spaces / awkward wrapping.\n2) Replace where possible with normal spaces inside text nodes (or rewrite JSX so spaces occur naturally).\n3) Prefer using \u003cTerm\u003e over raw \u003ccode\u003e for inline tokens so spacing is simpler and code color is consistent.\n4) Visually verify the problem areas in the browser.\n\nAcceptance\n- No obvious extra blank areas or weird spacing around inline code/tokens in Chapter1.\n- No semantic changes to content; only whitespace/rendering polish.\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:09.745719-05:00","updated_at":"2025-12-21T14:59:30.434537-05:00","closed_at":"2025-12-21T14:59:30.434537-05:00","close_reason":"Removed all {' '} spacers in Chapter1; replaced a couple inline \u003ccode\u003e tokens with \u003cTerm\u003e and rewrote JSX to keep natural spacing; build passes.","dependencies":[{"issue_id":"babygpt-glfs","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.416811-05:00","created_by":"daemon"}]}
{"id":"babygpt-gmw","title":"2.10 Gap: Why similar stats → nearby points (not proven)","description":"Line ~1129-1134: 'Similar statistics → similar gradients → nearby points' is CLAIMED but not PROVEN. It's a restatement, not a derivation. FIX: Add rigorous explanation. If tokens a,b have similar P(next|a) ≈ P(next|b), they make similar errors on similar examples. Similar errors = similar gradients. Gradient descent moves them in similar directions. Over many steps, they converge to nearby locations. Show this with the training viz.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:50:40.48566-05:00","updated_at":"2025-12-20T21:53:06.0745-05:00","closed_at":"2025-12-20T21:53:06.0745-05:00","close_reason":"Fixed: Concrete derivation - 'a' and 'e' both precede 'n', both get nudged toward E['n']. Same targets → same directions → nearby endpoints."}
{"id":"babygpt-hmy","title":"CorridorDemo UI: remove/replace 'Type to see the corridor narrow...' box; reuse that style for the input","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.30.36.png: the bottom prompt box (\"Type to see the corridor narrow...\") is not necessary, but the style of that box is good. User suggests using that style for the input textbox instead.\n\nWhere\n- Component: src/components/CorridorDemo.tsx (search for \"Type to see the corridor narrow...\")\n- Styles: src/components/CorridorDemo.module.css (.formula is the styled box with border-left accent)\n\nTask\n1) Remove or significantly de-emphasize the empty-state prompt inside the .formula area. Options: hide the formula block until input exists; or replace with a subtle hint near the input.\n2) Apply the “formula box” styling (background + left accent border) to the input area, so the input feels like the primary interaction target.\n3) Keep accessibility: input should remain clearly focusable and readable.\n4) Verify mobile layout (CorridorDemo uses a responsive grid).\n\nAcceptance\n- The demo no longer shows a big bottom prompt box when empty, or it is clearly secondary.\n- Input styling adopts the nice accent-left style and remains readable.\n- No layout regressions; npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:07.857136-05:00","updated_at":"2025-12-21T15:23:08.037218-05:00","closed_at":"2025-12-21T15:23:08.037218-05:00","close_reason":"Moved corridor prompt into input placeholder; hide formula box until input; styled input with left-accent formula treatment; npm run build passes.","dependencies":[{"issue_id":"babygpt-hmy","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.638852-05:00","created_by":"daemon"}]}
{"id":"babygpt-hn3","title":"GeometricDotProductViz: Polish sweep","description":"Sweep GeometricDotProductViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Compare against GradientDescentViz as gold standard","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:51:30.304017-05:00","updated_at":"2025-12-20T22:52:15.874626-05:00","closed_at":"2025-12-20T22:52:15.874626-05:00","close_reason":"Recreating with expanded gold standards"}
{"id":"babygpt-hp1","title":"1.13 Validate Ch2 map waypoint 2.10 The Handoff","description":"AUDIT: Navigate to 2.10, check if ChapterMap says 'The Handoff' but section is titled 'The Nudge'. IF title mismatch: decide which name is better and update for consistency. Verify 'updating when wrong' description.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:38.709081-05:00","updated_at":"2025-12-20T20:23:37.219635-05:00","closed_at":"2025-12-20T20:23:37.219635-05:00","close_reason":"FIXED: ChapterMap title changed from 'The Handoff' to 'The Nudge' to match Section 2.10 title","comments":[{"id":9,"issue_id":"babygpt-hp1","author":"andrewlouis","text":"NEEDS_FIX: Title mismatch - ChapterMap says 'The Handoff' but Section 2.10 is titled 'The Nudge'. Need to choose one title and update both places for consistency. Recommendation: check which title better captures the content (gradient updates vs. handoff to next layer).","created_at":"2025-12-21T00:37:56Z"}]}
{"id":"babygpt-hsy","title":"4.9 Audit 2.9 Synthesis dependencies","description":"AUDIT: Read section 2.9, verify it properly synthesizes 2.1-2.8. Check 'arc' summary is accurate to actual content. IF inaccurate: update synthesis to match reality.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:22.838707-05:00","updated_at":"2025-12-20T19:55:46.76411-05:00","closed_at":"2025-12-20T19:55:46.76411-05:00","close_reason":"Closed"}
{"id":"babygpt-hxa","title":"1.1.2 Gap: Why probabilities must sum to 1","description":"Line ~351-357: 'Probabilities must sum to 1' stated as requirement but WHY? Is it definition, constraint, or convention? FIX: Add explanation that sum-to-1 is the DEFINITION of exhaustive mutually exclusive outcomes. If you're certain SOMETHING happens, the total certainty must be 100%. This enables the 'probability mass' metaphor.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:44.802076-05:00","updated_at":"2025-12-20T20:54:58.273538-05:00","closed_at":"2025-12-20T20:54:58.273538-05:00","close_reason":"FIXED: Added explanation that sum-to-1 is the definition of exhaustive mutually exclusive outcomes, not just a constraint. Introduced 'probability mass' metaphor and conservation of certainty concept."}
{"id":"babygpt-i3d","title":"3.5 Audit 1.1.4 Conditional Probability dependencies","description":"AUDIT: Read section 1.1.4, verify P(X|Y) 'given' notation is introduced BEFORE chain rule in 1.1.5 uses it. IF missing: create ticket to add conditional probability primer.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:32.427399-05:00","updated_at":"2025-12-20T19:42:01.496245-05:00","closed_at":"2025-12-20T19:42:01.496245-05:00","close_reason":"Closed"}
{"id":"babygpt-i3g","title":"7.4 Difficulty audit: Ch1 internal flow","description":"AUDIT: Map difficulty curve through Ch1 (theory-\u003eimpl-\u003etheory). IF abrupt jumps: create tickets to add transition paragraphs at specific locations. Document the curve.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:29.199543-05:00","updated_at":"2025-12-20T19:44:56.133937-05:00","closed_at":"2025-12-20T19:44:56.133937-05:00","close_reason":"Closed"}
{"id":"babygpt-i7a","title":"CSS: Refactor large files (DotProductViz 997 LOC, CharacterClusterViz 952 LOC)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:13.535944-05:00","updated_at":"2025-12-20T23:31:12.937696-05:00","closed_at":"2025-12-20T23:31:12.937696-05:00","close_reason":"Skip - files are well-structured, refactoring is high-risk low-reward"}
{"id":"babygpt-igj","title":"2.6 Validate Ch2 invariant: Replace tables with shared matrices","description":"AUDIT: Read section 2.2, verify 'replace context tables with shared matrices' claim. IF missing: create ticket to add explicit comparison diagram. IF present but unclear: add transition paragraph.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:18.757813-05:00","updated_at":"2025-12-20T19:41:00.558501-05:00","closed_at":"2025-12-20T19:41:00.558501-05:00","close_reason":"Closed"}
{"id":"babygpt-imn","title":"6.5 Terminology audit: logits vs scores","description":"AUDIT: grep 'logit'/'score' in Ch2. IF used interchangeably without explanation: add parenthetical '(also called scores)' at first logits usage. Document findings.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.928108-05:00","updated_at":"2025-12-20T20:29:53.058318-05:00","closed_at":"2025-12-20T20:29:53.058318-05:00","close_reason":"PASS: 'logits' consistently used for raw outputs before softmax (line 821: 'dot product gives us logits'). 'scores' used as general term for similarity scores. Relationship clear."}
{"id":"babygpt-imo","title":"CodeWalkthrough: Polish sweep","description":"Audit CodeWalkthrough for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (code highlighting, step transitions)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n- Document patterns to replicate in other vizzes","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:54:06.164784-05:00","updated_at":"2025-12-20T22:54:57.804254-05:00","closed_at":"2025-12-20T22:54:57.804254-05:00","close_reason":"Wrong - this IS a gold standard (Dec 19), not needing sweep"}
{"id":"babygpt-io8m","title":"Ch1 pedagogy: strengthen the transition into context length (Markov assumption → what gets lost)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.43.21.png. The section setting up context length needs a stronger, more intuitive mental model before we move on.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Area: after decomposition/Markov assumption intro and before later “Sliding Window” mechanics.\n- Look for the paragraph that starts: “What dies when you truncate context?”\n\nTask\n1) Rewrite/expand this transition so the reader can clearly answer:\n   - What “context length” means operationally (what the model can see)\n   - What information is thrown away when n is small\n   - Why increasing context helps but explodes data needs\n2) Consider adding a tiny illustrative example with 2 different context windows (n=2 vs n=5) using the same sentence.\n3) Keep it gentle: explain in words first; keep math optional.\n\nAcceptance\n- Reader has a robust mental model of context length before the chapter moves on.\n- No abrupt jumps; tone stays supportive.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:09.998251-05:00","updated_at":"2025-12-21T14:29:06.381459-05:00","closed_at":"2025-12-21T14:29:06.381459-05:00","close_reason":"Expanded Markov/context-length transition with concrete n-gram window example (keys/cabinet sentence) and explicit memory↔data trade-off; build passes.","dependencies":[{"issue_id":"babygpt-io8m","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.635655-05:00","created_by":"daemon"}]}
{"id":"babygpt-j2e","title":"5.2 Validate SectionLink in 1.3 to 1.1","description":"AUDIT: Line 1164 in Ch1, find SectionLink to 1.1. Verify 'Decomposition Strategy' matches what 1.1 calls it. IF different name: update to match actual terminology used in 1.1.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:39.640729-05:00","updated_at":"2025-12-20T19:43:40.445855-05:00","closed_at":"2025-12-20T19:43:40.445855-05:00","close_reason":"Closed"}
{"id":"babygpt-j3v","title":"10.5 Transition audit: 2.4-\u003e2.5","description":"AUDIT: Check bridge from 'vectors are storage' concept to 'how lookup works' mechanics. IF abrupt: create ticket to add 'Now let's see how to actually retrieve these vectors.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:28.276122-05:00","updated_at":"2025-12-20T20:18:36.111869-05:00","closed_at":"2025-12-20T20:18:36.111869-05:00","close_reason":"Closed"}
{"id":"babygpt-j5q","title":"2.4 Gap: Circular reasoning on coordinates","description":"Line ~444-445: 'Coordinates make sense because of statistical relationships' — but this is circular. We ASSERT embeddings should exist because stats are countable. WHY do countable stats = valid coordinates? FIX: Add the missing step: coordinates are valid if operations on them (add, scale, dot) correspond to meaningful operations on the original objects. Stats form a vector space (can add, scale). That's the justification.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:51.738121-05:00","updated_at":"2025-12-20T21:38:06.397128-05:00","closed_at":"2025-12-20T21:38:06.397128-05:00","close_reason":"Fixed: replaced vague 'vector operations mirror statistical operations' with concrete example - averaging 'a' and 'e' vectors gives high value in followed-by-n slot because both precede n"}
{"id":"babygpt-j6l","title":"3.13 Audit 1.3.2 Free Lunch dependencies","description":"AUDIT: Read section 1.3.2, verify causal masking 'looking right is forbidden' is adequately motivated. IF unclear why: add 'you can't know the future when predicting' callout.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:55.221605-05:00","updated_at":"2025-12-20T19:42:29.111412-05:00","closed_at":"2025-12-20T19:42:29.111412-05:00","close_reason":"Closed"}
{"id":"babygpt-jpz","title":"Rewrite chain rule + log-prob explanation; reframe naive index; clarify KenLM tokens","description":"Update chapter prose: explain why joint probability multiplies via conditional filtering example; show why logs help compute; reframe naive approach as brute-force index of sequences; insert a simple graph viz (reuse NgramGraphViz); clarify that KenLM is an n-gram LM typically word-level but can treat any tokenization (chars/words).","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:20:38.153799-05:00","updated_at":"2025-12-21T10:45:34.282017-05:00","closed_at":"2025-12-21T10:45:34.282017-05:00","close_reason":"Updated Ch1 joint-prob explanation: infinite-resources lookup framing + NgramGraphViz, added 'Why AND multiplies' callout and clearer logs example, clarified KenLM tokenization"}
{"id":"babygpt-k85","title":"1.1.7.1 Gap: Backoff mathematical justification","description":"Line ~1064: Backoff introduced with ladder metaphor but mathematical mechanism missing. WHY is shortening context valid? FIX: Add explanation: if P('sat'|'the cat') is undefined (zero count), we ASSUME it's similar to P('sat'|'cat') or even P('sat'). This is the smoothing assumption — shorter contexts are less precise but non-zero. We trade accuracy for coverage.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:48.40969-05:00","updated_at":"2025-12-20T21:17:05.959569-05:00","closed_at":"2025-12-20T21:17:05.959569-05:00","close_reason":"FIXED: Added backoff ladder explanation + made limit precise: 'the cat sat' and 'the dog sat' remain separate keys, sharing requires exact substring match"}
{"id":"babygpt-kcu","title":"2.2 Gap: HOW modeling tokens solves reuse","description":"Line ~306-309: 'Stop memorizing contexts, start modeling tokens' — but HOW does this solve reuse? Claim stated, not derived. FIX: Add derivation: if 'cat' and 'dog' have similar embeddings, then contexts containing them produce similar predictions. One learned embedding serves ALL contexts containing that token. V embeddings serve V^T contexts. That's the compression: V \u003c\u003c V^T.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:19.739476-05:00","updated_at":"2025-12-20T21:28:49.181037-05:00","closed_at":"2025-12-20T21:28:49.181037-05:00","close_reason":"FIXED: Added derivation showing compression math (V embeddings serve V^T contexts). One embedding for 'cat' serves all contexts containing it. Concrete example: vocab 27, T=3 → 19,683 entries vs 27. Claim now derived from first principles."}
{"id":"babygpt-kic","title":"PHASE 7: Difficulty Curve","description":"Audit difficulty spikes and transitions (5 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:59.794166-05:00","updated_at":"2025-12-20T20:30:35.430159-05:00","closed_at":"2025-12-20T20:30:35.430159-05:00","close_reason":"Difficulty curve audit complete. All issues passed. Collapsibles used appropriately for math spikes."}
{"id":"babygpt-kks","title":"PHASE 3: Ch1 Dependency Audit","description":"Audit conceptual dependencies for each Chapter 1 section (15 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:58.613251-05:00","updated_at":"2025-12-20T20:30:21.076498-05:00","closed_at":"2025-12-20T20:30:21.076498-05:00","close_reason":"Ch1 dependency audit complete. 36/37 issues passed. Fixed: perplexity definition added before first use."}
{"id":"babygpt-kmd","title":"8.2 Gap audit: dot product","description":"AUDIT: Find first usage of dot product, verify definition precedes it. IF gap: create ticket to add conceptual intro before formula.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:46.23742-05:00","updated_at":"2025-12-20T20:09:28.205004-05:00","closed_at":"2025-12-20T20:09:28.205004-05:00","close_reason":"Closed"}
{"id":"babygpt-ko7","title":"PHASE 4: Ch2 Dependency Audit","description":"Audit conceptual dependencies for each Chapter 2 section (11 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:58.911021-05:00","updated_at":"2025-12-20T20:30:21.969548-05:00","closed_at":"2025-12-20T20:30:21.969548-05:00","close_reason":"Ch2 dependency audit complete. Fixed: SectionLinks corrected, Ch1 reference added in 2.3."}
{"id":"babygpt-kp5","title":"Softmax: Elevate from standard to exceptional","description":"Unique insight: Softmax is the ONLY answer to 'least-biased distribution given scores' (max entropy theorem). Add: (1) 'Why exp?' callout explaining inevitability, (2) SoftmaxSimplexViz showing 3-logit sliders → point on triangle, (3) Boltzmann connection as optional collapsible, (4) gradient cancellation explanation.","status":"closed","issue_type":"feature","created_at":"2025-12-20T22:13:13.034021-05:00","updated_at":"2025-12-20T22:25:16.177739-05:00","closed_at":"2025-12-20T22:25:16.177739-05:00","close_reason":"Implemented SoftmaxSimplexViz with probability triangle + temperature, added max-entropy callout (why exp is ONLY answer), gradient callout, and Boltzmann physics connection. Accessible and democratic."}
{"id":"babygpt-kua","title":"Extract StepDots reusable component (step indicator pattern)","description":"Extract the repeated step indicator UI (dots, active state, click handlers) into a StepDots component and migrate call sites.","status":"open","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:24.591116-05:00","updated_at":"2025-12-20T23:40:34.461466-05:00"}
{"id":"babygpt-ld77","title":"Ch1 explain: introduce Kneser–Ney smoothing from first principles (avoid name-drop)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.44.37.png. The chapter currently name-drops “Kneser–Ney 5-gram” without explaining what it is or why it matters.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: “Kneser–Ney”\n\nTask\n1) Add a first-principles explanation before/near the first mention:\n   - Why raw counts produce zeros (sparsity)\n   - What smoothing/backoff is trying to do (share probability mass)\n   - What makes Kneser–Ney special at a high level (continuation probability intuition)\n2) Keep the explanation concrete and avoid long math derivations. Use one small example (e.g., unseen bigram but common word).\n3) After the explanation, it is okay to mention the paper and benchmark.\n\nAcceptance\n- “Kneser–Ney” is no longer a pure name-drop; reader gets an intuition.\n- Explanation is short, concrete, and non-scary.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:10.253331-05:00","updated_at":"2025-12-21T13:34:00.913052-05:00","closed_at":"2025-12-21T13:34:00.913052-05:00","close_reason":"Added first-principles smoothing/backoff + Kneser–Ney intuition (continuation contexts) near first mention; removed name-drop feel; build passes.","dependencies":[{"issue_id":"babygpt-ld77","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:00.862719-05:00","created_by":"daemon"}]}
{"id":"babygpt-lfn","title":"4.8 Audit 2.8 Tensors dependencies","description":"AUDIT: Read section 2.8, verify it requires embedding lookup from 2.5. Check [B,T,D] notation has adequate context. IF shape notation unclear: add dimension explanation table.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:22.541482-05:00","updated_at":"2025-12-20T19:55:05.509377-05:00","closed_at":"2025-12-20T19:55:05.509377-05:00","close_reason":"Closed"}
{"id":"babygpt-lgn","title":"Extract VizCard reusable component (header + content + footer pattern from 11 vizzes)","description":"Create src/components/VizCard.tsx + VizCard.module.css to unify the common viz wrapper: ambient glow container, glass card, header (title + optional figNum/subtitle), content slot, optional footer. Export from src/components/index.ts and refactor initial viz components to use it.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:38:22.055457-05:00","updated_at":"2025-12-21T00:24:44.23236-05:00","closed_at":"2025-12-21T00:24:44.23236-05:00","close_reason":"Added VizCard component and migrated key vizzes"}
{"id":"babygpt-lhx","title":"1.1.5 Gap: What Markov assumption loses","description":"Line ~489-495: Markov Assumption introduced as solution to sparsity, but doesn't explain what we're LOSING. FIX: Add concrete example of information death. E.g., 'The doctor said she would...' — with n=2, we only see 'would' and lose 'doctor' and 'she'. Gender agreement, long-range coherence, topic — all die. This is WHY we eventually need attention.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:17.217057-05:00","updated_at":"2025-12-20T21:06:49.151068-05:00","closed_at":"2025-12-20T21:06:49.151068-05:00","close_reason":"FIXED: Added concrete example showing information loss from Markov truncation - 'The doctor said she would...' example demonstrates loss of gender agreement, professional roles, and topic when context window is too small"}
{"id":"babygpt-lwe","title":"4.6 Audit 2.6 Dot Product dependencies","description":"AUDIT: Read section 2.6, verify it requires fingerprint from 2.3. Check SectionLink to 2.1 Grassmann is accurate. IF inaccurate: fix link or description.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.962273-05:00","updated_at":"2025-12-20T19:53:40.269015-05:00","closed_at":"2025-12-20T19:53:40.269015-05:00","close_reason":"Closed"}
{"id":"babygpt-m4l","title":"6.3 Terminology audit: embedding vs vector","description":"AUDIT: check first usage establishes relationship. IF ambiguous: add definition where 'embedding' first appears. Create ticket to add term to glossary if none exists.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.362548-05:00","updated_at":"2025-12-20T20:29:38.77957-05:00","closed_at":"2025-12-20T20:29:38.77957-05:00","close_reason":"PASS: Relationship established in Ch2 Section 2.4 'Vectors Are Just Storage' - vectors store attributes, embeddings are learned vectors for tokens."}
{"id":"babygpt-mcb","title":"CSS: Add spacing scale tokens to replace ad-hoc values","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:15.095696-05:00","updated_at":"2025-12-20T23:31:50.097453-05:00","closed_at":"2025-12-20T23:31:50.097453-05:00","close_reason":"Closed"}
{"id":"babygpt-miu","title":"1.8 Validate Ch2 map waypoint 2.1 Grassmann","description":"AUDIT: Navigate to 2.1 via waypoint, verify 'colors to language' description matches Grassmann section. IF mismatch: update description to reflect actual historical narrative focus.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:37.19632-05:00","updated_at":"2025-12-20T19:31:16.181193-05:00","closed_at":"2025-12-20T19:31:16.181193-05:00","close_reason":"Closed"}
{"id":"babygpt-mvs","title":"Ch1 layout: remove huge whitespace + gentler transition after lookup-table section","description":"Context\n- User report + screenshot: Screenshot 2025-12-21 at 10.23.35.png shows a large blank vertical gap around the end of the lookup-table/phone-book discussion.\n- In Chapter 1 this is near the paragraph that ends with: \"It's a phone book: entries don't talk to each other.\"\n- Immediately after, the text jumps to: \"The solution? Decomposition—...\"\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for the exact string: \"It's a phone book: entries don't talk to each other.\"\n- Also note there is a nested \u003cSection number=\"1.1.3\" ...\u003e inside \u003cSection number=\"1.1.2\" ...\u003e. Layout.module.css sets .section { margin-bottom: 80px; } which can create huge gaps when sections are nested.\n\nTask\n1) Reproduce the issue locally (npm run dev) and scroll to the location shown in the screenshot.\n2) Remove the excessive whitespace. Prefer structural fixes over magic padding: either\n   - Stop nesting \u003cSection\u003e components (make 1.1.3 a sibling section, not a child), OR\n   - Add CSS rule(s) in src/components/Layout.module.css so nested sections have smaller bottom margin.\n3) Add a gentler transition sentence/paragraph between the phone-book conclusion and the decomposition/Markov assumption intro. The goal is: no abrupt topic switch.\n4) Verify anchors still work (section links in ChapterMap).\n\nAcceptance\n- The large blank vertical gap is gone and spacing matches surrounding content.\n- Transition reads smoothly (no whiplash).\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:07.043583-05:00","updated_at":"2025-12-21T13:16:31.143589-05:00","closed_at":"2025-12-21T13:16:31.143589-05:00","close_reason":"Unnested 1.1.3 from 1.1.2 to remove extra section margin; added bridge into decomposition; verified npm run build.","dependencies":[{"issue_id":"babygpt-mvs","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:57.90807-05:00","created_by":"daemon"}]}
{"id":"babygpt-mw5","title":"PHASE 1: ChapterMap Validation","description":"Validate all ChapterMap waypoints link correctly and descriptions match content (14 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:10:48.567941-05:00","updated_at":"2025-12-20T20:30:19.252676-05:00","closed_at":"2025-12-20T20:30:19.252676-05:00","close_reason":"All ChapterMap validation issues resolved. Fixed: Ch2 title mismatch (Handoff→Nudge), added missing waypoints (2.5, 2.7, 2.8, 2.9)."}
{"id":"babygpt-n36","title":"10.2 Transition audit: 1.1.8-\u003e1.2","description":"AUDIT: Check bridge from chain rule application to 'let's build the tokenizer'. IF abrupt: create ticket to add transition like 'Theory in hand, time to build.'","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.382782-05:00","updated_at":"2025-12-20T19:53:45.624286-05:00","closed_at":"2025-12-20T19:53:45.624286-05:00","close_reason":"Closed"}
{"id":"babygpt-n3z","title":"2.14 Validate Ch2 invariant: Training nudges based on error","description":"AUDIT: Read section 2.10, verify training loop mechanism is clear (predict, measure error, nudge). IF missing: add diagram or pseudo-code. IF unclear: add step-by-step walkthrough.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:21.091193-05:00","updated_at":"2025-12-20T19:48:21.392133-05:00","closed_at":"2025-12-20T19:48:21.392133-05:00","close_reason":"Closed"}
{"id":"babygpt-n6b","title":"Chain Rule: Add Jurafsky-Martin level formal rigor","description":"FormalRigor component: Collapsible with 'Formal' badge. Content: (1) P(B|A) definition → chain rule identity, (2) induction to n, (3) P(context)=0 edge case (formal smoothing motivation), (4) probability space Omega/F/P, (5) optional measure theory hook. Each part connects back to corridor intuition.","status":"closed","priority":1,"issue_type":"feature","created_at":"2025-12-20T22:13:14.680484-05:00","updated_at":"2025-12-20T22:21:24.652277-05:00","closed_at":"2025-12-20T22:21:24.652277-05:00","close_reason":"FormalRigor component implemented with accessible, democratic tone"}
{"id":"babygpt-p78","title":"5.3 Validate SectionLink in 2.6 to 2.1","description":"AUDIT: Line 718 in Ch2, find SectionLink to 2.1. Verify Grassmann backward reference accurately reflects 2.1 content. IF inaccurate: update link text to match.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:39.923788-05:00","updated_at":"2025-12-20T20:01:39.580451-05:00","closed_at":"2025-12-20T20:01:39.580451-05:00","close_reason":"Closed"}
{"id":"babygpt-pqb","title":"3.14 Audit 1.6 The Limit dependencies","description":"AUDIT: Read section 1.6, verify it summarizes sparsity limit and properly sets up Ch2. IF Ch2 not foreshadowed: add explicit 'next chapter will...' statement. IF sparsity not summarized: add recap.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:55.507254-05:00","updated_at":"2025-12-20T19:42:29.404919-05:00","closed_at":"2025-12-20T19:42:29.404919-05:00","close_reason":"Closed"}
{"id":"babygpt-q4t","title":"2.4 Gap: Proper nouns vs adjectives conflation","description":"Line ~428-438: The metaphor conflates two claims: (1) integers are just labels, (2) vectors enable learning. These aren't the same. WHY can't labels learn? FIX: Separate the claims. Labels can't learn because they have no shared structure. 'cat'=17, 'dog'=42 — there's no operation that moves 17 toward 42. Vectors CAN be nudged: v_cat += 0.01 * gradient. Continuous space enables gradient descent.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:43.994118-05:00","updated_at":"2025-12-20T21:32:51.782488-05:00","closed_at":"2025-12-20T21:32:51.782488-05:00","close_reason":"FIXED: Added gradient descent mechanism - integers can't learn because no operation moves 17 toward 42; vectors enable v += 0.01*gradient in continuous space"}
{"id":"babygpt-qh0","title":"4.5 Audit 2.5 Embedding Lookup dependencies","description":"AUDIT: Read section 2.5, verify it requires embedding table concept from 2.4. Check row selection mechanics are adequately motivated. IF unclear: add visual or SectionLink.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:21.678121-05:00","updated_at":"2025-12-20T19:52:55.44936-05:00","closed_at":"2025-12-20T19:52:55.44936-05:00","close_reason":"Closed"}
{"id":"babygpt-r1p","title":"4.10 KNOWN: Audit 2.10 loss function introduction","description":"KNOWN ISSUE: Cross-entropy loss may not be formally defined until this section. Check earlier loss refs","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:14:23.130719-05:00","updated_at":"2025-12-20T20:00:33.591449-05:00","closed_at":"2025-12-20T20:00:33.591449-05:00","close_reason":"Closed"}
{"id":"babygpt-r88","title":"2.2 Gap: WHY can't lookup tables generalize","description":"Line ~288: n-grams called 'islands' but WHY can't hash tables generalize? They can store ANY context... FIX: Add explanation: hash tables map keys to values with no notion of 'nearby keys'. Hash('dog sat') and Hash('cat sat') are unrelated integers. To generalize, you need GEOMETRY — a space where similar inputs are nearby. Hash tables have no geometry. This is the fundamental limit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:49:12.40026-05:00","updated_at":"2025-12-20T21:27:27.23295-05:00","closed_at":"2025-12-20T21:27:27.23295-05:00","close_reason":"FIXED: Added concrete explanation of hash table mechanism - hash functions map strings to unrelated integers (addresses/slots), not coordinates in a space. Change one letter and the hash jumps to a completely different memory location. No notion of distance built into the data structure."}
{"id":"babygpt-rd9","title":"1.11 Validate Ch2 map waypoint 2.4 Embedding Table","description":"AUDIT: Navigate to 2.4, verify 'D numbers per token' description matches 'Vectors Are Just Storage'. IF mismatch: update to reflect actual content.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:38.116683-05:00","updated_at":"2025-12-20T19:33:16.527827-05:00","closed_at":"2025-12-20T19:33:16.527827-05:00","close_reason":"Closed"}
{"id":"babygpt-res","title":"CodeWalkthrough: Polish sweep","description":"Sweep CodeWalkthrough for:\n- Theme consistency (colors, fonts, spacing)\n- Visual polish (code highlighting, step transitions)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n\nReference gold standards:\n- GradientDescentViz (ambient glow, step interaction)\n- CrossEntropyViz (dynamic value display)\n- DotProductViz (panel layouts)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:57:34.641723-05:00","updated_at":"2025-12-20T23:01:57.716875-05:00","closed_at":"2025-12-20T23:01:57.716875-05:00","close_reason":"Polish sweep complete: ambient glow, multi-gradient backgrounds, header structure, transitions, hover states, mobile responsive breakpoints, ARIA labels"}
{"id":"babygpt-sbz","title":"6.6 Terminology audit: context_length vs block_size vs T","description":"AUDIT: find all three terms. IF not explicitly connected: add callout 'These three names mean the same thing: context_length, block_size, and T all refer to...'. Create ticket for fix.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:12.210875-05:00","updated_at":"2025-12-20T20:29:54.015098-05:00","closed_at":"2025-12-20T20:29:54.015098-05:00","close_reason":"PASS: T used consistently for context length. block_size only appears in code context. context_length explained in prose. Equivalence clear in Section 2.8."}
{"id":"babygpt-scr","title":"CSS: Create shared hover/focus state utilities","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:18.33111-05:00","updated_at":"2025-12-20T23:33:21.099617-05:00","closed_at":"2025-12-20T23:33:21.099617-05:00","close_reason":"Closed"}
{"id":"babygpt-sgx","title":"3.7 Audit 1.1.6 Building From Corpus dependencies","description":"AUDIT: Read section 1.1.6, verify it requires chain rule (1.1.5) and builds concrete counting examples. IF chain rule not referenced: add SectionLink. IF counting unclear: add FrequencyTable example.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:33.003599-05:00","updated_at":"2025-12-20T19:42:02.066115-05:00","closed_at":"2025-12-20T19:42:02.066115-05:00","close_reason":"Closed"}
{"id":"babygpt-sh2","title":"Restore misc missing sections","description":"Add: Sampling: Let It Talk, From Surprise to Loss, The Math: Chain Rule, Build a toy model, Build the pipeline, Define the goal, Hit the limit, What comes next, Pick the biggest term in the sum, Retrain on the current chapter corpus","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T18:34:15.938805-05:00","updated_at":"2025-12-20T18:49:01.787359-05:00","closed_at":"2025-12-20T18:49:01.787359-05:00","close_reason":"Closed","comments":[{"id":2,"issue_id":"babygpt-sh2","author":"andrewlouis","text":"Added: A Running Mini-Corpus, Telescoping With The Same Example, We Just Built a Markov Chain, What Chapter 1 Can't Do (Yet)","created_at":"2025-12-20T23:38:37Z"},{"id":3,"issue_id":"babygpt-sh2","author":"andrewlouis","text":"Verified: The Math: Chain Rule content already exists (Geometric Intuition callout + corridor metaphor)","created_at":"2025-12-20T23:39:04Z"},{"id":8,"issue_id":"babygpt-sh2","author":"andrewlouis","text":"Final status: Sampling: Let It Talk ADDED, From Surprise to Loss ADDED (previous session), The Math: Chain Rule exists as 'The Chain Rule', Build a toy model/Build the pipeline/Define the goal/Hit the limit are ChapterMap entries not content sections, What comes next exists as 'What's Next', Pick the biggest term is a button label, Retrain on corpus is a toggle label. All content verified present.","created_at":"2025-12-20T23:48:59Z"}]}
{"id":"babygpt-sly","title":"GradientTraceDemo: Polish sweep","description":"Sweep GradientTraceDemo for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (code highlighting, step transitions, output formatting)\n- Accessibility (aria labels, keyboard nav for step buttons)\n- Mobile responsiveness (two-panel layout on small screens)\n\nGold standards to reference:\n- GradientDescentViz (ambient glow, step-by-step interaction)\n- CrossEntropyViz (dynamic value display)\n- CodeWalkthrough (code panel styling, step highlighting)\n- NeuralTrainingDemo (training step visualization)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:52:31.766306-05:00","updated_at":"2025-12-20T23:13:41.555954-05:00","closed_at":"2025-12-20T23:13:41.555954-05:00","close_reason":"Closed"}
{"id":"babygpt-t4d","title":"10.4 Transition audit: Ch1-\u003eCh2","description":"AUDIT: Check ChapterNav + Section 1.7 adequately prepare reader for Ch2's philosophical style (Grassmann history). IF gap: create ticket to adjust 1.7 ending or add Ch2 preamble.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:17:27.97311-05:00","updated_at":"2025-12-20T20:16:39.15769-05:00","closed_at":"2025-12-20T20:16:39.15769-05:00","close_reason":"Closed"}
{"id":"babygpt-t50","title":"GeometricDotProductViz: Polish sweep","description":"Sweep GeometricDotProductViz for:\n- Theme consistency (colors, fonts, spacing match existing vizzes)\n- Visual polish (gradients, transitions, hover states)\n- Accessibility (aria labels, keyboard nav)\n- Mobile responsiveness\n\nGold standards to reference:\n- GradientDescentViz (ambient glow, formula display, step interaction)\n- CrossEntropyViz (curve rendering, guide lines, axis labels)\n- CodeWalkthrough (panel layouts)\n- NeuralTrainingDemo (interactive training viz)","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T22:52:29.993735-05:00","updated_at":"2025-12-20T23:06:35.831951-05:00","closed_at":"2025-12-20T23:06:35.831951-05:00","close_reason":"Polish sweep complete: vector gradients, keyboard nav (arrow keys + shift), touch support, ARIA labels, cubic-bezier transitions, focus states, mobile responsive, theme-consistent rgba colors"}
{"id":"babygpt-t74","title":"3.9 Audit 1.1.7.2 Sparsity Trap dependencies","description":"AUDIT: Read section 1.1.7.2, verify it builds on counting from 1.1.6. Check zero-probability problem is clearly introduced before cross-entropy formula. IF gap: add bridging explanation.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:54.089992-05:00","updated_at":"2025-12-20T19:42:28.227795-05:00","closed_at":"2025-12-20T19:42:28.227795-05:00","close_reason":"Closed"}
{"id":"babygpt-tfi","title":"CSS: Add breakpoint tokens (currently hardcoded 600/700/768px)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:19.989928-05:00","updated_at":"2025-12-20T23:34:13.078193-05:00","closed_at":"2025-12-20T23:34:13.078193-05:00","close_reason":"Closed"}
{"id":"babygpt-tk7","title":"5.5 Audit implicit Ch2-\u003eCh1 references","description":"AUDIT: Search Ch2 for Ch1 concept reuse (P(next|c), tokenization, chain rule). Check if proper backward refs exist when reusing. IF no refs: add 'As established in Chapter 1...' callouts where appropriate.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:40.517066-05:00","updated_at":"2025-12-20T20:04:39.341073-05:00","closed_at":"2025-12-20T20:04:39.341073-05:00","close_reason":"Closed"}
{"id":"babygpt-tps","title":"8.7 KNOWN: Gap audit dot product-\u003eprobability","description":"KNOWN ISSUE: Connection between dot product and probability (via softmax) may not be foreshadowed in Ch1. AUDIT: check if Ch1 probability discussion sets up Ch2 dot product. IF gap: create ticket to add foreshadowing callout in Ch1.6.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:16:47.6341-05:00","updated_at":"2025-12-20T20:14:40.922591-05:00","closed_at":"2025-12-20T20:14:40.922591-05:00","close_reason":"Closed"}
{"id":"babygpt-twpi","title":"Ch1 transition: add recap of foundations + what we build by end of chapter (before Tokenization)","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.40.02.png. The “Okay, enough theory…” transition should recap what we've established and set expectations for what the reader will build by end of Chapter 1.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: \"Okay, enough theory. Let's build something.\"\n\nTask\n1) Before that sentence, add a short recap list of facts (3–6 bullets) that are now “known”:\n   - probability distributions sum to 1\n   - conditional probability is “filter worlds”\n   - chain rule decomposes sequence probability\n   - logs turn products into sums of surprise\n   - counting models fail due to sparsity / no sharing\n2) Add a clear “by the end of this chapter you will build…” preview. Be concrete (tokenization + sliding window training pairs).\n\nAcceptance\n- Reader feels oriented entering Tokenization.\n- Recap is factual (not hype) and preview is concrete.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:09.20324-05:00","updated_at":"2025-12-21T14:26:01.357195-05:00","closed_at":"2025-12-21T14:26:01.357195-05:00","close_reason":"Added recap (facts earned so far) + concrete 'what you'll build' preview right before Tokenization; build passes.","dependencies":[{"issue_id":"babygpt-twpi","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:59.893102-05:00","created_by":"daemon"}]}
{"id":"babygpt-u2a","title":"7.1 Difficulty audit: KenLM section 1.1.7.1","description":"AUDIT: Compare abstraction level (hashing, linear probing, pointer math) vs surrounding sections. IF spike: create ticket to move to appendix OR add 'optional deep dive' collapsible wrapper.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:28.330005-05:00","updated_at":"2025-12-20T19:44:55.839345-05:00","closed_at":"2025-12-20T19:44:55.839345-05:00","close_reason":"Closed"}
{"id":"babygpt-ud2","title":"PHASE 10: Missing Transitions","description":"Find abrupt section transitions (6 issues)","status":"closed","priority":2,"issue_type":"epic","created_at":"2025-12-20T19:11:00.667977-05:00","updated_at":"2025-12-20T20:30:45.499015-05:00","closed_at":"2025-12-20T20:30:45.499015-05:00","close_reason":"Missing transitions audit complete. All 6 issues passed. Transition prose exists between major sections."}
{"id":"babygpt-ujl","title":"8.1 KNOWN: Gap audit cross-entropy","description":"KNOWN ISSUE: Cross-entropy formula in 1.1.7.2 Sparsity Trap but may not be formally defined until Ch2. AUDIT: trace first usage vs first definition. IF gap: create ticket to add definition before first use OR add forward-ref callout.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-20T19:16:45.94936-05:00","updated_at":"2025-12-20T19:48:40.747249-05:00","closed_at":"2025-12-20T19:48:40.747249-05:00","close_reason":"Closed"}
{"id":"babygpt-upu","title":"1.2 Gap: Why character-level helps connectivity","description":"Line ~540: 'Word graph is disconnected' stated but no explanation of WHY character-level helps. Space char as 'bridge' mentioned but mechanism unclear. FIX: Add explanation: at word level, 'cat' and 'dog' share zero characters. At char level, both contain common suffixes/patterns. Space char appears after EVERY word, creating a hub node. Character-level has inherent overlap that word-level lacks.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:24.292738-05:00","updated_at":"2025-12-20T21:08:05.551959-05:00","closed_at":"2025-12-20T21:08:05.551959-05:00","close_reason":"FIXED: Added concrete explanation of why character-level helps with sparsity. Explained that word-level 'cat' and 'dog' share zero structure, while character-level reveals overlap (consonant endings, CVC patterns). Crucially explained that space character creates a universal hub node with stable statistics from high frequency, enabling recombination of any word-ending with any word-beginning."}
{"id":"babygpt-v1z","title":"Ch1 design: standardize inline code/token color (prefer cyan) across chapters","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.28.41.png asks to standardize code colors across the document, including inline. User prefers the existing blue/cyan token color.\n\nCurrent state\n- Inline \u003cTerm\u003e uses var(--accent-cyan) (src/components/Typography.module.css).\n- Inline \u003ccode\u003e uses a different color/background (src/styles/global.css). This creates inconsistent “code” styling.\n\nTask\n1) Define a single visual style for inline code-like tokens (identifiers, short snippets, literal strings). Use var(--accent-cyan) as the main text color unless there is a strong reason not to.\n2) Update styling in src/styles/global.css and/or src/components/Typography.module.css so \u003ccode\u003e and \u003cTerm\u003e look consistent (color and font).\n3) Audit Chapter 1 + Chapter 2 for raw \u003ccode\u003e usage inside prose; where appropriate replace raw \u003ccode\u003e with \u003cTerm\u003e (Term is the documented component for code-like tokens).\n4) Ensure CodeBlock (syntax highlighted blocks) remains unchanged.\n\nAcceptance\n- Inline code tokens are consistently the same “blue” color across chapters.\n- No regressions to code blocks (\u003cpre\u003e\u003ccode\u003e should still render normally).\n- npm run build passes.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-21T10:57:07.314007-05:00","updated_at":"2025-12-21T15:02:59.179104-05:00","closed_at":"2025-12-21T15:02:59.179104-05:00","close_reason":"Standardized inline \u003ccode\u003e to use accent-cyan and font-mono (matching \u003cTerm\u003e); kept pre code override so code blocks stay normal; build passes.","dependencies":[{"issue_id":"babygpt-v1z","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.164688-05:00","created_by":"daemon"}]}
{"id":"babygpt-vcq","title":"1.5 Validate Ch1 map waypoint 1.6 The Limit","description":"AUDIT: Navigate to 1.6, verify description about scaling limits. IF mismatch: update. Check if section clearly establishes why memorization doesn't scale.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:24.024794-05:00","updated_at":"2025-12-20T19:31:19.565161-05:00","closed_at":"2025-12-20T19:31:19.565161-05:00","close_reason":"Closed"}
{"id":"babygpt-veo","title":"1.10 Validate Ch2 map waypoint 2.3 Ground Truth","description":"AUDIT: Navigate to 2.3, verify similarity definition description matches 'What Can We Measure?' section. IF mismatch: update. Ensure description captures P(next|c) fingerprint concept.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:37.819215-05:00","updated_at":"2025-12-20T19:32:34.288948-05:00","closed_at":"2025-12-20T19:32:34.288948-05:00","close_reason":"Closed"}
{"id":"babygpt-vp6","title":"1.1 Validate Ch1 map waypoint 1.1 The Physics","description":"AUDIT: Navigate to section 1.1 via waypoint, verify anchor works. Check description 'What we are predicting, why probability is the whole game' matches section content. IF broken link: fix section ID. IF description mismatch: update ChapterMap description.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:22.858935-05:00","updated_at":"2025-12-20T19:31:18.438925-05:00","closed_at":"2025-12-20T19:31:18.438925-05:00","close_reason":"Closed"}
{"id":"babygpt-w7e","title":"2.7 Validate Ch2 invariant: Token IDs are arbitrary labels","description":"AUDIT: Read section 2.4, verify 'token IDs are arbitrary labels with no meaningful distance' is explicit. IF missing: create ticket to add Integer Distance Lie callout. IF implicit: make explicit.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:12:19.057044-05:00","updated_at":"2025-12-20T19:42:02.475425-05:00","closed_at":"2025-12-20T19:42:02.475425-05:00","close_reason":"Closed"}
{"id":"babygpt-wcf","title":"4.2 Audit 2.2 Reuse Question dependencies","description":"AUDIT: Read section 2.2, verify it requires Ch1 n-gram concept. Check 'the cat' vs 'a cat' example refs counting model limits. IF no Ch1 connection: add explicit backward ref.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:14:20.811776-05:00","updated_at":"2025-12-20T19:50:14.742609-05:00","closed_at":"2025-12-20T19:50:14.742609-05:00","close_reason":"Closed"}
{"id":"babygpt-wk3","title":"1.1.1 Gap: Why independence matters for language modeling","description":"Line ~205: 'Surprise should be additive' is stated but WHY independence matters for text prediction isn't explained. The jump from 'two coins' to language is unmotivated. FIX: Add 1-2 sentences explaining that text characters ARE approximately independent given context, so additive surprise lets us score sequences by summing character-level surprises.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:47:26.503652-05:00","updated_at":"2025-12-20T20:52:03.052046-05:00","closed_at":"2025-12-20T20:52:03.052046-05:00","close_reason":"FIXED: Added explanatory bridge between 'additive surprise' and language modeling - explained that text characters are approximately independent given context, enabling sequence scoring via summed per-character surprises"}
{"id":"babygpt-wss","title":"7.3 Difficulty audit: Gradient derivation 2.10","description":"AUDIT: Check if calculus-based gradient derivation has adequate collapsible/optional wrappers. IF naked math: create ticket to wrap in expandable section with 'skip if not interested in math' note.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:28.913342-05:00","updated_at":"2025-12-20T20:06:54.898417-05:00","closed_at":"2025-12-20T20:06:54.898417-05:00","close_reason":"Closed"}
{"id":"babygpt-wsy6","title":"Ch1 content: fix combinatorics section to address 'words exist' misconception (dictionary still huge)","description":"Context\n- User request (quoted): “words exist! your answer is BOUND to be somewhere within the dictionary set yes?” The combinatorics section currently uses 27-character sequences and can accidentally imply that word-level modeling is constrained enough to avoid explosion.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Search for: “Slide that to N=10” and the sentence about “205 trillion possible 10-character sequences”.\n\nTask\n1) Rewrite this explanation from first principles to address the misconception directly:\n   - Yes, tokens come from a finite vocabulary (dictionary / token set).\n   - But the number of possible sequences is still |V|^T (explodes with length).\n2) Add a concrete word-level example with realistic vocabulary size (e.g., 50,000 words): show that 10-word sequences is 50,000^{10}, an astronomically large number.\n3) Clarify that “valid English sequences” is a smaller subset than all sequences, but still far too large for counting/lookup coverage; the core problem is lack of sharing/generalization.\n4) Keep the tone organic and bottom-up; avoid sounding like you are arguing with the reader.\n\nAcceptance\n- Reader no longer leaves with “dictionary makes it tractable” intuition.\n- Character-level and word-level explosion are both explained clearly and concretely.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:10.544666-05:00","updated_at":"2025-12-21T14:22:40.092424-05:00","closed_at":"2025-12-21T14:22:40.092424-05:00","close_reason":"Reframed combinatorics to explicitly address finite-vocabulary misconception; added |V|^T word-level example (50k^10 ~ 1e47); build passes.","dependencies":[{"issue_id":"babygpt-wsy6","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:58:01.08506-05:00","created_by":"daemon"}]}
{"id":"babygpt-x1s","title":"CSS: Replace 28 hardcoded font-family with var(--font-*) (8 files)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:09.39879-05:00","updated_at":"2025-12-20T23:27:46.032006-05:00","closed_at":"2025-12-20T23:27:46.032006-05:00","close_reason":"Closed"}
{"id":"babygpt-x8o","title":"1.1.3 Gap: Derive 27^T explicitly","description":"Line ~407-408: The warehouse metaphor is vivid but 27^T isn't derived. WHERE does the exponent come from? FIX: Add explicit derivation: T positions × V choices per position = V^T total contexts. For V=27, T=5: 27^5 = 14,348,907. Show the combinatorial explosion as a concrete calculation, not just a claim.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T20:48:00.617686-05:00","updated_at":"2025-12-20T21:02:30.91003-05:00","closed_at":"2025-12-20T21:02:30.91003-05:00","close_reason":"FIXED: Deleted redundant paragraph. Extended MathBlock explanation with 'Multiplication, not addition — because each choice at position 1 can pair with any of 27 at position 2.'"}
{"id":"babygpt-y2k","title":"Ch1 prose: gentler intro for joint probability; integrate 'AND multiplies' into core narrative","description":"Context\n- User request + screenshot: Screenshot 2025-12-21 at 10.29.18.png: math notation appears too abruptly; readers who are smart but not fluent in math syntax get scared off.\n- User specifically asked to move the “Why AND multiplies” explanation out of a callout/textbox and into the main document flow.\n\nWhere\n- File: src/chapters/Chapter1.tsx\n- Section: 1.1.2 “The Problem With Sequences” (MathBlock P(x_1, x_2, ..., x_t))\n- The current “Why “AND” multiplies” Callout is later near the corridor demo (search for title: \"Why “AND” multiplies\").\n\nTask\n1) Rewrite the lead-in around joint probability so it starts with plain language first, then introduces notation.\n2) Move the “AND multiplies” worlds/filtering example into the core narrative as normal \u003cParagraph\u003e content (not inside \u003cCallout\u003e).\n3) Ensure the reader learns: “AND in sequence” means “filter the world further” → multiplication of fractions → chain rule.\n4) Keep the math blocks, but make them feel like a summary of what the reader already understands.\n\nAcceptance\n- The joint-probability section reads smoothly for non-math-native readers.\n- The “AND multiplies” intuition is in the main narrative (not gated behind a callout).\n- No duplicated explanations; flow stays tight.\n- npm run build passes.","status":"closed","priority":1,"issue_type":"task","created_at":"2025-12-21T10:57:07.583539-05:00","updated_at":"2025-12-21T11:11:46.403878-05:00","closed_at":"2025-12-21T11:11:46.403878-05:00","close_reason":"Rewrote 1.1.2 intro and integrated 'Why AND multiplies' into main text.","dependencies":[{"issue_id":"babygpt-y2k","depends_on_id":"babygpt-2a3","type":"parent-child","created_at":"2025-12-21T10:57:58.387596-05:00","created_by":"daemon"}]}
{"id":"babygpt-y3f","title":"7.2 Difficulty audit: Ch1-\u003eCh2 transition","description":"AUDIT: Compare difficulty at end of Ch1 vs start of Ch2 (Grassmann history). IF gap: create ticket to add bridging paragraph or adjust Ch1 ending tone.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:28.61759-05:00","updated_at":"2025-12-20T20:05:47.258521-05:00","closed_at":"2025-12-20T20:05:47.258521-05:00","close_reason":"Closed"}
{"id":"babygpt-ygf","title":"8.4 Gap audit: softmax","description":"AUDIT: Verify softmax not referenced before Section 2.7 definition. IF early reference found: create ticket to relocate or add forward-ref.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:46.79527-05:00","updated_at":"2025-12-20T20:12:43.20592-05:00","closed_at":"2025-12-20T20:12:43.20592-05:00","close_reason":"Closed"}
{"id":"babygpt-z4x","title":"3.2 Audit 1.1.1 What Is Probability dependencies","description":"AUDIT: Read section 1.1.1, verify surprise=-log2(p) is defined BEFORE any cross-entropy reference. IF cross-entropy appears first: create ticket to reorder or add surprise definition earlier.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:31.587934-05:00","updated_at":"2025-12-20T19:42:00.562455-05:00","closed_at":"2025-12-20T19:42:00.562455-05:00","close_reason":"Closed"}
{"id":"babygpt-zd1","title":"3.11 Audit 1.2 Tokenization dependencies","description":"AUDIT: Read section 1.2, verify proper transition from theory to implementation. Check if chars-vs-words choice is motivated by sparsity discussion. IF not connected: add SectionLink to 1.1.7.2.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:13:54.65702-05:00","updated_at":"2025-12-20T19:42:28.510479-05:00","closed_at":"2025-12-20T19:42:28.510479-05:00","close_reason":"Closed"}
{"id":"babygpt-zdb","title":"6.4 Terminology audit: loss vs surprise vs cross-entropy","description":"AUDIT: trace where each term first appears and if relationship is explained. IF gap: add callout establishing 'surprise = -log(p), loss = average surprise, cross-entropy = expected surprise'. Create remediation ticket.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:16:11.642076-05:00","updated_at":"2025-12-20T20:29:39.652984-05:00","closed_at":"2025-12-20T20:29:39.652984-05:00","close_reason":"PASS: Relationship established in Ch2: surprise=-log(p), loss=cross-entropy=average surprise, defined at line 1037 and 2.10 sections."}
{"id":"babygpt-zmt","title":"CSS: Replace 100 hardcoded hex colors with CSS variables (19 files)","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T23:18:08.49681-05:00","updated_at":"2025-12-20T23:25:25.33739-05:00","closed_at":"2025-12-20T23:25:25.33739-05:00","close_reason":"Closed"}
{"id":"babygpt-zww","title":"2.1 Validate Ch1 invariant: P(next|context) via chain rule","description":"AUDIT: Read Invariants component in Ch1, verify claim 'P(next|context) via chain rule'. Trace sections 1.1.5 + earlier to confirm derivation exists. IF missing: create ticket to add derivation. IF present but unclear: create ticket to clarify.","status":"closed","priority":2,"issue_type":"task","created_at":"2025-12-20T19:11:59.990794-05:00","updated_at":"2025-12-20T19:32:32.044197-05:00","closed_at":"2025-12-20T19:32:32.044197-05:00","close_reason":"Closed"}
